{
  
    
        "post0": {
            "title": "Analyzing NYC Taxi Trips: Understanding Demand and Optimizing Revenue",
            "content": "1. Introduction . Project objective: Our exploration will enable us to identify taxi trip patterns in NYC and develop strategies accordingly to improve revenue for taxi drivers and taxi companies. . We combined datasets from 2018 to 2021 to draw more insight from our analysis. As data from 2020 onwards show the impact of the pandemic on taxi demand, providing information from the two years prior to the pandemic would lead to a more accurate interpretation of results. . For Phase 1 of our project we explored the data to understand commonalities in demand, depending on year, month, day of the week, time of the day and location. In Phase 2 we utilized our findings to discern further how those factors affect revenue. . Motivation: Taxi market has been facing great competition in recent years, as an increasing number of people switch from taxi to share-riding, such as Uber and Lyft, as a means of transportation. While taxi companies and drivers may have a hard time going through this transition, there are people who prefer and need to take taxis. By analyzing taxi trip patterns in NYC, we will help taxi companies and drivers learn more about the customers they&#39;re serving and, more importantly, how to increase revenue to stay in business. . The analysis revealed that taxi trips were most popular during the morning and afternoon hours. Short-distance trips were the most popular, with the most frequently traveled routes being in the upper east and upper west side, spanning 66 blocks. Long trips were found to be the most expensive per minute. In terms of difference in demand based on the day of the week; Friday has the highest demand, while Sunday has the lowest. . Report Summary: We cleaned the dataframe to exclude unrepresentative data points and created features that better fit the purpose of our analyses. We conducted exploratory data analysis on taxi trip demand patterns, revenue patterns, and how they interrelate to one another. We also implemented machine learning methods, including linear regression, random forest, and GBT regression, to predict taxi trip price based on other features. . 2. Data source . Data Source: The datasets used were downloaded from BigQuery. The information in this dataset was made available by the New York City Taxi and Limousine Commission (TLC). . This project used datasets containing data regarding yellow taxi trips in New York City spanning from 2018 to 2021. We also used a taxi zone dataset to assign name locations to the zone_ids, which by itself, would not sufficiently contextualize the data. . We decided not to include data from 2022 as early exploration of that dataset indicated that values from the month of December were missing from the original dataset featured on BigQuery. . Data dictionary taxi_zone_geom . bigquery-public-data.new_york_taxi_trips.taxi_zone_geom . Column Name Description Type . zone_id | Unique ID number of each taxi zone. Corresponds with the pickup_location_id and dropoff_location_id in each of the trips tables | STRING | . zone_name | Full text name of the taxi zone | STRING | . borough | Borough containing the taxi zone | STRING | . zone_geom | Geometric outline that defines the taxi zone suitable for GIS analysis. | GEOGRAPHY | . tlc_yellow_trips . 2018: bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018 . 2019: bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2019 . 2020: bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2020 . 2021: bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2021 . Column Name Description Type . vendor_id | A code indicating the LPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc. | STRING | . pickup_datetime | The date and time when the meter was engaged | TIMESTAMP | . dropoff_datetime | The date and time when the meter was disengaged | TIMESTAMP | . passenger_count | The number of passengers in the vehicle. This is a driver-entered value. | INTEGER | . trip_distance | The elapsed trip distance in miles reported by the taximeter. | NUMERIC | . rate_code | The final rate code in effect at the end of the trip. 1= Standard rate 2=JFK 3=Newark 4=Nassau or Westchester 5=Negotiated fare 6=Group ride | STRING | . store_and_fwd_flag | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka &#39;store and forward,&#39; because the vehicle did not have a connection to the server. Y= store and forward trip N= not a store and forward trip | STRING | . payment_type | A numeric code signifying how the passenger paid for the trip. 1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided trip | STRING | . fare_amount | The time-and-distance fare calculated by the meter | NUMERIC | . extra | Miscellaneous extras and surcharges. Currently, this only includes the 0.50 and 1 dollar rush hour and overnight charges | NUMERIC | . mta_tax | 0.50 dollar MTA tax that is automatically triggered based on the metered rate in use | NUMERIC | . tip_amount | Tip amount. This field is automatically populated for credit card tips. Cash tips are not included. | NUMERIC | . tolls_amount | Total amount of all tolls paid in trip. | NUMERIC | . imp_surcharge | 0.30 dollar improvement surcharge assessed on hailed trips at the flag drop. The improvement surcharge began being levied in 2015. | NUMERIC | . airport_fee | - | NUMERIC | . total_amount | The total amount charged to passengers. Does not include cash tips. | NUMERIC | . pickup_location_id | TLC Taxi Zone in which the taximeter was engaged | STRING | . dropoff_location_id | TLC Taxi Zone in which the taximeter was disengaged | STRING | . data_file_year | Datafile timestamp year value | INTEGER | . data_file_month | Datafile timestamp month value | INTEGER | . 3. Data Cleaning . 3.a. Loading the data into a Spark DataFrame . data2021 = spark.read.format(&#39;bigquery&#39;).option(&#39;table&#39;, &#39;bigquery-public-data:new_york_taxi_trips.tlc_yellow_trips_2021&#39;).load() data2020 = spark.read.format(&#39;bigquery&#39;).option(&#39;table&#39;, &#39;bigquery-public-data:new_york_taxi_trips.tlc_yellow_trips_2020&#39;).load() data2019 = spark.read.format(&#39;bigquery&#39;).option(&#39;table&#39;, &#39;bigquery-public-data:new_york_taxi_trips.tlc_yellow_trips_2019&#39;).load() data2018 = spark.read.format(&#39;bigquery&#39;).option(&#39;table&#39;, &#39;bigquery-public-data:new_york_taxi_trips.tlc_yellow_trips_2018&#39;).load() df_raw = data2021.union(data2020).union(data2019).union(data2018) df_raw.printSchema() df_raw.show(5) . df_raw = data2021.union(data2020).union(data2019).union(data2018) df_raw.printSchema() df_raw.show(5) . root |-- vendor_id: string (nullable = false) |-- pickup_datetime: timestamp (nullable = true) |-- dropoff_datetime: timestamp (nullable = true) |-- passenger_count: long (nullable = true) |-- trip_distance: decimal(38,9) (nullable = true) |-- rate_code: string (nullable = true) |-- store_and_fwd_flag: string (nullable = true) |-- payment_type: string (nullable = true) |-- fare_amount: decimal(38,9) (nullable = true) |-- extra: decimal(38,9) (nullable = true) |-- mta_tax: decimal(38,9) (nullable = true) |-- tip_amount: decimal(38,9) (nullable = true) |-- tolls_amount: decimal(38,9) (nullable = true) |-- imp_surcharge: decimal(38,9) (nullable = true) |-- airport_fee: decimal(38,9) (nullable = true) |-- total_amount: decimal(38,9) (nullable = true) |-- pickup_location_id: string (nullable = true) |-- dropoff_location_id: string (nullable = true) |-- data_file_year: long (nullable = true) |-- data_file_month: long (nullable = true) . [Stage 0:&gt; (0 + 1) / 1] . ++-+-++-+++++--+--+--++-+--+++-+--++ |vendor_id| pickup_datetime| dropoff_datetime|passenger_count|trip_distance|rate_code|store_and_fwd_flag|payment_type| fare_amount| extra| mta_tax| tip_amount|tolls_amount|imp_surcharge|airport_fee|total_amount|pickup_location_id|dropoff_location_id|data_file_year|data_file_month| ++-+-++-+++++--+--+--++-+--+++-+--++ | 1|2021-01-01 00:43:30|2021-01-01 01:11:06| 1| 14.700000000| 1.0| N| 1|42.000000000|0.500000000|0.500000000|8.650000000| 0E-9| 0.300000000| null|51.950000000| 132| 165| 2021| 1| | 1|2021-01-01 00:15:48|2021-01-01 00:31:01| 0| 10.600000000| 1.0| N| 1|29.000000000|0.500000000|0.500000000|6.050000000| 0E-9| 0.300000000| null|36.350000000| 138| 132| 2021| 1| | 2|2021-01-01 00:19:57|2021-01-01 00:43:03| 3| 10.740000000| 1.0| N| 1|32.500000000|0.500000000|0.500000000|4.000000000| 0E-9| 0.300000000| null|40.300000000| 264| 231| 2021| 1| | 2|2021-01-01 00:44:58|2021-01-01 01:07:41| 2| 5.850000000| 1.0| N| 1|21.500000000|0.500000000|0.500000000|5.060000000| 0E-9| 0.300000000| null|30.360000000| 249| 238| 2021| 1| | 2|2021-01-01 00:06:11|2021-01-01 00:23:40| 1| 8.060000000| 1.0| N| 1|24.500000000|0.500000000|0.500000000|5.660000000| 0E-9| 0.300000000| null|33.960000000| 75| 264| 2021| 1| ++-+-++-+++++--+--+--++-+--+++-+--++ only showing top 5 rows . . 3.b. Dropping Unnecessary Columns . The columns &#39;data_file_year&#39; and &#39;data_file_month&#39; are not of interest, so we drop them here. . df_raw = df_raw.drop(&#39;data_file_year&#39;,&#39;data_file_month&#39;) . df_raw.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string] . 3.c. Handling missing values . Here we sample a portion from the whole dataframe only for more efficient visualization of missing values. We&#39;ll conduct the actually data cleaning steps on the original dataframe based on the pattern observed from the sample. The sample is 0.05% of the entire dataframe, giving us over 0.1 million records. This sample is chosen to be this size for the purpose of efficient code execution. . df_raw_sample = df_raw.sample(False, 0.0005, 843) df_raw_sample.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string] . import matplotlib.pyplot as plt import seaborn as sns . Upon investigation, we find four columns with missing values, which are not missing at random. The observations of columns &quot;passenger_count&quot;, and &quot;rate_code&quot; are missing only when &quot;payment_type&quot; is 0 and vice versa. Moreover, payment_type that has a value of 0 is not described in the data dictionary, so we will drop these rows. . df_raw_sample = df_raw_sample.where(~(df_raw_sample[&#39;payment_type&#39;]==0)) df_raw = df_raw.where(~(df_raw_sample[&#39;payment_type&#39;]==0)) . The airport_fee column had no values until a the end of March in 2021. We fill the missing values of &quot;airport_fee&quot; with -999 so we can keep the column for further analysis. . df_raw = df_raw.fillna(-999,subset=[&#39;airport_fee&#39;]) . df_raw.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string] . 3.d. Handling Outliers . 3.d.1. Handling outliers based on time and trip duration . from pyspark.sql.functions import year, hour, unix_timestamp, col, round # Extract year as a column df_raw = df_raw.withColumn(&#39;year&#39;,year(df_raw[&#39;pickup_datetime&#39;])) . As our dataset should only contain data from 2018 to 2021, we therefore drop all rows that are not within this year range. . df_raw = df_raw.where((df_raw[&#39;year&#39;]==2021)|(df_raw[&#39;year&#39;]==2020)|(df_raw[&#39;year&#39;]==2019)|(df_raw[&#39;year&#39;]==2018)) . Drop rows where pickup time is no earlier than dropoff time. . df_raw = df_raw.where(df_raw[&#39;pickup_datetime&#39;]&lt;df_raw[&#39;dropoff_datetime&#39;]) . In the following step, we&#39;re handling outliers based on the common sense that most taxi trips wouldn&#39;t take extremely long hours. We&#39;ll calculate the duration in minutes for each taxi trip and eliminate data points that fall out of the range of mean +/- 2.5 standard deviations, as is often the convention in statistical analysis. . df_raw = df_raw.withColumn(&quot;duration&quot;, (unix_timestamp(col(&quot;dropoff_datetime&quot;)) - unix_timestamp(col(&quot;pickup_datetime&quot;))) / 60) . df_raw = df_raw.withColumn(&quot;duration&quot;, round(col(&quot;duration&quot;), 2)) . std_duration = df_raw.agg({&#39;duration&#39;: &#39;stddev&#39;}).collect()[0][0] mean_duration = df_raw.agg({&#39;duration&#39;: &#39;mean&#39;}).collect()[0][0] hi_bound_duration = mean_duration + (2.5 * std_duration) low_bound_duration = mean_duration - (2.5 * std_duration) df_raw = df_raw.where((df_raw[&#39;duration&#39;]&gt;low_bound_duration)&amp;(df_raw[&#39;duration&#39;]&lt;hi_bound_duration)) . . df_raw.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string, year: int, duration: double] . 3.d.2. Handling outliers based on price and rate code . Each taxi trip is priced according to a fixed set of rules, where the major component of pricing is attributed to distance-and-time fare calculated by the meter (in our dataset, the &quot;fare_amount&quot; variable). While other fees and surcharges also apply to some taxi trips, they only account for a much smaller portion of taxi pricing. Based on this knowledge, we will exliminate &quot;abnormal&quot; data points that don&#39;t follow such patterns. . We noticed there exist out-of-range values (the value &quot;99.0&quot;, which is not a valid rate_code according to the data dictionary) and abnormal values (e.g. &quot;1.0&quot; and &quot;1&quot; exist at the same time) for the rate_code column, so we fix them here using the following code: . df_raw = df_raw.where((df_raw[&#39;rate_code&#39;] != &#39;99.0&#39;)&amp;(df_raw[&#39;rate_code&#39;] != &#39;99&#39;)) from pyspark.sql.functions import when df_raw = df_raw.withColumn(&#39;rate_code&#39;, when(df_raw[&#39;rate_code&#39;]==&#39;1.0&#39;,&#39;1&#39;) .when(df_raw[&#39;rate_code&#39;]==&#39;2.0&#39;,&#39;2&#39;) .when(df_raw[&#39;rate_code&#39;]==&#39;3.0&#39;,&#39;3&#39;) .when(df_raw[&#39;rate_code&#39;]==&#39;4.0&#39;,&#39;4&#39;) .when(df_raw[&#39;rate_code&#39;]==&#39;5.0&#39;,&#39;5&#39;) .when(df_raw[&#39;rate_code&#39;]==&#39;6.0&#39;,&#39;6&#39;) .otherwise(df_raw[&#39;rate_code&#39;])) . df_raw.select(col(&#39;rate_code&#39;)).distinct().show() . . ++ |rate_code| ++ | 3| | 5| | 6| | 1| | 4| | 2| ++ . . In the next step, we create two plots describing the correlation between trip distance and distance-and-time calculated fare. The plot on the left-hand side is for all trips, including standard-rate trips, airport trips, and &quot;negotiated price&quot; trips. The plot on the right-hand side only includes standard-rate trips whose fare is calculated by the meter based on distance and time. . When rate_code equals 1, the corresponding row represents a trip charging standard rate, meaning that the fare_amount column reflects a fare calculated based on distance and time. The base fare for such trips from 2018 to 2021 is $2.5. Each additional mile charges another $2.5 and each additional minute charges another $0.5. Therefore, the correlation between trip distance and fare_amount should be linear with a slope equal to or above 2.5. | When rate_code is 2 or 3 or 4, the corresponding row represents an airport taxi trip that uses a different base fare from standard rate. For example, there is a flat rate of $52 for trips between Manhattan to JFK airport, and such trips have a rate_code of 2 in the dataset. | When rate_code equals 5 or 6, the row represents a taxi trip with a negotiated price whose trip distance does not follow a linear correlation with fare_amount. | . As can be seen from the plots below, there are data points that do not follow the above rules and should be fixed. . df_raw_sample = df_raw.sample(False,0.0005,843) df_pd = df_raw_sample.toPandas() . . df_pd[&#39;trip_distance&#39;]= df_pd[&#39;trip_distance&#39;].astype(&#39;float&#39;) df_pd[&#39;fare_amount&#39;]= df_pd[&#39;fare_amount&#39;].astype(&#39;float&#39;) df_pd[&#39;total_amount&#39;]= df_pd[&#39;total_amount&#39;].astype(&#39;float&#39;) df_pd[&#39;tip_amount&#39;]= df_pd[&#39;tip_amount&#39;].astype(&#39;float&#39;) df_pd[&#39;extra&#39;]= df_pd[&#39;extra&#39;].astype(&#39;float&#39;) df_pd[&#39;mta_tax&#39;]= df_pd[&#39;mta_tax&#39;].astype(&#39;float&#39;) df_pd[&#39;tolls_amount&#39;]= df_pd[&#39;tolls_amount&#39;].astype(&#39;float&#39;) df_pd[&#39;imp_surcharge&#39;]= df_pd[&#39;imp_surcharge&#39;].astype(&#39;float&#39;) df_pd[&#39;airport_fee&#39;]= df_pd[&#39;airport_fee&#39;].astype(&#39;float&#39;) . fig, ax = plt.subplots(1, 2, figsize=(12, 6)) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;fare_amount&#39;,data=df_pd,hue=&#39;rate_code&#39;,ax=ax[0]) ax[0].set_title(&quot;Distance-fare Correlation for Standard Rate Trips&quot;) ax[0].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;fare_amount&#39;,data=df_pd[df_pd[&#39;rate_code&#39;]==&#39;1&#39;],ax=ax[1],hue=&#39;rate_code&#39;) ax[1].set_title(&quot;Distance-fare Correlation for Standard Rate Trips&quot;) ax[1].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) plt.show() . We use the code below to fix the data: . For all trips, we eliminate records that have a 0 or negative trip distance. | For all trips, we eliminate records with a total_amount less than $2.5. | For standard rate trips, we eliminate records whose trip distance doesn&#39;t folllow a linear correlation with a slope of at least 2.5 with fare_amount. | For non-standard-rate trips, because the correlation between trip distance and fare_amount largely depend on actual negotiation between customers and drivers, we will keep those data points where trip distance and fare_amount don&#39;t follow a linear correlation as discussed before. | . df_standard_rate =df_raw.where((df_raw[&#39;rate_code&#39;]==&#39;1&#39;)&amp;(df_raw[&#39;trip_distance&#39;]&gt;0)&amp;(df_raw[&#39;total_amount&#39;]&gt;=2.5)&amp;(df_raw[&#39;fare_amount&#39;]/df_raw[&#39;trip_distance&#39;]&gt;=2.5)) df_other_rates =df_raw.where((df_raw[&#39;rate_code&#39;]!=&#39;1&#39;)&amp;(df_raw[&#39;trip_distance&#39;]&gt;0)&amp;(df_raw[&#39;total_amount&#39;]&gt;=2.5)) . Dataframe, df, created below is a clean version that has addressed all missing values, outliers, and abnormal records. . df = df_standard_rate.union(df_other_rates) . Below are plots that describe the correlation between trip distance and fare after outliers are eliminated. . import pandas as pd df_pd = pd.concat([df_standard_rate_pd,df_other_rates_pd]) . fig, ax = plt.subplots(1, 2, figsize=(12, 6)) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;fare_amount&#39;,data=df_pd,hue=&#39;rate_code&#39;,hue_order = [&#39;1&#39;, &#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;],ax=ax[0]) ax[0].set_title(&quot;Distance-fare Correlation for All Trips (Clean)&quot;) ax[0].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;fare_amount&#39;,data=df_pd[df_pd[&#39;rate_code&#39;]==&#39;1&#39;],ax=ax[1],hue=&#39;rate_code&#39;) ax[1].set_title(&quot;Distance-fare Correlation for Standard Rate Trips (Clean)&quot;) ax[1].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) plt.show() . The right-hand side plot suggests that the fare for JFK trips, represented by the red dots, are quite fixed no matter how long the trip actually covers. Trips other than JFK and negotiated-price trips mostly follow linear distribution between distance and fare. Negotiated-price trips seem to follow a more unusual distribution, which might depend on the actual negotiation between customers and drivers. . Here we also create the same plots for trip distance and total price (not the fare calculated merely from distance and time). The correlation reflected in the plots resemble the fare-counterpart a lot, because there&#39;re other fees, surcharges, tax, and tips involved. . fig, ax = plt.subplots(1, 2, figsize=(12, 6)) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;total_amount&#39;,data=df_pd,hue=&#39;rate_code&#39;,ax=ax[0]) ax[0].set_title(&quot;Distance-price Correlation for All Trips (Clean)&quot;) ax[0].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) sns.scatterplot(x=&#39;trip_distance&#39;,y=&#39;total_amount&#39;,data=df_pd[df_pd[&#39;rate_code&#39;]==&#39;1&#39;],ax=ax[1],hue=&#39;rate_code&#39;) ax[1].set_title(&quot;Distance-price Correlation for Standard Rate Trips (Clean)&quot;) ax[1].legend(loc=&#39;lower right&#39;,title=&#39;rate_code&#39;) plt.show() . 3.e. Feature Engineering . from pyspark.sql.types import TimestampType EDT_start = &quot;2021-03-14&quot; EST_start = &quot;2021-11-07&quot; . from pyspark.sql.functions import from_utc_timestamp . Because pickup/dropoff time in the original dataset is indicated by UTC, we here convert UTC to Eastern Time . df = df.withColumn( &quot;pickup_ET&quot;, from_utc_timestamp(&#39;pickup_datetime&#39;,&#39;America/New_York&#39;) ) df = df.withColumn( &quot;dropoff_ET&quot;, from_utc_timestamp(&#39;dropoff_datetime&#39;,&#39;America/New_York&#39;) ) . df.printSchema() . root |-- vendor_id: string (nullable = false) |-- pickup_datetime: timestamp (nullable = true) |-- dropoff_datetime: timestamp (nullable = true) |-- passenger_count: long (nullable = true) |-- trip_distance: decimal(38,9) (nullable = true) |-- rate_code: string (nullable = true) |-- store_and_fwd_flag: string (nullable = true) |-- payment_type: string (nullable = true) |-- fare_amount: decimal(38,9) (nullable = true) |-- extra: decimal(38,9) (nullable = true) |-- mta_tax: decimal(38,9) (nullable = true) |-- tip_amount: decimal(38,9) (nullable = true) |-- tolls_amount: decimal(38,9) (nullable = true) |-- imp_surcharge: decimal(38,9) (nullable = true) |-- airport_fee: decimal(38,9) (nullable = true) |-- total_amount: decimal(38,9) (nullable = true) |-- pickup_location_id: string (nullable = true) |-- dropoff_location_id: string (nullable = true) |-- year: integer (nullable = true) |-- duration: double (nullable = true) |-- dropoff_ET: timestamp (nullable = true) |-- pickup_ET: timestamp (nullable = true) . df.select(&#39;pickup_datetime&#39;,&#39;pickup_ET&#39;,&#39;dropoff_datetime&#39;,&#39;dropoff_ET&#39;).show(5) . +-+-+-+-+ | pickup_datetime| pickup_ET| dropoff_datetime| dropoff_ET| +-+-+-+-+ |2021-01-01 00:43:30|2020-12-31 19:43:30|2021-01-01 01:11:06|2020-12-31 20:11:06| |2021-01-01 00:15:48|2020-12-31 19:15:48|2021-01-01 00:31:01|2020-12-31 19:31:01| |2021-01-01 00:19:57|2020-12-31 19:19:57|2021-01-01 00:43:03|2020-12-31 19:43:03| |2021-01-01 00:44:58|2020-12-31 19:44:58|2021-01-01 01:07:41|2020-12-31 20:07:41| |2021-01-01 00:06:11|2020-12-31 19:06:11|2021-01-01 00:23:40|2020-12-31 19:23:40| +-+-+-+-+ only showing top 5 rows . Create a column &quot;day_of_week&quot; to indicate the day of week for analysis . from pyspark.sql.functions import date_format df = df.withColumn(&#39;day_of_week&#39;, date_format(&#39;pickup_ET&#39;,&#39;EEEE&#39;)) . Extract the hour of the day for analysis . df = df.withColumn(&#39;hour&#39;, hour(df[&#39;pickup_ET&#39;])) . df.select(&#39;pickup_ET&#39;,&#39;hour&#39;).show(5) . +-+-+ | pickup_ET|hour| +-+-+ |2020-12-31 19:43:30| 19| |2020-12-31 19:15:48| 19| |2020-12-31 19:19:57| 19| |2020-12-31 19:44:58| 19| |2020-12-31 19:06:11| 19| +-+-+ only showing top 5 rows . df_raw.unpersist() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string, year: int, duration: double, dropoff_ET: timestamp, pickup_ET: timestamp, day_of_week: string, hour: int] . df.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string, year: int, duration: double, pickup_ET: timestamp, dropoff_ET: timestamp, day_of_week: string, hour: int] . df.printSchema() . root |-- vendor_id: string (nullable = false) |-- pickup_datetime: timestamp (nullable = true) |-- dropoff_datetime: timestamp (nullable = true) |-- passenger_count: long (nullable = true) |-- trip_distance: decimal(38,9) (nullable = true) |-- rate_code: string (nullable = true) |-- store_and_fwd_flag: string (nullable = true) |-- payment_type: string (nullable = true) |-- fare_amount: decimal(38,9) (nullable = true) |-- extra: decimal(38,9) (nullable = true) |-- mta_tax: decimal(38,9) (nullable = true) |-- tip_amount: decimal(38,9) (nullable = true) |-- tolls_amount: decimal(38,9) (nullable = true) |-- imp_surcharge: decimal(38,9) (nullable = true) |-- airport_fee: decimal(38,9) (nullable = true) |-- total_amount: decimal(38,9) (nullable = true) |-- pickup_location_id: string (nullable = true) |-- dropoff_location_id: string (nullable = true) |-- year: integer (nullable = true) |-- duration: double (nullable = true) |-- dropoff_ET: timestamp (nullable = true) |-- pickup_ET: timestamp (nullable = true) |-- day_of_week: string (nullable = true) |-- hour: integer (nullable = true) . 4. Summary Statistics . The clean dataframe has rows and columns. . print(df.count(),len(df.columns)) . [Stage 54:=====================================================&gt;(114 + 1) / 115] . 239441040 24 . . 4.a. Distribution of duration, distance, and price . The distribution of three key columns are displayed below. . The range of taxi trip durations is from 0.02 to 153 minutes with the average being 14 minutes. There&#39;s more concentration in short-duration according to the histgram. | The range of taxi trip distance is from 0.01 to 167,329 miles with the average being 3 miles. Most data points fall under the lower range of the distance span. (Eliminating extreme values for trip distances takes too long, so it was not done due to time ristriction) | The range of taxi trip prices is from $2.5 to $998,325 dollars with the average being $18. The majority of data points fall under the range of 2.5 to 30 dollars. (Eliminating extreme values for total price takes too long, so it was not done due to time ristriction) | . print(df.select(&#39;duration&#39;).describe().show(),df.select(&#39;trip_distance&#39;).describe().show(),df.select(&#39;total_amount&#39;).describe().show()) . . +-++ |summary| duration| +-++ | count| 236937680| | mean|14.202429541683667| | stddev|11.379465486849186| | min| 0.02| | max| 185.52| +-++ . . +-++ |summary| trip_distance| +-++ | count| 236937680| | mean| 2.9694555070000| | stddev|18.803850644132353| | min| 0.010000000| | max| 167329.450000000| +-++ . [Stage 75:=====================================================&gt;(229 + 1) / 230] . +-++ |summary| total_amount| +-++ | count| 236937680| | mean| 17.7396180310000| | stddev|146.89305083547453| | min| 2.500000000| | max| 998325.610000000| +-++ None None None . . The histograms below are based on a portion sampled from the whole dataframe for efficient visualization only. Further analysis will be done on the whole dataframe. . fig, ax = plt.subplots(3, 1, figsize=(12, 12)) sns.histplot(df_pd[&#39;duration&#39;], ax=ax[0]) ax[0].set_title(&#39;Distribution of Taxi Trip Durations&#39;) ax[0].set_xlabel(&#39;Duration (minutes)&#39;) sns.histplot(df_pd[&#39;trip_distance&#39;], ax=ax[1]) ax[1].set_title(&#39;Distribution of Trip Distance&#39;) ax[1].set_xlabel(&#39;Distance (miles)&#39;) sns.histplot(df_pd[&#39;total_amount&#39;], ax=ax[2]) ax[2].set_title(&#39;Distribution of Taxi Trip Prices&#39;) ax[2].set_xlabel(&#39;Price ($)&#39;) plt.subplots_adjust(hspace=0.4) plt.show() . 4.b. Taxi Trip Pricing . The code and graph below show the composition of a taxi trip price. We only include standard-rate trips to show the percentage of tips in total price, as our dataset only has the tip_amount column for trips paid with credit card. . from pyspark.sql.functions import mean, sum avg_prices = df.where(col(&#39;payment_type&#39;)==&#39;1&#39;).select( mean(&quot;fare_amount&quot;).alias(&quot;avg_fare&quot;), mean(&quot;tip_amount&quot;).alias(&quot;avg_tips&quot;), mean(&quot;extra&quot;).alias(&quot;avg_extra&quot;), mean(&quot;mta_tax&quot;).alias(&quot;avg_tax&quot;), mean(&quot;tolls_amount&quot;).alias(&quot;avg_tolls&quot;), mean(&quot;imp_surcharge&quot;).alias(&quot;avg_surcharge&quot;) ) . avg_prices = avg_prices.withColumn(&#39;avg_total&#39;, col(&#39;avg_fare&#39;)+ col(&#39;avg_extra&#39;)+ col(&#39;avg_tips&#39;)+ col(&#39;avg_tax&#39;)+ col(&#39;avg_surcharge&#39;)+ col(&#39;avg_tolls&#39;)) avg = avg_prices.first() . . fare_pctg = avg_prices.select(col(&#39;avg_fare&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] tips_pctg = avg_prices.select(col(&#39;avg_tips&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] extra_pctg = avg_prices.select(col(&#39;avg_extra&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] tax_pctg = avg_prices.select(col(&#39;avg_tax&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] tolls_pctg = avg_prices.select(col(&#39;avg_tolls&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] surcharge_pctg = avg_prices.select(col(&#39;avg_surcharge&#39;)/col(&#39;avg_total&#39;)*100).collect()[0][0] . . import matplotlib.pyplot as plt labels = [&#39;Fare&#39;, &#39;Tips&#39;, &#39;Extra&#39;, &#39;Tax&#39;, &#39;Tolls&#39;, &#39;Surcharge&#39;] sizes = [fare_pctg, tips_pctg, extra_pctg, tax_pctg, tolls_pctg, surcharge_pctg] plt.pie(sizes, labels=labels, autopct=&#39;%1.1f%%&#39;) plt.axis(&#39;equal&#39;) plt.title(&#39;Average Percentage of Price Components&#39;) plt.show() . This pie chart indicates that the majority of taxi trip price comes from distance-and-time calculated fare, accounting for 73% of the total price. An average of 16% of total price is attributed to tips. Fees, tax, and surcharges account for the rest of total price. . df= spark.read.format(&quot;parquet&quot;).load(&quot;gs://is843-team6/notebooks/jupyter/df.parquet&quot;) . 5. Exploratory Data Analysis . 5.a. Demand . Revenue = Demand * Price According to the equation above, we want to start with looking at the demand patterns for NYC taxi trips. . Question 1: How does the distribution of taxi demand differ across the days of the week, time of day, and month? . Taxi demand distribution across hours of a day in 2021 . df.createOrReplaceTempView(&#39;df&#39;) . from pyspark.sql.functions import col df_2021 = df.where((col(&#39;year&#39;)==2021)&amp;(col(&#39;pickup_ET&#39;)!=&#39;2121-11-07&#39;)&amp;(col(&#39;pickup_ET&#39;)!=&#39;2021-03-14&#39;)) . df_2021.createOrReplaceTempView(&#39;df_2021&#39;) . hourly_demand2021 = spark.sql(&quot;&quot;&quot; SELECT hour, COUNT(*) AS demand_hour FROM df_2021 GROUP BY hour ORDER BY hour &quot;&quot;&quot;) hourly_demand2021_pd = hourly_demand2021.toPandas() . . sns.set_style(&#39;whitegrid&#39;) plt.figure(figsize=(6,6)) plot = sns.catplot(x=&#39;hour&#39;, y=&#39;demand_hour&#39;,data=hourly_demand2021_pd,kind=&#39;bar&#39;,color=&#39;red&#39;) plot.set(xlabel=&#39;hour of day&#39;, ylabel=&#39;average taxi demand&#39;) plt.title(&#39;Average Taxi Demand of a Day for 2021&#39;) plot.set_xticklabels(rotation=90) plt.subplots_adjust(top=0.7) plt.show() . &lt;Figure size 600x600 with 0 Axes&gt; . Rush hours are 10 am to 3 pm, when the demand for taxis is high. For drivers to optimize their income, avoiding rush hours that may see traffic congestion and midnight when taxi demand is too low would be a good strategy. . Taxi demand distribution across days of the week . df_weekly_demand_pd = df_weekly_demand.toPandas() . . sns.set_style(&#39;whitegrid&#39;) plt.figure(figsize=(8,6)) plot = sns.catplot(x=&#39;day_of_week&#39;, y=&#39;avg_demand&#39;,data=df_weekly_demand_pd,kind=&#39;bar&#39;,color=&#39;red&#39;,order=[&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Wednesday&#39;,&#39;Thursday&#39;,&#39;Friday&#39;,&#39;Saturday&#39;,&#39;Sunday&#39;]) plot.set(xlabel=&#39;day of the week&#39;, ylabel=&#39;average taxi demand&#39;) plt.title(&#39;Distribution of Average Taxi Demand over a Week&#39;) plot.set_xticklabels(rotation=45) plt.show() . &lt;Figure size 800x600 with 0 Axes&gt; . Based on the graph above, from 2018 to 2021, Sunday saw the lowest demand for taxi trips, while Friday saw the highest demand. If drivers want to make more money, they may not take business on these two days because Friday may see congestion, and you may only take a few customers on Sunday. . Hourly taxi demand distribution . Count the number of taxi trips for each hour of a day . hourly_distribution = df.groupBy(&quot;hour&quot;).count().orderBy(&quot;hour&quot;) hourly_distribution.show(25) . [Stage 105:====================================================&gt;(229 + 1) / 230] . +-+--+ |hour| count| +-+--+ | 0| 1851309| | 1| 3128851| | 2| 6334319| | 3| 9349140| | 4|10813174| | 5|11137490| | 6|11521145| | 7|12229850| | 8|12805310| | 9|13202922| | 10|13714879| | 11|13308316| | 12|13422739| | 13|14877029| | 14|15236563| | 15|14033836| | 16|12990995| | 17|12459798| | 18|10918785| | 19| 8513946| | 20| 5990693| | 21| 4125097| | 22| 2903280| | 23| 2068214| +-+--+ . . Monthly taxi demand distribution in 2019, 2020, and 2021 . from pyspark.sql.functions import year, month, count, date_trunc import seaborn as sns df_filtered = df.filter((df[&#39;year&#39;] == 2019) | (df[&#39;year&#39;] == 2020) | (df[&#39;year&#39;] == 2021)) . df_monthly_distribution = df_filtered.groupBy((&quot;year&quot;), month(&quot;pickup_ET&quot;).alias(&quot;month&quot;)) .count() .orderBy(&quot;year&quot;, &quot;month&quot;) . df_monthly_distribution_graph= sns.FacetGrid(df_monthly_distribution.toPandas(), col=&quot;year&quot;, col_wrap=3, sharey=False) df_monthly_distribution_graph.map(sns.barplot, &quot;month&quot;, &quot;count&quot;, palette=[&quot;#ff4c4c&quot;]) df_monthly_distribution_graph.set_axis_labels(&quot;Month&quot;, &quot;Count of Trips&quot;) . /opt/conda/miniconda3/lib/python3.8/site-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . &lt;seaborn.axisgrid.FacetGrid at 0x7f5f2696b700&gt; . Based on the graph above, it is evident that in 2019 most months had a similar distribution of trips, with winter months showing slightly higher amounts when compared to summer. January and February of 2020 displayed nearly identical results. However, there was a significant decrease in taxi trips starting in March due to the covid pandemic and stay-at-home mandates. Consequently, the entirety of 2020 displayed much lower numbers than the previous years. However, in 2021, with the distribution of the covid vaccine and the start of a return to normalcy, the number of trips shows an upward trend. . Question 2: How does taxi demand differ across different time categories? . &#39;&#39;&#39; * Late Night: This is the period between midnight and sunrise, usually from 12:00 AM to 5:59 AM. * Morning: This is the period after sunrise and before noon, usually from 6:00 AM to 11:59 AM. * Afternoon: This is the period between noon and evening, usually from 12:00 PM to 4:59 PM. * Evening: This is the period between late afternoon and late night, usually from 5:00 PM to 8:59 PM. * Night: This is the period between late evening and early morning, usually from 9:00 PM to 11:59 PM.&#39;&#39;&#39; from pyspark.sql.functions import hour, when # Categorizing the pickup_ET time in different time categories (5 in our case) spark_df_changed_casted_dataframe_f = df.withColumn(&#39;pickup_time_category&#39;, when((df.hour &gt;= 0) &amp; (df.hour &lt;= 5), &#39;Late Night&#39;) .when((df.hour &gt;= 6) &amp; (df.hour &lt;= 11), &#39;Morning&#39;) .when((df.hour &gt;= 12) &amp; (df.hour &lt;= 16), &#39;Afternoon&#39;) .when((df.hour &gt;= 17) &amp; (df.hour &lt;= 20), &#39;Evening&#39;) .otherwise(&#39;Night&#39;)) # Show the resulting dataframe spark_df_changed_casted_dataframe_f.select(&#39;pickup_ET&#39;, &#39;pickup_time_category&#39;).show(5) . +-+--+ | pickup_ET|pickup_time_category| +-+--+ |2020-12-31 19:43:30| Evening| |2020-12-31 19:15:48| Evening| |2020-12-31 19:19:57| Evening| |2020-12-31 19:44:58| Evening| |2020-12-31 19:06:11| Evening| +-+--+ only showing top 5 rows . spark_df_changed_casted_dataframe_f.cache() . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string, year: int, duration: double, pickup_ET: timestamp, dropoff_ET: timestamp, day_of_week: string, hour: int, pickup_time_category: string] . import matplotlib.pyplot as plt from pyspark.sql.functions import count # Calculating demand of number of rides through count by pickup time category pickup_demand = spark_df_changed_casted_dataframe_f.groupBy(&#39;pickup_time_category&#39;).agg(count(&#39;*&#39;).alias(&#39;demand&#39;)).orderBy(&#39;demand&#39;, ascending=False) # Displaying pickup demand in decreasing order print(&quot;Pickup Demand:&quot;) pickup_demand.show() . Pickup Demand: . [Stage 117:====================================================&gt;(227 + 3) / 230] . +--+--+ |pickup_time_category| demand| +--+--+ | Morning|76782422| | Afternoon|70561162| | Late Night|42614283| | Evening|37883222| | Night| 9096591| +--+--+ . . pickup_demand = spark_df_changed_casted_dataframe_f.groupBy(&#39;pickup_time_category&#39;).agg(count(&#39;*&#39;).alias(&#39;demand&#39;)).orderBy(&#39;demand&#39;, ascending=False) # Plot pie chart of demand by pickup time category plt.pie(pickup_demand.select(&#39;demand&#39;).rdd.flatMap(lambda x: x).collect(), labels=pickup_demand.select(&#39;pickup_time_category&#39;).rdd.flatMap(lambda x: x).collect(), autopct=&#39;%1.1f%%&#39;, startangle=90) plt.axis(&#39;equal&#39;) plt.title(&#39;Demand for different time categories (pickup)&#39;) plt.show() . . Most of the Taxis were taken in Morning and Afternoon Time | Least Taxis were taken at night from: 9:00 PM to 11:59 PM. | . Question 3: What are the ten most frequently traveled routes for taxi rides in NYC? . Building on our findings about demand for different times, we want to look into locations and routes that lead to high revenue. . B = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load().select(&quot;zone_name&quot;, &quot;zone_id&quot;) A_with_zone = df.join(B, df.dropoff_location_id == B.zone_id, how=&quot;left&quot;).withColumn(&quot;dropoff_zone_name&quot;, B.zone_name) .drop(&quot;zone_id&quot;, &quot;zone_name&quot;).join(B, df.pickup_location_id == B.zone_id, how=&quot;left&quot;).withColumn(&quot;pickup_zone_name&quot;, B.zone_name).drop(&quot;zone_id&quot;, &quot;zone_name&quot;) . A_with_zone.show(5) . 23/04/30 23:19:26 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &#39;spark.sql.debug.maxToStringFields&#39;. [Stage 137:&gt; (0 + 1) / 1] . ++-+-++-+++++--+--+--++-+--+++-+-+--+-+-+--+-+--+--+ |vendor_id| pickup_datetime| dropoff_datetime|passenger_count|trip_distance|rate_code|store_and_fwd_flag|payment_type| fare_amount| extra| mta_tax| tip_amount|tolls_amount|imp_surcharge| airport_fee|total_amount|pickup_location_id|dropoff_location_id|year|duration| pickup_ET| dropoff_ET|day_of_week|hour| dropoff_zone_name| pickup_zone_name| ++-+-++-+++++--+--+--++-+--+++-+-+--+-+-+--+-+--+--+ | 1|2021-01-01 00:43:30|2021-01-01 01:11:06| 1| 14.700000000| 1| N| 1|42.000000000|0.500000000|0.500000000|8.650000000| 0E-9| 0.300000000|-999.000000000|51.950000000| 132| 165|2021| 27.6|2020-12-31 19:43:30|2020-12-31 20:11:06| Thursday| 19| Midwood| JFK Airport| | 1|2021-01-01 00:15:48|2021-01-01 00:31:01| 0| 10.600000000| 1| N| 1|29.000000000|0.500000000|0.500000000|6.050000000| 0E-9| 0.300000000|-999.000000000|36.350000000| 138| 132|2021| 15.22|2020-12-31 19:15:48|2020-12-31 19:31:01| Thursday| 19| JFK Airport|LaGuardia Airport| | 2|2021-01-01 00:19:57|2021-01-01 00:43:03| 3| 10.740000000| 1| N| 1|32.500000000|0.500000000|0.500000000|4.000000000| 0E-9| 0.300000000|-999.000000000|40.300000000| 264| 231|2021| 23.1|2020-12-31 19:19:57|2020-12-31 19:43:03| Thursday| 19|TriBeCa/Civic Center| null| | 2|2021-01-01 00:44:58|2021-01-01 01:07:41| 2| 5.850000000| 1| N| 1|21.500000000|0.500000000|0.500000000|5.060000000| 0E-9| 0.300000000|-999.000000000|30.360000000| 249| 238|2021| 22.72|2020-12-31 19:44:58|2020-12-31 20:07:41| Thursday| 19|Upper West Side N...| West Village| | 2|2021-01-01 00:06:11|2021-01-01 00:23:40| 1| 8.060000000| 1| N| 1|24.500000000|0.500000000|0.500000000|5.660000000| 0E-9| 0.300000000|-999.000000000|33.960000000| 75| 264|2021| 17.48|2020-12-31 19:06:11|2020-12-31 19:23:40| Thursday| 19| null|East Harlem South| ++-+-++-+++++--+--+--++-+--+++-+-+--+-+-+--+-+--+--+ only showing top 5 rows . . from pyspark.sql.functions import count, avg, expr from pyspark.sql.window import Window df_zone = A_with_zone.select(&quot;pickup_zone_name&quot;, &quot;dropoff_zone_name&quot;, &quot;fare_amount&quot;, &quot;pickup_datetime&quot;, &quot;dropoff_datetime&quot;, &quot;trip_distance&quot;) .filter((A_with_zone.pickup_zone_name.isNotNull()) &amp; (A_with_zone.dropoff_zone_name.isNotNull()) &amp; (A_with_zone.fare_amount.isNotNull()) &amp; (A_with_zone.pickup_datetime.isNotNull()) &amp; (A_with_zone.dropoff_datetime.isNotNull()) &amp; (A_with_zone.trip_distance.isNotNull())) #new column df_zone = df_zone.withColumn(&quot;duration_minutes&quot;, expr(&quot;(UNIX_TIMESTAMP(dropoff_datetime) - UNIX_TIMESTAMP(pickup_datetime))/60&quot;)) #groupby df_zone = df_zone.groupBy(&quot;pickup_zone_name&quot;, &quot;dropoff_zone_name&quot;) .agg(count(&quot;*&quot;).alias(&quot;trip_count&quot;), avg(&quot;fare_amount&quot;).alias(&quot;avg_fare_amount&quot;), avg(&quot;duration_minutes&quot;).alias(&quot;avg_duration_minutes&quot;), avg(&quot;trip_distance&quot;).alias(&quot;avg_trip_distance&quot;)) .orderBy(&quot;trip_count&quot;, ascending=False) .limit(10) df_zone.show() . [Stage 141:========================================&gt; (5 + 2) / 7] . +--+--+-++--+--+ | pickup_zone_name| dropoff_zone_name|trip_count|avg_fare_amount|avg_duration_minutes|avg_trip_distance| +--+--+-++--+--+ |Upper East Side S...|Upper East Side N...| 1536223|6.4637527170000| 6.567530657983904| 1.0572492410000| |Upper East Side N...|Upper East Side S...| 1301958|7.0365252410000| 7.903893942815366| 1.0562288340000| |Upper East Side N...|Upper East Side N...| 1230563|5.1022814760000| 4.508901779104363| 0.6337434900000| |Upper East Side S...|Upper East Side S...| 1166238|5.4694903610000| 5.212215145336262| 0.6620207370000| |Upper West Side S...|Upper West Side N...| 687418|5.4631325630000| 4.91102594539761| 0.8376672270000| |Upper West Side S...| Lincoln Square East| 667270|5.8577696130000| 5.711464999175746| 0.8722473960000| | Lincoln Square East|Upper West Side S...| 630715|6.1414540010000| 6.07700062627336| 0.9803069220000| |Upper East Side S...| Midtown East| 626600|6.9054814870000| 7.9227933822747065| 0.9700193270000| |Upper East Side S...| Midtown Center| 610173|7.9622868920000| 9.977792090658436| 1.0930640490000| |Upper West Side N...|Upper West Side S...| 596473|5.4437153570000| 4.925779764269852| 0.8031172580000| +--+--+-++--+--+ . . import matplotlib.pyplot as plt import pandas as pd pandas_df = df_zone.toPandas() pandas_df = pandas_df.set_index([&quot;pickup_zone_name&quot;, &quot;dropoff_zone_name&quot;]) pandas_df.plot(kind=&quot;bar&quot;, y=&quot;trip_count&quot;) plt.title(&quot;Top 10 Taxi Routes in NYC in 2022&quot;) plt.xlabel(&quot;Pickup-Dropoff Location&quot;) plt.ylabel(&quot;Number of Trips&quot;) plt.show() . . 5.b. Revenue . After identifying times and locations that have high taxi demand, we are switching gears to see what factors contribute to higher revenue. . Question 4(a): Which trip category produces the highest revenue: short-distance, medium-distance, or long-distance? . In this subsection of our Exploratory Data Analysis, we examined what types of trips lead to the highest revenue. . from pyspark.sql.functions import month, round, avg # Calculate revenue per trip for each month revenue_per_month_df = df.groupBy(month(&quot;pickup_datetime&quot;).alias(&quot;month&quot;)).agg(avg(&quot;fare_amount&quot;)).withColumnRenamed(&quot;avg(fare_amount)&quot;, &quot;revenue_per_trip&quot;) # Calculate number of trips in each month # trips_per_month_df = df_raw.groupBy(month(&quot;pickup_datetime&quot;).alias(&quot;month&quot;)).count().withColumnRenamed(&quot;count&quot;, &quot;num_trips&quot;) # Calculate revenue per trip and number of trips for each month # revenue_and_trips_per_month_df = revenue_per_month_df.join(trips_per_month_df, &quot;month&quot;).withColumn(&quot;revenue_per_trip_per_month&quot;, round(col(&quot;revenue_per_trip&quot;) / col(&quot;num_trips&quot;), 2)) # Sort results by month revenue_and_trips_per_month_df = revenue_per_month_df.orderBy(&quot;month&quot;) # Show results revenue_and_trips_per_month_df.show() . [Stage 147:===================================================&gt; (225 + 2) / 230] . +--+-+ |month|revenue_per_trip| +--+-+ | 1|12.1986250570000| | 2|12.3261965430000| | 3|12.6058248030000| | 4|12.8352320990000| | 5|13.1640045580000| | 6|13.2056539470000| | 7|13.1338113010000| | 8|13.2188672570000| | 9|13.3761508200000| | 10|13.2840054000000| | 11|13.1711842640000| | 12|13.1354568650000| +--+-+ . . Question 4(b): How much each type of trip (long, short, medium) makes per minute of trip? . Cost was calulated based on minute because it was thoght that in some location or some time short trips may take same time as long trip, and sometimes long trip may take too much time, so considering per minute makes the cost more easier to compare with different types of trip rather than comparing with the distance. . So from the below code we can see that overall long trip cost more per minute, then shorter one cost more and then medium one cost the least. . from pyspark.sql.functions import col, round, when, month, from_unixtime, unix_timestamp, round, avg, sort_array # Calculate trip distance bucket and pickup hour df_with_distance_bucket = df.withColumn(&quot;trip_distance_bucket&quot;, when(col(&quot;trip_distance&quot;) &lt; 3, &quot;short&quot;).when((col(&quot;trip_distance&quot;) &gt;= 3) &amp; (col(&quot;trip_distance&quot;) &lt; 6), &quot;medium&quot;).otherwise(&quot;long&quot;)) df_with_pickup_hour = df_with_distance_bucket.withColumn(&quot;pickup_hour&quot;, from_unixtime(unix_timestamp(col(&quot;pickup_datetime&quot;)), &quot;H&quot;)) # Calculate duration of trip in minutes df_with_duration = df_with_pickup_hour.withColumn(&quot;duration&quot;, (unix_timestamp(col(&quot;dropoff_datetime&quot;)) - unix_timestamp(col(&quot;pickup_datetime&quot;))) / 60) # Calculate revenue per minute of trip for each trip distance bucket and pickup hour revenue_per_minute_df = df_with_duration.groupBy(&quot;trip_distance_bucket&quot;, &quot;pickup_hour&quot;).agg(avg(&quot;fare_amount&quot;) / avg(&quot;duration&quot;)).withColumnRenamed(&quot;(avg(fare_amount) / avg(duration))&quot;, &quot;revenue_per_minute&quot;) revenue_per_minute_df = revenue_per_minute_df.orderBy(col(&quot;pickup_hour&quot;)) pivot_table = revenue_per_minute_df.groupBy(&quot;pickup_hour&quot;) .pivot(&quot;trip_distance_bucket&quot;, [&quot;short&quot;, &quot;medium&quot;, &quot;long&quot;]) .agg(round(avg(&quot;revenue_per_minute&quot;), 2)) .select(&quot;pickup_hour&quot;, &quot;short&quot;, &quot;medium&quot;, &quot;long&quot;) .orderBy(col(&quot;pickup_hour&quot;)) pivot_table.show(24) . [Stage 1279:====================================&gt; (8 + 4) / 12] . +--+--++-+ |pickup_hour|short|medium|long| +--+--++-+ | 0| 0.98| 0.92|1.28| | 1| 1.04| 0.96|1.29| | 10| 0.83| 0.8|1.08| | 11| 0.81| 0.79|1.08| | 12| 0.81| 0.79|1.09| | 13| 0.81| 0.8|1.05| | 14| 0.81| 0.77|0.96| | 15| 0.81| 0.76| 0.9| | 16| 0.82| 0.76|0.89| | 17| 0.82| 0.76|0.91| | 18| 0.83| 0.78|1.01| | 19| 0.86| 0.81|1.11| | 2| 1.05| 1.0|1.26| | 20| 0.88| 0.84|1.18| | 21| 0.9| 0.86|1.19| | 22| 0.91| 0.87| 1.2| | 23| 0.94| 0.89|1.23| | 3| 1.07| 1.02| 1.3| | 4| 1.15| 1.06|1.43| | 5| 1.33| 1.09| 1.5| | 6| 1.08| 1.01|1.26| | 7| 0.93| 0.88|1.07| | 8| 0.84| 0.81|1.02| | 9| 0.81| 0.8|1.06| +--+--++-+ . . import pandas as pd import matplotlib.pyplot as plt # Convert Spark DataFrame to Pandas DataFrame pandas_df = pivot_table.toPandas() # Set the index to pickup_hour pandas_df.set_index(&#39;pickup_hour&#39;, inplace=True) # Plot the line chart pandas_df.plot.line() # Set the labels for the x and y axes plt.xlabel(&#39;Pickup Hour&#39;) plt.ylabel(&#39;Revenue per Minute&#39;) # Set the title for the chart plt.title(&#39;Revenue per Minute by Trip Distance&#39;) # Show the chart plt.show() . . Question 5: When do the most costly or least expensive trips usually take place? . We also analyzed the cost of trips based on time of day, day of the week and month. . Cost of Trips Based on Time of Day . from pyspark.sql.functions import hour, col hourly_avg_fare = df.groupBy(hour(&quot;pickup_datetime&quot;).alias(&quot;pickup_hour&quot;)).avg(&quot;fare_amount&quot;) . most_costly_hours = hourly_avg_fare.orderBy(col(&quot;avg(fare_amount)&quot;).desc()) most_costly_hours.show(12) . [Stage 150:====================================================&gt;(229 + 1) / 230] . +--+-+ |pickup_hour|avg(fare_amount)| +--+-+ | 5|16.5496947010000| | 4|15.1665596820000| | 16|13.7225114830000| | 0|13.5548416160000| | 23|13.5405844990000| | 15|13.5048257380000| | 14|13.4591031480000| | 17|13.2227479380000| | 6|13.1736325640000| | 22|13.1476809780000| | 13|13.0749049110000| | 1|12.9630381250000| +--+-+ only showing top 12 rows . . The table above indicates that the hours with the lowest average fare amounts are generally in the late night (4-6am), and during the afternoon (1-4pm). . least_costly_hours = hourly_avg_fare.orderBy(col(&quot;avg(fare_amount)&quot;).asc()) least_costly_hours.show(12) . [Stage 153:====================================================&gt;(227 + 2) / 230] . +--+-+ |pickup_hour|avg(fare_amount)| +--+-+ | 19|12.2015580750000| | 7|12.2539309220000| | 8|12.2789897940000| | 2|12.3477758000000| | 9|12.3588731210000| | 18|12.5260646320000| | 11|12.5427978560000| | 10|12.5480344350000| | 20|12.5684936080000| | 12|12.7564840840000| | 3|12.8283908540000| | 21|12.8956748400000| +--+-+ only showing top 12 rows . . The table above indicates that the hours with the highest average fare amounts are generally in the evening and late night. However, the prices are fairly consistent. . Cost of Trips Based on Day of the Week . from pyspark.sql.functions import dayofweek, col dayofweek_avg_fare = df.groupBy(dayofweek(&quot;pickup_datetime&quot;).alias(&quot;pickup_dow&quot;)).avg(&quot;fare_amount&quot;) . most_costly_days = dayofweek_avg_fare.orderBy(col(&quot;avg(fare_amount)&quot;).desc()) most_costly_days.show(7) . [Stage 156:====================================================&gt;(229 + 1) / 230] . +-+-+ |pickup_dow|avg(fare_amount)| +-+-+ | 5|13.2198030620000| | 1|13.1577534700000| | 6|13.0683533850000| | 2|13.0171181340000| | 4|12.9279615040000| | 3|12.7937460980000| | 7|12.4044454370000| +-+-+ . . Cost of Trips Based on Month . from pyspark.sql.functions import month, avg cost_by_month = df.groupBy(month(&quot;pickup_datetime&quot;).alias(&quot;pickup_month&quot;)).agg(avg(&quot;fare_amount&quot;).alias(&quot;avg_fare&quot;)).orderBy(&quot;pickup_month&quot;) cost_by_month.show() . [Stage 159:====================================================&gt;(226 + 2) / 230] . ++-+ |pickup_month| avg_fare| ++-+ | 1|12.1986250570000| | 2|12.3261965430000| | 3|12.6058248030000| | 4|12.8352320990000| | 5|13.1640045580000| | 6|13.2056539470000| | 7|13.1338113010000| | 8|13.2188672570000| | 9|13.3761508200000| | 10|13.2840054000000| | 11|13.1711842640000| | 12|13.1354568650000| ++-+ . . Question 6: What proportion of all taxi trips are airport taxi trips, and what percentage of revenue do these trips generate? . from pyspark.sql.functions import lower taxi_zone_geom = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load() taxi_zone_geom = taxi_zone_geom.filter(lower(col(&quot;zone_name&quot;)).like(&quot;%airport%&quot;)).withColumn(&quot;zone_id_int&quot;, taxi_zone_geom[&quot;zone_id&quot;].cast(&quot;int&quot;)).orderBy(&quot;zone_id_int&quot;) taxi_zone_geom.show() . [Stage 2093:&gt; (0 + 1) / 1] . +-+--+-+--+--+ |zone_id| zone_name|borough| zone_geom|zone_id_int| +-+--+-+--+--+ | 1| Newark Airport| EWR|POLYGON((-74.1856...| 1| | 132| JFK Airport| Queens|MULTIPOLYGON(((-7...| 132| | 138|LaGuardia Airport| Queens|MULTIPOLYGON(((-7...| 138| +-+--+-+--+--+ . . !pip install folium . Requirement already satisfied: folium in /opt/conda/miniconda3/lib/python3.8/site-packages (0.14.0) Requirement already satisfied: jinja2&gt;=2.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from folium) (3.0.3) Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from folium) (1.19.5) Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from folium) (2.25.1) Requirement already satisfied: branca&gt;=0.6.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from folium) (0.6.0) Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from jinja2&gt;=2.9-&gt;folium) (2.1.2) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-&gt;folium) (1.25.11) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-&gt;folium) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-&gt;folium) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests-&gt;folium) (2022.12.7) WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv . import folium from shapely import wkt import pandas as pd # Convert the Spark DataFrame to a Pandas DataFrame taxi_zone_geom_pd = taxi_zone_geom.toPandas() # Create a map centered on New York City nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=10) # Extract the coordinates for each airport zone and add them as polygons to the map for _, row in taxi_zone_geom_pd.iterrows(): zone_geom = wkt.loads(row[&quot;zone_geom&quot;]) if zone_geom.geom_type == &#39;Polygon&#39;: polygons = [zone_geom] elif zone_geom.geom_type == &#39;MultiPolygon&#39;: polygons = list(zone_geom) for polygon in polygons: coordinates = [] for point in polygon.exterior.coords: coordinates.append((point[1], point[0])) folium.Polygon( locations=coordinates, color=&#39;blue&#39;, fill=True, fill_color=&#39;blue&#39;, fill_opacity=0.2, tooltip=row[&#39;zone_name&#39;] ).add_to(nyc_map) # Display the map nyc_map . /tmp/ipykernel_13493/394209981.py:18: ShapelyDeprecationWarning: __len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the number of parts of a multi-part geometry. polygons = list(zone_geom) /tmp/ipykernel_13493/394209981.py:18: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry. polygons = list(zone_geom) . Make this Notebook Trusted to load map: File -&gt; Trust Notebook . Question 7(a): Which pickup or dropoff locations generate the highest revenue? . from pyspark.sql.functions import sum # Read the taxi data into a DataFrame taxi_data = df # Group the data by pickup and dropoff location and sum the total revenue for each group revenue_by_location = taxi_data.groupBy(&quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot;).agg(sum(&quot;total_amount&quot;).alias(&quot;total_revenue&quot;)) # Join the location IDs with their corresponding names from the taxi zone geometry table pickup_locations = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load() pickup_locations = pickup_locations.selectExpr(&quot;zone_id AS PULocationID&quot;, &quot;zone_name AS pickup_name&quot;, &quot;zone_geom AS pickup_zone_geom&quot;) dropoff_locations = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load() dropoff_locations = dropoff_locations.selectExpr(&quot;zone_id AS DOLocationID&quot;, &quot;zone_name AS dropoff_name&quot;, &quot;zone_geom AS dropoff_zone_geom&quot;) revenue_by_location = revenue_by_location.join(pickup_locations, revenue_by_location.pickup_location_id == pickup_locations.PULocationID) .join(dropoff_locations, revenue_by_location.dropoff_location_id == dropoff_locations.DOLocationID) .drop(&quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot;) # Show the top 10 locations by total revenue revenue_by_location.orderBy(&quot;total_revenue&quot;, ascending=False).show(10) . [Stage 2121:============================================&gt; (8 + 2) / 10] . +++--+--++--+--+ | total_revenue|PULocationID| pickup_name| pickup_zone_geom|DOLocationID| dropoff_name| dropoff_zone_geom| +++--+--++--+--+ |17419717.680000000| 138| LaGuardia Airport|MULTIPOLYGON(((-7...| 230|Times Sq/Theatre ...|POLYGON((-73.9831...| |17172582.880000000| 132| JFK Airport|MULTIPOLYGON(((-7...| 230|Times Sq/Theatre ...|POLYGON((-73.9831...| |15734735.880000000| 237|Upper East Side S...|POLYGON((-73.9656...| 236|Upper East Side N...|POLYGON((-73.9572...| |14132190.440000000| 236|Upper East Side N...|POLYGON((-73.9572...| 237|Upper East Side S...|POLYGON((-73.9656...| |11946245.300000000| 138| LaGuardia Airport|MULTIPOLYGON(((-7...| 161| Midtown Center|POLYGON((-73.9748...| |11917091.230000000| 230|Times Sq/Theatre ...|POLYGON((-73.9831...| 138| LaGuardia Airport|MULTIPOLYGON(((-7...| |11263123.430000000| 132| JFK Airport|MULTIPOLYGON(((-7...| 48| Clinton East|POLYGON((-73.9907...| |11050990.500000000| 138| LaGuardia Airport|MULTIPOLYGON(((-7...| 162| Midtown East|POLYGON((-73.9707...| |10547087.040000000| 236|Upper East Side N...|POLYGON((-73.9572...| 236|Upper East Side N...|POLYGON((-73.9572...| |10461780.240000000| 132| JFK Airport|MULTIPOLYGON(((-7...| 164| Midtown South|POLYGON((-73.9831...| +++--+--++--+--+ only showing top 10 rows . . import folium import shapely.wkt import geopandas as gpd #Show the top 10 locations top_10_revenue_locations = revenue_by_location.orderBy(&quot;total_revenue&quot;, ascending=False).head(10) #Initialize the map nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12) # Function to create a GeoJSON object for a polygon def create_geojson_polygon(polygon_wkt): polygon = shapely.wkt.loads(polygon_wkt) return folium.GeoJson(gpd.GeoSeries(polygon).__geo_interface__) #Plot the top 10 locations on the map for row in top_10_revenue_locations: #Add the pickup pickup_geojson = create_geojson_polygon(row[&quot;pickup_zone_geom&quot;]) pickup_popup = f&quot;Pickup: {row[&#39;pickup_name&#39;]} (Revenue: ${row[&#39;total_revenue&#39;]:.2f})&quot; pickup_geojson.add_child(folium.Popup(pickup_popup)).add_to(nyc_map) #Add the dropoff dropoff_geojson = create_geojson_polygon(row[&quot;dropoff_zone_geom&quot;]) dropoff_popup = f&quot;Dropoff: {row[&#39;dropoff_name&#39;]} (Revenue: ${row[&#39;total_revenue&#39;]:.2f})&quot; dropoff_geojson.add_child(folium.Popup(dropoff_popup)).add_to(nyc_map) #Display nyc_map . . Make this Notebook Trusted to load map: File -&gt; Trust Notebook from IPython.display import Image . Image(filename=&#39;images/image.png&#39;) . Question 7(b): Which Location cost the most per mile each hour of the day in different months? . Below code is to find the cost during a particular hour of the month, which location cost the most. The reason for creating def function for it was because we speculated that some months might see different expensive location on different hours. so whichever month we are interested in we may can put in and get the result. For example, month March and month December shows different location for different hours. In March, location which cost the most per mile per hour at 1 am was near &#39;Time Square&#39;. For December the place cost most per mile per hour at 1 am was &#39;Boerum Hill&#39;. So 2 months had different location whihc generated the most cost per mile at the same time of day . from pyspark.sql.functions import year, month, to_date def get_best_location_by_hour(df, pickup_month): # Convert the pickup datetime column to a date df = df.withColumn(&quot;pickup_date&quot;, to_date(df.pickup_datetime)) # Filter by pickup year and month filtered_df = df.filter((month(df.pickup_date) == pickup_month)) # Group by pickup location and hour location_hour_df = filtered_df.groupBy(&quot;pickup_location_id&quot;, &quot;hour&quot;).agg( {&quot;fare_amount&quot;: &quot;sum&quot;, &quot;trip_distance&quot;: &quot;sum&quot;, &quot;passenger_count&quot;: &quot;sum&quot;, &quot;pickup_datetime&quot;: &quot;count&quot;} ).withColumnRenamed(&quot;sum(fare_amount)&quot;, &quot;fare_amount&quot;).withColumnRenamed(&quot;sum(trip_distance)&quot;, &quot;trip_distance&quot;) .withColumnRenamed(&quot;sum(passenger_count)&quot;, &quot;passenger_count&quot;) .withColumnRenamed(&quot;count(pickup_datetime)&quot;, &quot;num_trips&quot;) # Calculate revenue per trip and per mile revenue_per_trip_df = location_hour_df.withColumn(&quot;revenue_per_trip&quot;, location_hour_df.fare_amount / location_hour_df.num_trips) revenue_per_mile_df = location_hour_df.withColumn(&quot;revenue_per_mile&quot;, location_hour_df.fare_amount / location_hour_df.trip_distance) # Find the location with the highest revenue per mile for each hour best_location_by_hour_df = revenue_per_trip_df.join(revenue_per_mile_df, on=[&quot;pickup_location_id&quot;, &quot;hour&quot;]) .groupBy(&quot;hour&quot;).agg( {&quot;revenue_per_trip&quot;: &quot;avg&quot;, &quot;pickup_location_id&quot;: &quot;first&quot;, &quot;revenue_per_mile&quot;: &quot;avg&quot;} ).orderBy(&quot;hour&quot;) return best_location_by_hour_df # return best_location_by_hour_df . best_loc_in_x_month = get_best_location_by_hour(df, 3) B = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load().select(&quot;zone_name&quot;, &quot;zone_id&quot;) A_with_zone = best_loc_in_x_month.join(B, (best_loc_in_x_month[&#39;first(pickup_location_id)&#39;] == B.zone_id), how=&quot;left&quot;).withColumn(&quot;pickup_zone_name&quot;, B.zone_name).drop(&quot;zone_id&quot;, &quot;zone_name&quot;) A_with_zone.orderBy(&quot;hour&quot;).show(24) . [Stage 1294:========================================&gt; (8 + 3) / 11] . +-+++-+--+ |hour|avg(revenue_per_mile)|avg(revenue_per_trip)|first(pickup_location_id)| pickup_zone_name| +-+++-+--+ | 0| 3.6071360000| 22.0128615650000| 73| East Flushing| | 1| 3.7167900000| 25.1772937010000| 230|Times Sq/Theatre ...| | 2| 3.7376970000| 26.9930079920000| 13| Battery Park City| | 3| 3.9939360000| 27.2469502200000| 113|Greenwich Village...| | 4| 4.1392920000| 24.2483896790000| 152| Manhattanville| | 5| 4.1292230000| 22.9201988920000| 224|Stuy Town/Peter C...| | 6| 4.0952650000| 23.5621748130000| 174| Norwood| | 7| 4.0795830000| 22.4115058850000| 254|Williamsbridge/Ol...| | 8| 4.1361110000| 22.7225837270000| 83| Elmhurst/Maspeth| | 9| 4.4325070000| 23.2106507860000| 113|Greenwich Village...| | 10| 4.2754650000| 24.4419844300000| 14| Bay Ridge| | 11| 4.2877400000| 25.2556868280000| 33| Brooklyn Heights| | 12| 4.4836920000| 27.4382130120000| 246|West Chelsea/Huds...| | 13| 4.3031660000| 24.5958372810000| 265| null| | 14| 4.2603740000| 23.4114182890000| 16| Bayside| | 15| 4.2637190000| 23.2635202520000| 140| Lenox Hill East| | 16| 3.9597600000| 24.2184612740000| 66| DUMBO/Vinegar Hill| | 17| 4.4763190000| 22.8459577030000| 138| LaGuardia Airport| | 18| 4.1770650000| 24.3352750540000| 61| Crown Heights North| | 19| 4.3354690000| 22.0115520580000| 140| Lenox Hill East| | 20| 4.3472940000| 21.3567342690000| 117| Hammels/Arverne| | 21| 17.2021770000| 21.4533527800000| 87|Financial Distric...| | 22| 5.8810730000| 19.8293350500000| 249| West Village| | 23| 20.5301470000| 20.4168576460000| 223| Steinway| +-+++-+--+ . . best_loc_in_x_month = get_best_location_by_hour(df, 12) B = spark.read.format(&quot;bigquery&quot;).option(&quot;table&quot;, &quot;bigquery-public-data.new_york_taxi_trips.taxi_zone_geom&quot;).load().select(&quot;zone_name&quot;, &quot;zone_id&quot;) A_with_zone = best_loc_in_x_month.join(B, (best_loc_in_x_month[&#39;first(pickup_location_id)&#39;] == B.zone_id), how=&quot;left&quot;).withColumn(&quot;pickup_zone_name&quot;, B.zone_name).drop(&quot;zone_id&quot;, &quot;zone_name&quot;) A_with_zone.orderBy(&quot;hour&quot;).show(24) . [Stage 1304:=================================================&gt; (10 + 1) / 11] . +-+++-+--+ |hour|avg(revenue_per_mile)|avg(revenue_per_trip)|first(pickup_location_id)| pickup_zone_name| +-+++-+--+ | 0| 4.4413530000| 26.3011313740000| 209| Seaport| | 1| 3.8153360000| 27.5981832150000| 25| Boerum Hill| | 2| 4.0594810000| 26.6663061000000| 13| Battery Park City| | 3| 4.0988370000| 24.9411265850000| 135| Kew Gardens Hills| | 4| 4.1116680000| 24.7182936980000| 113|Greenwich Village...| | 5| 4.1597260000| 23.8086866720000| 89|Flatbush/Ditmas Park| | 6| 4.1781250000| 24.8906365660000| 75| East Harlem South| | 7| 4.1562500000| 24.6282581640000| 17| Bedford| | 8| 4.3369320000| 24.1291749610000| 155|Marine Park/Mill ...| | 9| 4.3443550000| 24.6035097240000| 265| null| | 10| 4.4141220000| 24.8711955350000| 14| Bay Ridge| | 11| 4.6034140000| 26.9579387780000| 124| Howard Beach| | 12| 4.6075950000| 26.4553612890000| 134| Kew Gardens| | 13| 4.3872870000| 25.4452079070000| 99| Freshkills Park| | 14| 5.4221040000| 24.5527099210000| 40| Carroll Gardens| | 15| 4.2521170000| 23.9861812460000| 127| Inwood| | 16| 5.7496390000| 25.0294353530000| 209| Seaport| | 17| 4.0782220000| 24.1617298080000| 247| West Concourse| | 18| 4.9091950000| 24.4563556860000| 196| Rego Park| | 19| 4.2460210000| 23.3190398400000| 75| East Harlem South| | 20| 6.0426140000| 21.6948185220000| 74| East Harlem North| | 21| 4.1824460000| 20.0469317040000| 87|Financial Distric...| | 22| 4.4939010000| 21.2333006540000| 249| West Village| | 23| 12.8383230000| 23.1053412370000| 141| Lenox Hill West| +-+++-+--+ . . Question 8: What and where do people give the highest percentage of tips? . As tips account for 16% of the total price for taxi trips on average, we checked if there&#39;s a time in a day when people usually tip more generously. Also, we identified pickup and dropoff locations where customers are willing to give higher percentage of tips. . from pyspark.sql.functions import round A_with_zone = A_with_zone.withColumn(&#39;tip_percentage&#39;,round(A_with_zone[&#39;tip_amount&#39;]/A_with_zone[&#39;total_amount&#39;]*100,2)) . df_credit = A_with_zone.where(col(&#39;payment_type&#39;)==&#39;1&#39;) . df_tip_mean = df_credit.groupBy(&#39;hour&#39;).agg({&#39;tip_percentage&#39;:&quot;mean&quot;}).orderBy(&#39;hour&#39;) . pd_df_tip_mean = df_tip_mean.toPandas() . . plt.bar(pd_df_tip_mean[&#39;hour&#39;], pd_df_tip_mean[&#39;avg(tip_percentage)&#39;]) plt.title(&#39;Mean Tip Percentage by Hour&#39;) plt.xlabel(&#39;Hour&#39;) plt.ylabel(&#39;Mean Tip Percentage&#39;) plt.show() . from pyspark.sql.functions import desc loc_tip_mean = df_credit.groupBy(&#39;pickup_zone_name&#39;).agg({&#39;tip_percentage&#39;:&quot;mean&quot;}).orderBy(desc(&#39;avg(tip_percentage)&#39;)) . pd_loc_tip_mean = loc_tip_mean.toPandas() . . pd_loc_tip_mean[&#39;avg(tip_percentage)&#39;] = pd_loc_tip_mean[&#39;avg(tip_percentage)&#39;].astype(&#39;float&#39;) top_10 = pd_loc_tip_mean.nlargest(10, &#39;avg(tip_percentage)&#39;) plt.bar(top_10[&#39;pickup_zone_name&#39;], top_10[&#39;avg(tip_percentage)&#39;]) plt.title(&#39;Mean Tip Percentage by Pickup Location&#39;) plt.xlabel(&#39;Pickup Location&#39;) plt.xticks(rotation=90) plt.ylabel(&#39;Mean Tip Percentage&#39;) plt.show() . loc_dropoff_tip_mean = df_credit.groupBy(&#39;dropoff_zone_name&#39;).agg({&#39;tip_percentage&#39;:&quot;mean&quot;}).orderBy(desc(&#39;avg(tip_percentage)&#39;)) . pd_loc_dropoff_tip_mean = loc_dropoff_tip_mean.toPandas() . . pd_loc_dropoff_tip_mean[&#39;avg(tip_percentage)&#39;] = pd_loc_dropoff_tip_mean[&#39;avg(tip_percentage)&#39;].astype(&#39;float&#39;) top_10 = pd_loc_dropoff_tip_mean.nlargest(10, &#39;avg(tip_percentage)&#39;) plt.bar(top_10[&#39;dropoff_zone_name&#39;], top_10[&#39;avg(tip_percentage)&#39;]) plt.title(&#39;Mean Tip Percentage by Dropoff Location&#39;) plt.xlabel(&#39;Dropoff Location&#39;) plt.xticks(rotation=90) plt.ylabel(&#39;Mean Tip Percentage&#39;) plt.show() . Based on the two graphs above, we conclude that Carroll Gardens, LaGuardia Airport, and Cobble Hill are the three locations (no matter whether pickup or dropoff location) where people tend to pay the highest percentage of tips. To motivate drivers and help them generate more revenue, these three locations would be recommended for drivers to pick up customers. . # df.write.format(&quot;parquet&quot;) # .option(&quot;header&quot;, &quot;True&quot;) # .mode(&quot;overwrite&quot;) # .save(&quot;gs://is843-team6/notebooks/jupyter/df.parquet&quot;) . . 6. Machine Learning for Taxi Price Prediction . 6.a. Required Data-Preprocessing : . spark_df = spark.read.format(&quot;parquet&quot;).load(&quot;gs://is843-team6/notebooks/jupyter/df_sample.parquet&quot;) . spark_df_changed_casted_dataframe_f = spark_df.sample(False,fraction = .0005, seed =843) # randomly selecting a sample from the spark_df DataFrame to perform Ml on. . spark_df_changed_casted_dataframe_f # name of the dataframe . DataFrame[vendor_id: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: bigint, trip_distance: decimal(38,9), rate_code: string, store_and_fwd_flag: string, payment_type: string, fare_amount: decimal(38,9), extra: decimal(38,9), mta_tax: decimal(38,9), tip_amount: decimal(38,9), tolls_amount: decimal(38,9), imp_surcharge: decimal(38,9), airport_fee: decimal(38,9), total_amount: decimal(38,9), pickup_location_id: string, dropoff_location_id: string, year: int, duration: double, speed: double, pickup_ET: timestamp, dropoff_ET: timestamp, day_of_week: string, hour: int] . from pyspark.sql.functions import col spark_df_changed_casted_dataframe_f = spark_df_changed_casted_dataframe_f.withColumn(&quot;rate_code&quot;, col(&quot;rate_code&quot;).cast(&quot;int&quot;)) .withColumn(&quot;extra&quot;, col(&quot;extra&quot;).cast(&quot;double&quot;)) .withColumn(&quot;mta_tax&quot;, col(&quot;mta_tax&quot;).cast(&quot;double&quot;)) .withColumn(&quot;tip_amount&quot;, col(&quot;tip_amount&quot;).cast(&quot;double&quot;)) .withColumn(&quot;tolls_amount&quot;, col(&quot;tolls_amount&quot;).cast(&quot;double&quot;)) .withColumn(&quot;imp_surcharge&quot;, col(&quot;imp_surcharge&quot;).cast(&quot;double&quot;)) .withColumn(&quot;airport_fee&quot;, col(&quot;airport_fee&quot;).cast(&quot;double&quot;)) .withColumn(&quot;total_amount&quot;, col(&quot;total_amount&quot;).cast(&quot;double&quot;)) .withColumn(&quot;passenger_count&quot;, col(&quot;passenger_count&quot;).cast(&quot;int&quot;)) .withColumn(&quot;trip_distance&quot;, col(&quot;trip_distance&quot;).cast(&quot;double&quot;)) .withColumn(&quot;payment_type&quot;, col(&quot;payment_type&quot;).cast(&quot;int&quot;)) .withColumn(&quot;fare_amount&quot;, col(&quot;fare_amount&quot;).cast(&quot;double&quot;)) .withColumn(&quot;duration&quot;, col(&quot;duration&quot;).cast(&quot;double&quot;)) .withColumn(&quot;vendor_id&quot;, col(&quot;vendor_id&quot;).cast(&quot;int&quot;)) .withColumn(&quot;rate_code&quot;, col(&quot;rate_code&quot;).cast(&quot;int&quot;)) . spark_df_changed_casted_dataframe_f.dtypes . [(&#39;vendor_id&#39;, &#39;int&#39;), (&#39;pickup_datetime&#39;, &#39;timestamp&#39;), (&#39;dropoff_datetime&#39;, &#39;timestamp&#39;), (&#39;passenger_count&#39;, &#39;int&#39;), (&#39;trip_distance&#39;, &#39;double&#39;), (&#39;rate_code&#39;, &#39;int&#39;), (&#39;store_and_fwd_flag&#39;, &#39;string&#39;), (&#39;payment_type&#39;, &#39;int&#39;), (&#39;fare_amount&#39;, &#39;double&#39;), (&#39;extra&#39;, &#39;double&#39;), (&#39;mta_tax&#39;, &#39;double&#39;), (&#39;tip_amount&#39;, &#39;double&#39;), (&#39;tolls_amount&#39;, &#39;double&#39;), (&#39;imp_surcharge&#39;, &#39;double&#39;), (&#39;airport_fee&#39;, &#39;double&#39;), (&#39;total_amount&#39;, &#39;double&#39;), (&#39;pickup_location_id&#39;, &#39;string&#39;), (&#39;dropoff_location_id&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;duration&#39;, &#39;double&#39;), (&#39;speed&#39;, &#39;double&#39;), (&#39;pickup_ET&#39;, &#39;timestamp&#39;), (&#39;dropoff_ET&#39;, &#39;timestamp&#39;), (&#39;day_of_week&#39;, &#39;string&#39;), (&#39;hour&#39;, &#39;int&#39;)] . &#39;&#39;&#39; * Late Night: This is the period between midnight and sunrise, usually from 12:00 AM to 5:59 AM. * Morning: This is the period after sunrise and before noon, usually from 6:00 AM to 11:59 AM. * Afternoon: This is the period between noon and evening, usually from 12:00 PM to 4:59 PM. * Evening: This is the period between late afternoon and late night, usually from 5:00 PM to 8:59 PM. * Night: This is the period between late evening and early morning, usually from 9:00 PM to 11:59 PM. &#39;&#39;&#39; from pyspark.sql.functions import hour, when # Add new columns with rider pickup hours spark_df_changed_casted_dataframe_f = spark_df_changed_casted_dataframe_f.withColumn(&#39;pickup_hour&#39;, hour(&#39;pickup_ET&#39;)) # Categorize the pickup time into 5 categories spark_df_changed_casted_dataframe_f = spark_df_changed_casted_dataframe_f.withColumn(&#39;pickup_time_category&#39;, when((spark_df_changed_casted_dataframe_f.pickup_hour &gt;= 0) &amp; (spark_df_changed_casted_dataframe_f.pickup_hour &lt;= 5), &#39;Late Night&#39;) .when((spark_df_changed_casted_dataframe_f.pickup_hour &gt;= 6) &amp; (spark_df_changed_casted_dataframe_f.pickup_hour &lt;= 11), &#39;Morning&#39;) .when((spark_df_changed_casted_dataframe_f.pickup_hour &gt;= 12) &amp; (spark_df_changed_casted_dataframe_f.pickup_hour &lt;= 16), &#39;Afternoon&#39;) .when((spark_df_changed_casted_dataframe_f.pickup_hour &gt;= 17) &amp; (spark_df_changed_casted_dataframe_f.pickup_hour &lt;= 20), &#39;Evening&#39;) .otherwise(&#39;Night&#39;)) # Show the resulting dataframe spark_df_changed_casted_dataframe_f.select(&#39;pickup_ET&#39;, &#39;pickup_time_category&#39;).show() . [Stage 600:&gt; (0 + 1) / 1] . +-+--+ | pickup_ET|pickup_time_category| +-+--+ |2020-09-27 08:00:14| Morning| |2020-09-12 09:29:42| Morning| |2020-12-12 08:50:33| Morning| |2021-11-29 04:23:54| Late Night| |2021-03-24 14:38:42| Afternoon| |2018-12-31 06:40:50| Morning| |2019-02-14 15:52:37| Afternoon| |2019-02-12 11:42:04| Morning| |2019-04-06 12:00:49| Afternoon| |2018-03-09 05:25:12| Late Night| |2018-01-26 07:52:31| Morning| |2018-11-03 06:45:50| Morning| |2018-10-11 04:34:24| Late Night| |2018-11-08 19:30:06| Evening| |2018-06-23 13:40:06| Afternoon| |2019-02-23 17:48:46| Evening| |2019-01-28 08:02:35| Morning| |2019-05-20 12:39:31| Afternoon| |2018-07-23 15:12:59| Afternoon| |2018-04-23 11:07:08| Morning| +-+--+ only showing top 20 rows . . spark_df_changed_casted_dataframe_f.cache() . DataFrame[vendor_id: int, pickup_datetime: timestamp, dropoff_datetime: timestamp, passenger_count: int, trip_distance: double, rate_code: int, store_and_fwd_flag: string, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, imp_surcharge: double, airport_fee: double, total_amount: double, pickup_location_id: string, dropoff_location_id: string, year: int, duration: double, speed: double, pickup_ET: timestamp, dropoff_ET: timestamp, day_of_week: string, hour: int, pickup_hour: int, pickup_time_category: string] . 6.b. Required Feature Engineering: . 6.b.1. Handling Categorical Features in Data: . When we worked with a large dataset, we encountered several columns with repeated or less unique categorical values. To handle such features, we converted them to string type and then numerically encoded them using a technique like String Indexer. This approach helped to improve the performance of the model and reduce the computational complexity. . spark_df_changed_casted_dataframe_f.dtypes . [(&#39;vendor_id&#39;, &#39;int&#39;), (&#39;pickup_datetime&#39;, &#39;timestamp&#39;), (&#39;dropoff_datetime&#39;, &#39;timestamp&#39;), (&#39;passenger_count&#39;, &#39;int&#39;), (&#39;trip_distance&#39;, &#39;double&#39;), (&#39;rate_code&#39;, &#39;int&#39;), (&#39;store_and_fwd_flag&#39;, &#39;string&#39;), (&#39;payment_type&#39;, &#39;int&#39;), (&#39;fare_amount&#39;, &#39;double&#39;), (&#39;extra&#39;, &#39;double&#39;), (&#39;mta_tax&#39;, &#39;double&#39;), (&#39;tip_amount&#39;, &#39;double&#39;), (&#39;tolls_amount&#39;, &#39;double&#39;), (&#39;imp_surcharge&#39;, &#39;double&#39;), (&#39;airport_fee&#39;, &#39;double&#39;), (&#39;total_amount&#39;, &#39;double&#39;), (&#39;pickup_location_id&#39;, &#39;string&#39;), (&#39;dropoff_location_id&#39;, &#39;string&#39;), (&#39;year&#39;, &#39;int&#39;), (&#39;duration&#39;, &#39;double&#39;), (&#39;speed&#39;, &#39;double&#39;), (&#39;pickup_ET&#39;, &#39;timestamp&#39;), (&#39;dropoff_ET&#39;, &#39;timestamp&#39;), (&#39;day_of_week&#39;, &#39;string&#39;), (&#39;hour&#39;, &#39;int&#39;), (&#39;pickup_hour&#39;, &#39;int&#39;), (&#39;pickup_time_category&#39;, &#39;string&#39;)] . from pyspark.ml.feature import StringIndexer # Note: In pyspark String Indexer documentation, StringIndexer transformer automatically casts numeric columns to strings before indexing the value using the desired: stringOrderType (in our case: frequencyAsc ) indexer = StringIndexer( inputCols=[&quot;vendor_id&quot;, &quot;rate_code&quot;, &quot;passenger_count&quot;, &quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot;, &#39;payment_type&#39;, &#39;pickup_time_category&#39;, &#39;pickup_hour&#39;, &#39;day_of_week&#39;, &#39;mta_tax&#39;,&#39;imp_surcharge&#39;, &#39;year&#39;], # output form of string indexer outputCols=[&quot;vendor_id_indexed&quot;,&quot;rate_code_indexed&quot;, &quot;passenger_count_indexed&quot;, &quot;pickup_location_id_indexed&quot;, &quot;dropoff_location_id_indexed&quot;, &quot;payment_type_indexed&quot;, &quot;pickup_time_category_indexed&quot;, &quot;pickup_hour_indexed&quot;, &quot;day_of_week_indexed&quot;, &quot;mta_tax_indexed&quot;,&quot;imp_surcharge_indexed&quot;, &quot;year_indexed&quot;], stringOrderType=&#39;frequencyDesc&#39; # Note: In string indexer, default 0 is assignned to maximum frequency element: useful if there are a small number of unique values in a column and some values occur much more frequently than others, this also Reduces memory usage for ML algorithms ) spark_df_changed_casted_dataframe_f = indexer.fit(spark_df_changed_casted_dataframe_f).transform(spark_df_changed_casted_dataframe_f) # We can try: stringOrderType=&#39;frequencyDesc&#39;stringOrderType=&#39;frequencyAsc&#39; . . We perform one-hot encoding on categorical columns after string indexing to create binary features. The reason for this is that one-hot encoding is applied to columns where the number of unique values is relatively small, but each value is equally important. This allows our model to treat each value in the column as a separate independent variable, rather than assigning an arbitrary numerical value to each value in the column, which can introduce bias in the case of string indexing. . Note: It is important to note that it is not necessary to apply one-hot encoding to every column after string indexing. It should only be applied to columns where we feel that the order of the encoding by string indexing should not bias our regression model. . spark_df_changed_casted_dataframe_f.printSchema() . root |-- vendor_id: integer (nullable = true) |-- pickup_datetime: timestamp (nullable = true) |-- dropoff_datetime: timestamp (nullable = true) |-- passenger_count: integer (nullable = true) |-- trip_distance: double (nullable = true) |-- rate_code: integer (nullable = true) |-- store_and_fwd_flag: string (nullable = true) |-- payment_type: integer (nullable = true) |-- fare_amount: double (nullable = true) |-- extra: double (nullable = true) |-- mta_tax: double (nullable = true) |-- tip_amount: double (nullable = true) |-- tolls_amount: double (nullable = true) |-- imp_surcharge: double (nullable = true) |-- airport_fee: double (nullable = true) |-- total_amount: double (nullable = true) |-- pickup_location_id: string (nullable = true) |-- dropoff_location_id: string (nullable = true) |-- year: integer (nullable = true) |-- duration: double (nullable = true) |-- speed: double (nullable = true) |-- pickup_ET: timestamp (nullable = true) |-- dropoff_ET: timestamp (nullable = true) |-- day_of_week: string (nullable = true) |-- hour: integer (nullable = true) |-- pickup_hour: integer (nullable = true) |-- pickup_time_category: string (nullable = false) |-- vendor_id_indexed: double (nullable = false) |-- rate_code_indexed: double (nullable = false) |-- passenger_count_indexed: double (nullable = false) |-- pickup_location_id_indexed: double (nullable = false) |-- dropoff_location_id_indexed: double (nullable = false) |-- payment_type_indexed: double (nullable = false) |-- pickup_time_category_indexed: double (nullable = false) |-- pickup_hour_indexed: double (nullable = false) |-- day_of_week_indexed: double (nullable = false) |-- mta_tax_indexed: double (nullable = false) |-- imp_surcharge_indexed: double (nullable = false) |-- year_indexed: double (nullable = false) . from pyspark.ml.feature import OneHotEncoder # OneHotEncoder on indexed columns encoder = OneHotEncoder(inputCols=[&quot;vendor_id_indexed&quot;, &quot;rate_code_indexed&quot;,&#39;pickup_time_category_indexed&#39;,&#39;pickup_hour_indexed&#39;, &quot;day_of_week_indexed&quot;, &quot;year_indexed&quot;, &quot;passenger_count_indexed&quot;], outputCols=[&quot;vendor_id_encoded&quot;, &quot;rate_code_encoded&quot;,&#39;pickup_time_category_encoded&#39;,&#39;pickup_hour_encoded&#39;, &quot;day_of_week_encoded&quot;, &quot;year_encoded&quot;, &quot;passenger_count_encoded&quot;]) encoded = encoder.fit(spark_df_changed_casted_dataframe_f).transform(spark_df_changed_casted_dataframe_f) encoded.show(4) . ++-+-++-++++--+--+-+-++-+--+++-+-+--+--+-+-+--+-+--+--+--+--+--+--++--+-+-+-++++--+--+-+-+-+-+--+ |vendor_id| pickup_datetime| dropoff_datetime|passenger_count|trip_distance|rate_code|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|imp_surcharge|airport_fee|total_amount|pickup_location_id|dropoff_location_id|year|duration|speed| pickup_ET| dropoff_ET|day_of_week|hour|pickup_hour|pickup_time_category|vendor_id_indexed|rate_code_indexed|passenger_count_indexed|pickup_location_id_indexed|dropoff_location_id_indexed|payment_type_indexed|pickup_time_category_indexed|pickup_hour_indexed|day_of_week_indexed|mta_tax_indexed|imp_surcharge_indexed|year_indexed|vendor_id_encoded|rate_code_encoded|pickup_time_category_encoded|pickup_hour_encoded|day_of_week_encoded| year_encoded|passenger_count_encoded| ++-+-++-++++--+--+-+-++-+--+++-+-+--+--+-+-+--+-+--+--+--+--+--+--++--+-+-+-++++--+--+-+-+-+-+--+ | 1|2020-09-27 12:00:14|2020-09-27 12:04:47| 2| 0.9| 1| N| 1| 5.5| 2.5| 0.5| 1.75| 0.0| 0.3| -999.0| 10.55| 114| 234|2020| 4.55| 0.2|2020-09-27 08:00:14|2020-09-27 08:04:47| Sunday| 8| 8| Morning| 1.0| 0.0| 1.0| 6.0| 5.0| 0.0| 0.0| 3.0| 4.0| 0.0| 0.0| 3.0| (1,[],[])| (1,[0],[1.0])| (4,[0],[1.0])| (19,[3],[1.0])| (6,[4],[1.0])| (3,[],[])| (4,[1],[1.0])| | 2|2020-09-12 13:29:42|2020-09-12 13:34:19| 1| 1.05| 1| N| 2| 5.5| 0.0| 0.5| 0.0| 0.0| 0.3| -999.0| 8.8| 161| 164|2020| 4.62| 0.23|2020-09-12 09:29:42|2020-09-12 09:34:19| Saturday| 9| 9| Morning| 0.0| 0.0| 0.0| 8.0| 10.0| 1.0| 0.0| 10.0| 0.0| 0.0| 0.0| 3.0| (1,[0],[1.0])| (1,[0],[1.0])| (4,[0],[1.0])| (19,[10],[1.0])| (6,[0],[1.0])| (3,[],[])| (4,[0],[1.0])| | 2|2020-12-12 13:50:33|2020-12-12 14:09:37| 1| 3.23| 1| N| 1| 14.5| 0.0| 0.5| 1.0| 0.0| 0.3| -999.0| 18.8| 48| 79|2020| 19.07| 0.17|2020-12-12 08:50:33|2020-12-12 09:09:37| Saturday| 8| 8| Morning| 0.0| 0.0| 0.0| 10.0| 30.0| 0.0| 0.0| 3.0| 0.0| 0.0| 0.0| 3.0| (1,[0],[1.0])| (1,[0],[1.0])| (4,[0],[1.0])| (19,[3],[1.0])| (6,[0],[1.0])| (3,[],[])| (4,[0],[1.0])| | 1|2021-11-29 09:23:54|2021-11-29 09:55:51| 2| 16.9| 1| Y| 1| 46.5| 2.5| 0.5| 11.25| 6.55| 0.3| 0.0| 67.6| 261| 138|2021| 31.95| 0.53|2021-11-29 04:23:54|2021-11-29 04:55:51| Monday| 4| 4| Late Night| 1.0| 0.0| 1.0| 26.0| 18.0| 0.0| 2.0| 0.0| 1.0| 0.0| 0.0| 2.0| (1,[],[])| (1,[0],[1.0])| (4,[2],[1.0])| (19,[0],[1.0])| (6,[1],[1.0])|(3,[2],[1.0])| (4,[1],[1.0])| ++-+-++-++++--+--+-+-++-+--+++-+-+--+--+-+-+--+-+--+--+--+--+--+--++--+-+-+-++++--+--+-+-+-+-+--+ only showing top 4 rows . # drop_cols=[&#39;hour&#39;,&#39;rate_code&#39;,&#39;passenger_count&#39;, &#39;pickup_location_id&#39;,&#39;dropoff_location_id&#39;,&#39;payment_type&#39;, &#39;pickup_time_category&#39;, &#39;pickup_hour&#39;, &#39;day_of_week&#39;, &#39;mta_tax&#39;, &quot;vendor_id_indexed&quot;, &quot;rate_code_indexed&quot;, &quot;pickup_time_category_indexed&quot;, &quot;day_of_week_indexed&quot;, &quot;year_indexed&quot;, &quot;passenger_count_indexed&quot;] # Dropping 18 redundant columns drop_list= [&#39;pickup_time_category_indexed&#39;, &#39;year_indexed&#39;, &#39;pickup_hour&#39;, &#39;payment_type&#39;, &#39;pickup_location_id&#39;, &#39;imp_surcharge&#39;, &#39;passenger_count&#39;, &#39;day_of_week_indexed&#39;, &#39;pickup_time_category&#39;, &#39;mta_tax&#39;, &#39;hour&#39;,&#39;pickup_hour_indexed&#39;,&#39;day_of_week&#39;, &#39;rate_code_indexed&#39;, &#39;dropoff_location_id&#39;, &#39;rate_code&#39;, &#39;vendor_id_indexed&#39;, &#39;year&#39;, &#39;vendor_id&#39;] encoded = encoded.drop(*drop_list) . spark_df = encoded spark_df.cache() . DataFrame[pickup_datetime: timestamp, dropoff_datetime: timestamp, trip_distance: double, store_and_fwd_flag: string, fare_amount: double, extra: double, tip_amount: double, tolls_amount: double, airport_fee: double, total_amount: double, duration: double, speed: double, pickup_ET: timestamp, dropoff_ET: timestamp, passenger_count_indexed: double, pickup_location_id_indexed: double, dropoff_location_id_indexed: double, payment_type_indexed: double, mta_tax_indexed: double, imp_surcharge_indexed: double, vendor_id_encoded: vector, rate_code_encoded: vector, pickup_time_category_encoded: vector, pickup_hour_encoded: vector, day_of_week_encoded: vector, year_encoded: vector, passenger_count_encoded: vector] . 6.b.2. Handling Numerical Features in Data: . In order to prepare our numerical data for regression modeling, we perform feature scaling or normalization on the independent features. However, we do not scale or normalize the target variable since it represents the actual value we want to predict and is not used as a feature in the model. Scaling or normalizing the target variable may not significantly impact the model&#39;s performance and can even lead to a loss of accuracy. . Instead, we focus on scaling or normalizing the independent features in order to ensure that they are on a comparable scale and avoid issues such as feature dominance or bias. We leave the target variable as is to avoid any changes in its interpretation and to prevent potentially incorrect predictions. . spark_df.dtypes . [(&#39;pickup_datetime&#39;, &#39;timestamp&#39;), (&#39;dropoff_datetime&#39;, &#39;timestamp&#39;), (&#39;trip_distance&#39;, &#39;double&#39;), (&#39;store_and_fwd_flag&#39;, &#39;string&#39;), (&#39;fare_amount&#39;, &#39;double&#39;), (&#39;extra&#39;, &#39;double&#39;), (&#39;tip_amount&#39;, &#39;double&#39;), (&#39;tolls_amount&#39;, &#39;double&#39;), (&#39;airport_fee&#39;, &#39;double&#39;), (&#39;total_amount&#39;, &#39;double&#39;), (&#39;duration&#39;, &#39;double&#39;), (&#39;speed&#39;, &#39;double&#39;), (&#39;pickup_ET&#39;, &#39;timestamp&#39;), (&#39;dropoff_ET&#39;, &#39;timestamp&#39;), (&#39;passenger_count_indexed&#39;, &#39;double&#39;), (&#39;pickup_location_id_indexed&#39;, &#39;double&#39;), (&#39;dropoff_location_id_indexed&#39;, &#39;double&#39;), (&#39;payment_type_indexed&#39;, &#39;double&#39;), (&#39;mta_tax_indexed&#39;, &#39;double&#39;), (&#39;imp_surcharge_indexed&#39;, &#39;double&#39;), (&#39;vendor_id_encoded&#39;, &#39;vector&#39;), (&#39;rate_code_encoded&#39;, &#39;vector&#39;), (&#39;pickup_time_category_encoded&#39;, &#39;vector&#39;), (&#39;pickup_hour_encoded&#39;, &#39;vector&#39;), (&#39;day_of_week_encoded&#39;, &#39;vector&#39;), (&#39;year_encoded&#39;, &#39;vector&#39;), (&#39;passenger_count_encoded&#39;, &#39;vector&#39;)] . spark_df.select(&#39;pickup_hour_encoded&#39;).show(5) . [Stage 9:&gt; (0 + 1) / 1] . +-+ |pickup_hour_encoded| +-+ | (23,[0],[1.0])| | (23,[14],[1.0])| | (23,[15],[1.0])| | (23,[10],[1.0])| | (23,[13],[1.0])| +-+ only showing top 5 rows . . Note: We chose to use RobustScaler instead of StandardScaler because, after looking at the distribution of numerical features in our data, we determined that each feature is not approximately Gaussian. . StandardScaler assumes that the data is normally distributed and rescales features based on their mean and standard deviation. However, if the data is not normally distributed and has outliers, the mean and standard deviation can be strongly influenced by these outliers, resulting in suboptimal scaling. On the other hand, RobustScaler is less sensitive to outliers and works well for non-Gaussian data. We selected RobustScaler as it scales the data based on the median and quartiles, which are more robust measures of central tendency and spread. This makes it a good choice for datasets with non-Gaussian features, as it can handle outliers and skewed distributions more effectively. . from pyspark.ml.feature import RobustScaler, VectorAssembler from pyspark.ml import Pipeline # Defining each of the numerical columns numerical_cols_list = [&#39;trip_distance&#39;, &#39;extra&#39;, &#39;tip_amount&#39;, &#39;tolls_amount&#39;, &#39;airport_fee&#39;, &#39;duration&#39;] # &#39;speed&#39; excluded # Scaling the numerical columns using RobustScaler: num_assembler = VectorAssembler(inputCols=numerical_cols_list, outputCol = &quot;num_features&quot;) rScaler = RobustScaler(inputCol = &quot;num_features&quot;, outputCol = &quot;scaled_num_features&quot;) num_pipeline = Pipeline(stages=[num_assembler, rScaler]) scaled_numerical_df = num_pipeline.fit(spark_df).transform(spark_df) . scaled_numerical_df.dtypes . [(&#39;pickup_datetime&#39;, &#39;timestamp&#39;), (&#39;dropoff_datetime&#39;, &#39;timestamp&#39;), (&#39;trip_distance&#39;, &#39;double&#39;), (&#39;store_and_fwd_flag&#39;, &#39;string&#39;), (&#39;fare_amount&#39;, &#39;double&#39;), (&#39;extra&#39;, &#39;double&#39;), (&#39;tip_amount&#39;, &#39;double&#39;), (&#39;tolls_amount&#39;, &#39;double&#39;), (&#39;airport_fee&#39;, &#39;double&#39;), (&#39;total_amount&#39;, &#39;double&#39;), (&#39;duration&#39;, &#39;double&#39;), (&#39;speed&#39;, &#39;double&#39;), (&#39;pickup_ET&#39;, &#39;timestamp&#39;), (&#39;dropoff_ET&#39;, &#39;timestamp&#39;), (&#39;passenger_count_indexed&#39;, &#39;double&#39;), (&#39;pickup_location_id_indexed&#39;, &#39;double&#39;), (&#39;dropoff_location_id_indexed&#39;, &#39;double&#39;), (&#39;payment_type_indexed&#39;, &#39;double&#39;), (&#39;mta_tax_indexed&#39;, &#39;double&#39;), (&#39;imp_surcharge_indexed&#39;, &#39;double&#39;), (&#39;vendor_id_encoded&#39;, &#39;vector&#39;), (&#39;rate_code_encoded&#39;, &#39;vector&#39;), (&#39;pickup_time_category_encoded&#39;, &#39;vector&#39;), (&#39;pickup_hour_encoded&#39;, &#39;vector&#39;), (&#39;day_of_week_encoded&#39;, &#39;vector&#39;), (&#39;year_encoded&#39;, &#39;vector&#39;), (&#39;passenger_count_encoded&#39;, &#39;vector&#39;), (&#39;num_features&#39;, &#39;vector&#39;), (&#39;scaled_num_features&#39;, &#39;vector&#39;)] . 6.c. Combining Categorical and Scaled Numerical Features for Model Input . from pyspark.ml.feature import VectorAssembler independent_cols = [ &#39;vendor_id_encoded&#39;, &#39;rate_code_encoded&#39;, &#39;passenger_count_encoded&#39;, &#39;payment_type_indexed&#39;, &#39;mta_tax_indexed&#39;, &#39;imp_surcharge_indexed&#39;, &#39;year_encoded&#39;, &#39;day_of_week_encoded&#39;, &#39;pickup_hour_encoded&#39;, &#39;pickup_time_category_encoded&#39;, &#39;pickup_location_id_indexed&#39;, &#39;dropoff_location_id_indexed&#39;, &#39;scaled_num_features&#39;] # the scaled numerical features combined together column we created earlier assembler = VectorAssembler(inputCols=independent_cols, outputCol=&#39;Independent_Features&#39;) output = assembler.transform(scaled_numerical_df) finalized_data = output.select(&quot;Independent_Features&quot;, &quot;total_amount&quot;) . import pandas as pd finalized_data.limit(10).toPandas().head() finalized_data.show(5) . +--++ |Independent_Features|total_amount| +--++ |(49,[1,3,16,21,37...| 10.55| |(49,[0,1,2,6,12,2...| 8.8| |(49,[0,1,2,12,21,...| 18.8| |(49,[1,3,11,13,18...| 67.6| |(49,[1,2,6,11,23,...| 14.8| +--++ only showing top 5 rows . finalized_data.printSchema() . root |-- Independent_Features: vector (nullable = true) |-- total_amount: double (nullable = true) . finalized_data.cache() . DataFrame[Independent_Features: vector, total_amount: double] . 6.d. Applying ML Models . 6.d.1. Linear Regression: . from pyspark.ml.regression import LinearRegression from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # Split the data into training and testing sets train_data, test_data = finalized_data.randomSplit([0.75, 0.25], seed=42) # Define the model lr = LinearRegression(featuresCol=&#39;Independent_Features&#39;, labelCol=&#39;total_amount&#39;) #Hyperparameter tuning part with 9 possible combinations in this parameter grid to search over. param_grid = ParamGridBuilder() .addGrid(lr.regParam, [0.01, 0.1, 1.0]) .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) .build() # Defining regression model evaluation metrics to check it&#39;s performance metrics = [&#39;rmse&#39;, &#39;mse&#39;, &#39;r2&#39;, &#39;mae&#39;] evaluators = [RegressionEvaluator(predictionCol=&#39;prediction&#39;, labelCol=&#39;total_amount&#39;, metricName=metric) for metric in metrics] # Performing k-fold(5) cross-validation cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluators[0], numFolds = 5) # Train the model using cross-validation to find the best hyperparameters cv_model = cv.fit(train_data) # Get the best model&#39;s parameters best_model = cv_model.bestModel print(&quot;Best model parameters: regParam = {}, elasticNetParam = {}&quot;.format(best_model.getRegParam(), best_model.getElasticNetParam())) . Best model parameters: regParam = 0.1, elasticNetParam = 0.5 . Model Performance Evaluation on train/test set using Different Metrics: . train_predictions = best_model.transform(train_data) # Calculate the evaluation metrics for the training data for evaluator in evaluators: metric_value = evaluator.evaluate(train_predictions) print(&quot;{} on training set: {:.2f}&quot;.format(evaluator.getMetricName(), metric_value)) . rmse on training set: 0.49 mse on training set: 0.24 r2 on training set: 0.99 mae on training set: 0.41 . test_predictions = best_model.transform(test_data) # Evaluate the best model on the test set using all four metrics for i, evaluator in enumerate(evaluators): metric_name = metrics[i] metric_value = evaluator.evaluate(test_predictions) print(&quot;{} on test set: {:.2f}&quot;.format(metric_name.upper(), metric_value)) . RMSE on test set: 1.39 MSE on test set: 1.93 R2 on test set: 0.99 MAE on test set: 1.11 . k = len(test_data.select(&#39;Independent_Features&#39;).first()[0]) n = test_data.count() print(k, n) # Get the number of observations and predictors n = train_data.count() k = len(train_data.select(&#39;Independent_Features&#39;).first()[0]) # Calculate R-squared for the training data r2_evaluator = RegressionEvaluator(predictionCol=&#39;prediction&#39;, labelCol=&#39;total_amount&#39;, metricName=&#39;r2&#39;) r2 = r2_evaluator.evaluate(train_predictions) # Calculate adjusted R-squared for the training data adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1) print(&quot;Adjusted R-squared on training set: {:.2f}&quot;.format(adjusted_r2)) . 49 17 Adjusted R-squared on training set: 1.02 . # Get the number of observations and predictors n = test_data.count() k = len(test_data.select(&#39;Independent_Features&#39;).first()[0]) print(k, n) # Calculate R-squared for the test data r2_evaluator = RegressionEvaluator(predictionCol=&#39;prediction&#39;, labelCol=&#39;total_amount&#39;, metricName=&#39;r2&#39;) r2 = r2_evaluator.evaluate(test_predictions) # Calculate adjusted R-squared for the training data adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1) print(&quot;Adjusted R-squared on test set: {:.2f}&quot;.format(adjusted_r2)) . 49 17 Adjusted R-squared on test set: 1.00 . Note: If the adjusted R-squared on both the train and test sets is 1, it indicates that the model can perfectly explain all the variability in the target variable using the predictors. While this is ideal, it could also indicate overfitting to the training data. In such a case, the model may have learned the patterns in the training data so well that it is unable to generalize to new data. Thus, it&#39;s important to inspect the data, model, and training process carefully to ensure that the high performance is not due to chance or overfitting. . Moreover, if the adjusted R-squared on the test set is close to 1, it suggests good generalization performance of the model, and accurate predictions on new, unseen data. However, to ensure the effectiveness of the model for the specific task at hand, other metrics and aspects of the model also need to be evaluated. . from pyspark.sql.functions import abs, col # Make predictions on the training data train_predictions = best_model.transform(train_data) # Calculate the absolute percentage error for each observation train_ape = train_predictions.select(abs((col(&#39;prediction&#39;) - col(&#39;total_amount&#39;)) / col(&#39;total_amount&#39;)).alias(&#39;ape&#39;)) # Calculate the mean absolute percentage error for the training data train_mape = train_ape.agg({&#39;ape&#39;: &#39;mean&#39;}).collect()[0][0] print(&quot;MAPE on training set: {:.2f}%&quot;.format(train_mape * 100)) # Make predictions on the test data test_predictions = best_model.transform(test_data) # Calculate the absolute percentage error for each observation test_ape = test_predictions.select(abs((col(&#39;prediction&#39;) - col(&#39;total_amount&#39;)) / col(&#39;total_amount&#39;)).alias(&#39;ape&#39;)) # Calculate the mean absolute percentage error for the test data test_mape = test_ape.agg({&#39;ape&#39;: &#39;mean&#39;}).collect()[0][0] print(&quot;MAPE on test set: {:.2f}%&quot;.format(test_mape * 100)) . MAPE on training set: 3.55% MAPE on test set: 6.93% . train_predictions.show(10), test_predictions.show(10) . +--+++ |Independent_Features|total_amount| prediction| +--+++ |(49,[0,1,2,6,12,2...| 8.8| 8.358497689456259| |(49,[0,1,2,9,13,2...| 8.8| 8.18064284497928| |(49,[0,1,2,10,12,...| 18.96|18.379055700889072| |(49,[0,1,2,10,13,...| 10.3| 9.786557032052778| |(49,[0,1,2,10,15,...| 12.8|11.914897060153264| |(49,[0,1,2,12,21,...| 18.8|18.431195076406627| |(49,[0,1,5,6,9,15...| 4.8| 4.567943875027166| |(49,[1,2,6,11,23,...| 14.8|14.949242260853811| |(49,[1,2,9,12,22,...| 7.8| 7.985483015344615| |(49,[1,2,9,13,20,...| 24.8|25.474098217432616| +--+++ only showing top 10 rows +--+++ |Independent_Features|total_amount| prediction| +--+++ |(49,[0,1,2,9,15,1...| 27.36|28.983387281933013| |(49,[0,1,2,10,17,...| 15.18|13.450067865541277| |(49,[0,1,3,10,12,...| 13.39|13.590535186770902| |(49,[0,1,4,9,14,2...| 5.8| 6.230113378166406| |(49,[1,2,9,13,19,...| 11.15|10.559346970466091| |(49,[1,3,11,13,18...| 67.6| 66.98269941593372| |(49,[0,1,2,6,9,12...| 7.8| 7.084809065111886| |(49,[0,1,2,9,16,2...| 20.8|23.792373448101344| |(49,[0,1,2,9,20,3...| 13.3|15.361190805130096| |(49,[0,1,2,10,17,...| 10.38|10.322866045865927| +--+++ only showing top 10 rows . (None, None) . coefficients = best_model.coefficients feature_importances = coefficients.toArray() # Get the names of the input columns input_cols = independent_cols[:-1] # Exclude the scaled numerical features combined together column # Merge the input column names and their corresponding feature importances feature_importances_dict = {} for i, col in enumerate(input_cols): feature_importances_dict[col] = feature_importances[i] # Plot the feature importances using a chart import matplotlib.pyplot as plt plt.bar(feature_importances_dict.keys(), feature_importances_dict.values()) plt.xticks(rotation=90) plt.xlabel(&#39;Feature&#39;) plt.ylabel(&#39;Importance&#39;) plt.show() . import builtins # Get the coefficients of the linear regression model coefficients = best_model.coefficients # Get the names of the independent columns independent_cols = [&#39;vendor_id_encoded&#39;, &#39;rate_code_encoded&#39;, &#39;passenger_count_encoded&#39;, &#39;payment_type_indexed&#39;, &#39;mta_tax_indexed&#39;, &#39;imp_surcharge_indexed&#39;, &#39;year_encoded&#39;, &#39;day_of_week_encoded&#39;, &#39;pickup_hour_encoded&#39;, &#39;pickup_time_category_encoded&#39;, &#39;pickup_location_id_indexed&#39;, &#39;dropoff_location_id_indexed&#39;, &#39;scaled_num_features&#39;] # Create a dictionary to store the feature importance values feature_importance = {} # Loop through each independent column and its corresponding coefficient to calculate its feature importance for i, col_name in enumerate(independent_cols): feature_importance[col_name] = builtins.abs(coefficients[i]) # Print the feature importance values in descending order for col, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True): print(&quot;{}: {}&quot;.format(col, importance)) . pickup_time_category_encoded: 1.0461000812645387 vendor_id_encoded: 0.9509066866801812 mta_tax_indexed: 0.46458492552601416 passenger_count_encoded: 0.41946753508302476 imp_surcharge_indexed: 0.39502160115553636 rate_code_encoded: 0.28826920967785713 dropoff_location_id_indexed: 0.14502162667373236 payment_type_indexed: 0.0 year_encoded: 0.0 day_of_week_encoded: 0.0 pickup_hour_encoded: 0.0 pickup_location_id_indexed: 0.0 scaled_num_features: 0.0 . Feature Importance: . After fitting the model, we can extract the coefficients of the linear regression model and use them as a measure of feature importance. The higher the absolute value of the coefficient, the stronger the impact of that feature on the target variable (total_amount in this case). . feature_importance = best_model.coefficients len(feature_importance) . 49 . predictions = best_model.transform(test_data) # Get the feature names from the VectorAssembler inputCols parameter feature_names = assembler.getInputCols() # Get the feature importance feature_importance = best_model.coefficients # Create a dictionary of feature names and their importance feature_dict = {feature_names[i]: feature_importance[i] for i in range(len(feature_names))} # Print the feature importance for feature, importance in feature_dict.items(): print(f&quot;{feature}: {importance}&quot;) . vendor_id_encoded: 0.9509066866801812 rate_code_encoded: -0.28826920967785713 passenger_count_encoded: 0.41946753508302476 payment_type_indexed: 0.0 mta_tax_indexed: 0.46458492552601416 imp_surcharge_indexed: -0.39502160115553636 year_encoded: 0.0 day_of_week_encoded: 0.0 pickup_hour_encoded: 0.0 pickup_time_category_encoded: -1.0461000812645387 pickup_location_id_indexed: 0.0 dropoff_location_id_indexed: -0.14502162667373236 scaled_num_features: 0.0 . 6.d.2. Random Forest: . from pyspark.ml.regression import RandomForestRegressor #Split the data into training and testing sets train_data, test_data = finalized_data.randomSplit([0.8, 0.2]) #Create the RandomForestRegressor model # Set the maxBins parameter to a value larger than or equal to the maximum number of categories in your categorical features rf = RandomForestRegressor(featuresCol=&#39;Independent_Features&#39;, labelCol=&#39;total_amount&#39;, numTrees=100, maxBins=260) #Train the Random Forest regression model rf_model = rf.fit(train_data) #Evaluate the model performance on the testing set predictions = rf_model.transform(test_data) #Calculate the evaluation metrics evaluator = RegressionEvaluator(labelCol=&#39;total_amount&#39;, predictionCol=&#39;prediction&#39;, metricName=&#39;rmse&#39;) rmse = evaluator.evaluate(predictions) print(&quot;Root Mean Squared Error (RMSE) on test data =&quot;, rmse) #Feature importance importances = rf_model.featureImportances print(&quot;Feature importances: n&quot;, importances) . 23/05/01 17:03:16 WARN org.apache.spark.ml.tree.impl.DecisionTreeMetadata: DecisionTree reducing maxBins from 260 to 46 (= number of training instances) . Root Mean Squared Error (RMSE) on test data = 6.254776822660716 Feature importances: (49,[0,1,2,3,4,5,6,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,32,33,34,36,37,38,39,40,41,42,43,44,45,48],[0.00739446936593841,0.000338034201469969,0.01188494944677649,0.008065393978651286,0.0003195590623120424,0.000682730381104635,0.002243370571601835,0.00678115921884193,0.004393841176417902,0.005057788332127199,0.0007361930065056649,0.005320907498450368,0.00019789813946026488,3.8827102656537824e-05,0.004701862631693487,4.615359419877855e-05,0.006982148192152428,0.0008070638769954514,0.006404400787503051,0.0009491902173024396,0.0029010775038111375,8.374062102664835e-05,1.798182984710389e-05,0.00016084743726844052,0.00022345779807598666,0.003832862960180666,0.0001269805207467428,3.9063523909880426e-05,0.0012860744754177732,2.01157903350511e-06,4.0683755745483945e-05,0.0006027730756145802,8.25009919435316e-06,0.0002740622804697816,0.002207705663000316,0.0027699754246257404,0.0015484663708714614,0.1605239854830129,0.1643736527334664,0.2724171237896862,0.007798386268502481,0.16630081630721505,0.13911407971711737]) . r2_evaluator = RegressionEvaluator(labelCol=&#39;total_amount&#39;, predictionCol=&#39;prediction&#39;, metricName=&#39;r2&#39;) r2 = r2_evaluator.evaluate(predictions) print(&quot;R-squared on test data =&quot;, r2) . R-squared on test data = 0.4881194640860249 . from pyspark.sql.functions import col def calculate_mape(test_data, predictions): test_data_with_pred = test_data.join(predictions, on=[&#39;Independent_Features&#39;, &#39;total_amount&#39;], how=&#39;inner&#39;) mape_df = test_data_with_pred.select(((abs(col(&#39;total_amount&#39;) - col(&#39;prediction&#39;)) / col(&#39;total_amount&#39;)) * 100).alias(&#39;mape&#39;)) total_count = mape_df.count() total_mape = mape_df.agg({&#39;mape&#39;: &#39;sum&#39;}).collect()[0][0] return total_mape / total_count mape = calculate_mape(test_data, predictions) print(&quot;Mean Absolute Percentage Error (MAPE) on test data =&quot;, mape) . Mean Absolute Percentage Error (MAPE) on test data = 40.520127814706036 . print(f&quot;Length of independent_cols: {len(independent_cols)}&quot;) print(f&quot;Length of importances: {len(importances.toArray())}&quot;) . Length of independent_cols: 13 Length of importances: 49 . import pandas as pd feature_importances = importances.toArray() encoded_columns = finalized_data.select(&quot;Independent_Features&quot;).schema[&quot;Independent_Features&quot;].metadata[&quot;ml_attr&quot;][&quot;attrs&quot;][&quot;binary&quot;] + finalized_data.select(&quot;Independent_Features&quot;).schema[&quot;Independent_Features&quot;].metadata[&quot;ml_attr&quot;][&quot;attrs&quot;][&quot;numeric&quot;] #df encoded_importances_df = pd.DataFrame(list(zip([col[&quot;name&quot;] for col in encoded_columns], feature_importances)), columns=[&quot;Feature&quot;, &quot;Importance&quot;]) print(encoded_importances_df) . Feature Importance 0 vendor_id_encoded_2 0.007394 1 rate_code_encoded_1 0.000338 2 passenger_count_encoded_1 0.011885 3 passenger_count_encoded_2 0.008065 4 passenger_count_encoded_3 0.000320 5 passenger_count_encoded_6 0.000683 6 year_encoded_2018 0.002243 7 year_encoded_2019 0.000000 8 year_encoded_2021 0.000000 9 day_of_week_encoded_Saturday 0.006781 10 day_of_week_encoded_Monday 0.004394 11 day_of_week_encoded_Friday 0.005058 12 day_of_week_encoded_Thursday 0.000736 13 day_of_week_encoded_Sunday 0.005321 14 day_of_week_encoded_Tuesday 0.000198 15 pickup_hour_encoded_4 0.000039 16 pickup_hour_encoded_11 0.004702 17 pickup_hour_encoded_15 0.000046 18 pickup_hour_encoded_8 0.006982 19 pickup_hour_encoded_13 0.000807 20 pickup_hour_encoded_14 0.006404 21 pickup_hour_encoded_12 0.000949 22 pickup_hour_encoded_17 0.002901 23 pickup_hour_encoded_6 0.000084 24 pickup_hour_encoded_7 0.000018 25 pickup_hour_encoded_9 0.000161 26 pickup_hour_encoded_20 0.000223 27 pickup_hour_encoded_5 0.003833 28 pickup_hour_encoded_1 0.000127 29 pickup_hour_encoded_16 0.000039 30 pickup_hour_encoded_18 0.001286 31 pickup_hour_encoded_19 0.000000 32 pickup_hour_encoded_2 0.000002 33 pickup_hour_encoded_21 0.000041 34 pickup_time_category_encoded_Morning 0.000603 35 pickup_time_category_encoded_Afternoon 0.000000 36 pickup_time_category_encoded_Late Night 0.000008 37 pickup_time_category_encoded_Evening 0.000274 38 scaled_num_features_0 0.002208 39 scaled_num_features_1 0.002770 40 scaled_num_features_2 0.001548 41 scaled_num_features_3 0.160524 42 scaled_num_features_4 0.164374 43 scaled_num_features_5 0.272417 . categorical_features = [ &quot;vendor_id&quot;, &quot;rate_code&quot;, &quot;passenger_count&quot;, &quot;payment_type&quot;, &quot;mta_tax&quot;, &quot;imp_surcharge&quot;, &quot;year&quot;, &quot;day_of_week&quot;, &quot;pickup_hour&quot;, &quot;pickup_time_category&quot;, &quot;pickup_location_id&quot;, &quot;dropoff_location_id&quot; ] #Aggregate aggregated_importances = [] for cat_feat in categorical_features: aggregated_importance = encoded_importances_df[encoded_importances_df[&quot;Feature&quot;].str.startswith(cat_feat)][&quot;Importance&quot;].sum() aggregated_importances.append((cat_feat, aggregated_importance)) numeric_features = [ &quot;scaled_num_features_0&quot;, # trip_distance &quot;scaled_num_features_1&quot;, # extra &quot;scaled_num_features_2&quot;, # tip_amount &quot;scaled_num_features_3&quot;, # tolls_amount &quot;scaled_num_features_4&quot;, # airport_fee &quot;scaled_num_features_5&quot;, # duration ] for num_feat in numeric_features: aggregated_importances.append((num_feat, encoded_importances_df.loc[encoded_importances_df[&quot;Feature&quot;] == num_feat, &quot;Importance&quot;].values[0])) aggregated_importances_df = pd.DataFrame(aggregated_importances, columns=[&quot;Feature&quot;, &quot;Importance&quot;]) aggregated_importances_df = aggregated_importances_df.sort_values(by=&quot;Importance&quot;, ascending=False) print(aggregated_importances_df) . Feature Importance 17 scaled_num_features_5 0.272417 16 scaled_num_features_4 0.164374 15 scaled_num_features_3 0.160524 8 pickup_hour 0.028644 7 day_of_week 0.022488 2 passenger_count 0.020953 0 vendor_id 0.007394 13 scaled_num_features_1 0.002770 6 year 0.002243 12 scaled_num_features_0 0.002208 14 scaled_num_features_2 0.001548 9 pickup_time_category 0.000885 1 rate_code 0.000338 5 imp_surcharge 0.000000 10 pickup_location_id 0.000000 11 dropoff_location_id 0.000000 4 mta_tax 0.000000 3 payment_type 0.000000 . encoded_to_original_names = { &quot;scaled_num_features_0&quot;: &quot;trip_distance&quot;, &quot;scaled_num_features_1&quot;: &quot;extra&quot;, &quot;scaled_num_features_2&quot;: &quot;tip_amount&quot;, &quot;scaled_num_features_3&quot;: &quot;tolls_amount&quot;, &quot;scaled_num_features_4&quot;: &quot;airport_fee&quot;, &quot;scaled_num_features_5&quot;: &quot;duration&quot; } #Update the &#39;Feature&#39; column aggregated_importances_df[&quot;Feature&quot;] = aggregated_importances_df[&quot;Feature&quot;].replace(encoded_to_original_names) print(aggregated_importances_df) . Feature Importance 17 duration 0.272417 16 airport_fee 0.164374 15 tolls_amount 0.160524 8 pickup_hour 0.028644 7 day_of_week 0.022488 2 passenger_count 0.020953 0 vendor_id 0.007394 13 extra 0.002770 6 year 0.002243 12 trip_distance 0.002208 14 tip_amount 0.001548 9 pickup_time_category 0.000885 1 rate_code 0.000338 5 imp_surcharge 0.000000 10 pickup_location_id 0.000000 11 dropoff_location_id 0.000000 4 mta_tax 0.000000 3 payment_type 0.000000 . import matplotlib.pyplot as plt plt.figure(figsize=(12, 6)) plt.bar(aggregated_importances_df[&quot;Feature&quot;], aggregated_importances_df[&quot;Importance&quot;]) plt.xlabel(&quot;Features&quot;) plt.ylabel(&quot;Importance&quot;) plt.title(&quot;Feature Importances&quot;) plt.xticks(rotation=90) plt.show() . 6.d.3. GBTRegressor: . from pyspark.ml.regression import GBTRegressor from pyspark.ml.evaluation import RegressionEvaluator #Split the data train_data, test_data = finalized_data.randomSplit([0.8, 0.2]) #Create the GBTRegressor model gbt = GBTRegressor(featuresCol=&#39;Independent_Features&#39;, labelCol=&#39;total_amount&#39;, maxBins=260) #Train gbt_model = gbt.fit(train_data) #Evaluate the model predictions = gbt_model.transform(test_data) evaluator = RegressionEvaluator(labelCol=&#39;total_amount&#39;, predictionCol=&#39;prediction&#39;, metricName=&#39;rmse&#39;) rmse = evaluator.evaluate(predictions) print(&quot;Root Mean Squared Error (RMSE) on test data =&quot;, rmse) . 23/05/01 17:04:21 WARN org.apache.spark.ml.tree.impl.DecisionTreeMetadata: DecisionTree reducing maxBins from 260 to 49 (= number of training instances) . Root Mean Squared Error (RMSE) on test data = 18.10462378361858 . r2_evaluator = RegressionEvaluator(labelCol=&#39;total_amount&#39;, predictionCol=&#39;prediction&#39;, metricName=&#39;r2&#39;) r2 = r2_evaluator.evaluate(predictions) print(&quot;R-squared on test data =&quot;, r2) . R-squared on test data = -23.13416830384304 . from pyspark.sql.functions import col # Define a function to calculate the Mean Absolute Percentage Error (MAPE) def calculate_mape(test_data, predictions): test_data_with_pred = test_data.join(predictions, on=[&#39;Independent_Features&#39;, &#39;total_amount&#39;], how=&#39;inner&#39;) mape_df = test_data_with_pred.select(((abs(col(&#39;total_amount&#39;) - col(&#39;prediction&#39;)) / col(&#39;total_amount&#39;)) * 100).alias(&#39;mape&#39;)) total_count = mape_df.count() total_mape = mape_df.agg({&#39;mape&#39;: &#39;sum&#39;}).collect()[0][0] return total_mape / total_count # Calculate the MAPE on test data mape = calculate_mape(test_data, predictions) print(&quot;Mean Absolute Percentage Error (MAPE) on test data =&quot;, mape) . Mean Absolute Percentage Error (MAPE) on test data = 114.54368701975697 . Summary of Different Model Metrics:: . Model RMSE R2 MAPE Interpretation . Linear Regression | 1.39 | 0.99 | 6.93% | Accurate predictions with high proportion of explained variance and good overall fit to the data. | . Random Forest | 6.25 | 0.49 | 40.52% | Less accurate predictions with lower proportion of explained variance and poorer overall fit to the data. | . GBTRegressor | 18.10 | -23.13 | 114.54% | Least accurate predictions with negative R-squared indicating worse performance than the mean value of the target variable, and very poor overall fit to the data. | . In conclusion, for the ML part the Linear Regression model outperformed the other two models, with accurate predictions, high R-squared, and low MAPE. Random Forest had higher errors and lower R-squared, indicating weaker performance. GBTRegressor performed the worst with very high errors, negative R-squared, and poor overall fit to the data. . Thus, we can conclude from ML part that the Linear Regression model performed the best, with low RMSE, high R-squared, and low MAPE. Random Forest had higher errors and lower R-squared, indicating weaker performance. GBTRegressor performed the worst with very high errors and negative R-squared, indicating a poor fit to the data. . 7. Challenges . Challenges with large datasets were solved via encoding, scaling, and vectorizing. | . We encountered challenges while working with a large dataset with various features. To overcome these challenges, we numerically encoded nominal categorical features using String Indexer and applied one-hot encoding to ordinal categorical features to reduce bias in the regression model. We also used the robust scaler technique to scale numerical features and avoid feature dominance or bias issues. We did not scale or normalize the target variable as it represented the actual value we wanted to predict. Finally, we used feature vectors to create a sparse matrix to combine the categorical and scaled numerical features for our model input. . Not generalizable to the entire dataset: | . Due to the large dataset size, a small sample was used to build the regression models. This may limit the model&#39;s representativeness and accuracy, as important patterns or relationships may be missed. Additionally, the model may not generalize well to new data, as it was trained on only a small portion of the available data and may not fully represent the entire population. Therefore, caution should be exercised when using this model to make predictions or inform decision-making. . Critical external factors missing from the dataset: | . One challenge with predicting total_amount in the given dataset is the absence of potentially essential features such as the route taken by the taxi, weather, and traffic conditions during the trip. These factors could significantly impact the fare amount and total revenue earned per trip, but their absence makes it difficult to build an accurate predictive model. . 8. Conclusion . Explaratory Data Analysis | . Based on the data analysis conducted, it is evident that demand for taxi trips in New York City was significantly impacted by the COVID-19 pandemic. While this conclusion is self-evident, our data shows that demand for taxis after March 2020 plummeted. However, our data also indicates that there has been a steady increase in demand since 2021. . The analysis also revealed that taxi trips were most popular during the morning and afternoon hours, with demand decreasing significantly during the late evening and night hours. In terms of trip length, short-distance trips were the most popular, with the most frequently traveled routes being in the upper east and upper west side, spanning 66 blocks. Additionally, long trips were found to be the most expensive per minute, followed by shorter trips and medium-length trips, which had the lowest cost per minute. . In terms of difference in demand based on the day of the week; Friday has the highest demand, while Sunday has the lowest. If drivers want to make more money, they may not take business on these two days because Friday may see congestion, and you may only take a few customers on Sunday. . Machine Learning | . In terms of performance, we evaluated three ML models on our dataset: Linear Regression, Random Forest, and GBTRegressor. The Linear Regression model outperformed the other models with accurate predictions, high R-squared, and low MAPE. On the other hand, Random Forest had higher errors and lower R-squared, indicating weaker performance. GBTRegressor performed the worst with very high errors, negative R-squared, and poor overall fit to the data. Based on our evaluation, we recommend using Linear Regression as the preferred model for this dataset due to its superior performance compared to the other models. However, it is essential to note that the choice of the best model depends on the specific problem and dataset characteristics, and the performance of the models may vary depending on various factors. Therefore, it is recommended to interpret the model performance with a grain of salt and consider the study&#39;s specific context and limitations. . In addition, our ML model revealed some important factors that drive taxi trip earnings, such as duration, airport fee, and tolls amount, as well as passenger count, pickup hour, and day of the week. However, it&#39;s important to note that the analysis could be further improved by including additional features such as weather and traffic conditions, which were not included in our dataset. Nonetheless, these insights can provide some valuable guidance for decision-making for both taxi driversandvendors . Other Contributers in this Project: . Gabriela Vargas Horle | Harshil Thakkar | Heijang Wu | Wenxuan Yan | .",
            "url": "https://priyank7n.me/2023/05/04/Analyzing-NYC-Taxi-Trips-Understanding-Demand-and-Optimizing-Revenue.html",
            "relUrl": "/2023/05/04/Analyzing-NYC-Taxi-Trips-Understanding-Demand-and-Optimizing-Revenue.html",
            "date": "  May 4, 2023"
        }
        
    
  
    
  
    
  
    
        ,"post3": {
            "title": "SatFootprint:  Foot Print Detection of Buildings in Satellite Images",
            "content": "Introduction . In this notebook I implement a neural network based solution for building footprint detection on the SpaceNet7 dataset. I ignore the temporal aspect of the orginal challenge and focus on performing segmentation to detect buildings on single images. I use fastai, a deep learning library based on PyTorch. It provides functionality to train neural networks with modern best practices while reducing the amount of boilerplate code required. . Dataset Downloading . from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) . Mounted at /content/gdrive . cd /content/gdrive/Shareddrives/Undrive . /content/gdrive/Shareddrives/Undrive . # !unzip spacenet-7-multitemporal-urban-development.zip -d s7 . START . The dataset is stored on AWS. Instructions how to install are here. . Installing Libraries and Preparing requirements.txt for reproducbillity . . !pip freeze &gt; requirements.txt . ls . models/ s7/ wandb/ requirements.txt spacenet-7-multitemporal-urban-development.zip . Setup . cd /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train/ . /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train . . !ls . L15-0331E-1257N_1327_3160_13 L15-1203E-1203N_4815_3378_13 L15-0357E-1223N_1429_3296_13 L15-1204E-1202N_4816_3380_13 L15-0358E-1220N_1433_3310_13 L15-1204E-1204N_4819_3372_13 L15-0361E-1300N_1446_2989_13 L15-1209E-1113N_4838_3737_13 L15-0368E-1245N_1474_3210_13 L15-1210E-1025N_4840_4088_13 L15-0387E-1276N_1549_3087_13 L15-1276E-1107N_5105_3761_13 L15-0434E-1218N_1736_3318_13 L15-1289E-1169N_5156_3514_13 L15-0457E-1135N_1831_3648_13 L15-1296E-1198N_5184_3399_13 L15-0487E-1246N_1950_3207_13 L15-1298E-1322N_5193_2903_13 L15-0506E-1204N_2027_3374_13 L15-1335E-1166N_5342_3524_13 L15-0544E-1228N_2176_3279_13 L15-1389E-1284N_5557_3054_13 L15-0566E-1185N_2265_3451_13 L15-1438E-1134N_5753_3655_13 L15-0571E-1075N_2287_3888_13 L15-1439E-1134N_5759_3655_13 L15-0577E-1243N_2309_3217_13 L15-1479E-1101N_5916_3785_13 L15-0586E-1127N_2345_3680_13 L15-1481E-1119N_5927_3715_13 L15-0595E-1278N_2383_3079_13 L15-1538E-1163N_6154_3539_13 L15-0614E-0946N_2459_4406_13 L15-1615E-1205N_6460_3370_13 L15-0632E-0892N_2528_4620_13 L15-1615E-1206N_6460_3366_13 L15-0683E-1006N_2732_4164_13 L15-1617E-1207N_6468_3360_13 L15-0760E-0887N_3041_4643_13 L15-1669E-1153N_6678_3579_13 L15-0924E-1108N_3699_3757_13 L15-1669E-1160N_6678_3548_13 L15-0977E-1187N_3911_3441_13 L15-1669E-1160N_6679_3549_13 L15-1014E-1375N_4056_2688_13 L15-1672E-1207N_6691_3363_13 L15-1015E-1062N_4061_3941_13 L15-1690E-1211N_6763_3346_13 L15-1025E-1366N_4102_2726_13 L15-1691E-1211N_6764_3347_13 L15-1049E-1370N_4196_2710_13 L15-1703E-1219N_6813_3313_13 L15-1138E-1216N_4553_3325_13 L15-1709E-1112N_6838_3742_13 L15-1172E-1306N_4688_2967_13 L15-1716E-1211N_6864_3345_13 L15-1185E-0935N_4742_4450_13 models L15-1200E-0847N_4802_4803_13 wandb . . L15-0331E-1257N_1327_3160_13 L15-1200E-0847N_4802_4803_13 L15-0357E-1223N_1429_3296_13 L15-1203E-1203N_4815_3378_13 L15-0358E-1220N_1433_3310_13 L15-1204E-1202N_4816_3380_13 L15-0361E-1300N_1446_2989_13 L15-1204E-1204N_4819_3372_13 L15-0368E-1245N_1474_3210_13 L15-1209E-1113N_4838_3737_13 L15-0387E-1276N_1549_3087_13 L15-1210E-1025N_4840_4088_13 L15-0434E-1218N_1736_3318_13 L15-1276E-1107N_5105_3761_13 L15-0457E-1135N_1831_3648_13 L15-1289E-1169N_5156_3514_13 L15-0487E-1246N_1950_3207_13 L15-1296E-1198N_5184_3399_13 L15-0506E-1204N_2027_3374_13 L15-1298E-1322N_5193_2903_13 L15-0544E-1228N_2176_3279_13 L15-1335E-1166N_5342_3524_13 L15-0566E-1185N_2265_3451_13 L15-1389E-1284N_5557_3054_13 L15-0571E-1075N_2287_3888_13 L15-1438E-1134N_5753_3655_13 L15-0577E-1243N_2309_3217_13 L15-1439E-1134N_5759_3655_13 L15-0586E-1127N_2345_3680_13 L15-1479E-1101N_5916_3785_13 L15-0595E-1278N_2383_3079_13 L15-1481E-1119N_5927_3715_13 L15-0614E-0946N_2459_4406_13 L15-1538E-1163N_6154_3539_13 L15-0632E-0892N_2528_4620_13 L15-1615E-1205N_6460_3370_13 L15-0683E-1006N_2732_4164_13 L15-1615E-1206N_6460_3366_13 L15-0760E-0887N_3041_4643_13 L15-1617E-1207N_6468_3360_13 L15-0924E-1108N_3699_3757_13 L15-1669E-1153N_6678_3579_13 L15-0977E-1187N_3911_3441_13 L15-1669E-1160N_6678_3548_13 L15-1014E-1375N_4056_2688_13 L15-1669E-1160N_6679_3549_13 L15-1015E-1062N_4061_3941_13 L15-1672E-1207N_6691_3363_13 L15-1025E-1366N_4102_2726_13 L15-1690E-1211N_6763_3346_13 L15-1049E-1370N_4196_2710_13 L15-1691E-1211N_6764_3347_13 L15-1138E-1216N_4553_3325_13 L15-1703E-1219N_6813_3313_13 L15-1172E-1306N_4688_2967_13 L15-1709E-1112N_6838_3742_13 L15-1185E-0935N_4742_4450_13 L15-1716E-1211N_6864_3345_13 . path.ls() . (#59) [Path(&#39;L15-0331E-1257N_1327_3160_13&#39;),Path(&#39;L15-0357E-1223N_1429_3296_13&#39;),Path(&#39;L15-0358E-1220N_1433_3310_13&#39;),Path(&#39;L15-0361E-1300N_1446_2989_13&#39;),Path(&#39;L15-0368E-1245N_1474_3210_13&#39;),Path(&#39;L15-0387E-1276N_1549_3087_13&#39;),Path(&#39;L15-0434E-1218N_1736_3318_13&#39;),Path(&#39;L15-0457E-1135N_1831_3648_13&#39;),Path(&#39;L15-0487E-1246N_1950_3207_13&#39;),Path(&#39;L15-0506E-1204N_2027_3374_13&#39;)...] . Defining training parameters: . cd /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train . /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train . ls . L15-0331E-1257N_1327_3160_13/ L15-1200E-0847N_4802_4803_13/ L15-0357E-1223N_1429_3296_13/ L15-1203E-1203N_4815_3378_13/ L15-0358E-1220N_1433_3310_13/ L15-1204E-1202N_4816_3380_13/ L15-0361E-1300N_1446_2989_13/ L15-1204E-1204N_4819_3372_13/ L15-0368E-1245N_1474_3210_13/ L15-1209E-1113N_4838_3737_13/ L15-0387E-1276N_1549_3087_13/ L15-1210E-1025N_4840_4088_13/ L15-0434E-1218N_1736_3318_13/ L15-1276E-1107N_5105_3761_13/ L15-0457E-1135N_1831_3648_13/ L15-1289E-1169N_5156_3514_13/ L15-0487E-1246N_1950_3207_13/ L15-1296E-1198N_5184_3399_13/ L15-0506E-1204N_2027_3374_13/ L15-1298E-1322N_5193_2903_13/ L15-0544E-1228N_2176_3279_13/ L15-1335E-1166N_5342_3524_13/ L15-0566E-1185N_2265_3451_13/ L15-1389E-1284N_5557_3054_13/ L15-0571E-1075N_2287_3888_13/ L15-1438E-1134N_5753_3655_13/ L15-0577E-1243N_2309_3217_13/ L15-1439E-1134N_5759_3655_13/ L15-0586E-1127N_2345_3680_13/ L15-1479E-1101N_5916_3785_13/ L15-0595E-1278N_2383_3079_13/ L15-1481E-1119N_5927_3715_13/ L15-0614E-0946N_2459_4406_13/ L15-1538E-1163N_6154_3539_13/ L15-0632E-0892N_2528_4620_13/ L15-1615E-1205N_6460_3370_13/ L15-0683E-1006N_2732_4164_13/ L15-1615E-1206N_6460_3366_13/ L15-0760E-0887N_3041_4643_13/ L15-1617E-1207N_6468_3360_13/ L15-0924E-1108N_3699_3757_13/ L15-1669E-1153N_6678_3579_13/ L15-0977E-1187N_3911_3441_13/ L15-1669E-1160N_6678_3548_13/ L15-1014E-1375N_4056_2688_13/ L15-1669E-1160N_6679_3549_13/ L15-1015E-1062N_4061_3941_13/ L15-1672E-1207N_6691_3363_13/ L15-1025E-1366N_4102_2726_13/ L15-1690E-1211N_6763_3346_13/ L15-1049E-1370N_4196_2710_13/ L15-1691E-1211N_6764_3347_13/ L15-1138E-1216N_4553_3325_13/ L15-1703E-1219N_6813_3313_13/ L15-1172E-1306N_4688_2967_13/ L15-1709E-1112N_6838_3742_13/ L15-1185E-0935N_4742_4450_13/ L15-1716E-1211N_6864_3345_13/ . BATCH_SIZE = 12 # (3 for xresnet50, 12 for xresnet34 with Tesla P100/T4) TILES_PER_SCENE = 16 ARCHITECTURE = xresnet34 EPOCHS = 40 CLASS_WEIGHTS = [0.25,0.75] LR_MAX = 3e-4 ENCODER_FACTOR = 10 CODES = [&#39;Land&#39;,&#39;Building&#39;] . # Weights and Biases config config_dictionary = dict( bs=BATCH_SIZE, tiles_per_scene=TILES_PER_SCENE, architecture = str(ARCHITECTURE), epochs = EPOCHS, class_weights = CLASS_WEIGHTS, lr_max = LR_MAX, encoder_factor = ENCODER_FACTOR ) . BATCH_SIZE = 12 # 3 for xresnet50, 12 for xresnet34 with Tesla P100 (16GB) TILES_PER_SCENE = 16 ARCHITECTURE = xresnet34 EPOCHS = 80 CLASS_WEIGHTS = [0.25,0.75] LR_MAX = 3e-4 ENCODER_FACTOR = 10 CODES = [&#39;Land&#39;,&#39;Building&#39;] . BATCH_SIZE = 3 # 3 for xresnet50, 12 for xresnet34 with Tesla P100 (16GB) TILES_PER_SCENE = 16 ARCHITECTURE = xresnet50 EPOCHS = 40 CLASS_WEIGHTS = [0.25,0.75] LR_MAX = 3e-4 ENCODER_FACTOR = 10 CODES = [&#39;Land&#39;,&#39;Building&#39;] . !ls . models s7 wandb requirements.txt spacenet-7-multitemporal-urban-development.zip . Data-Preprocessing . Exploring dataset structure, display sample scene directories: . !nvidia-smi . Tue Jul 6 07:44:58 2021 +--+ | NVIDIA-SMI 465.27 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 37C P8 9W / 70W | 3MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +--+ . scenes = path.ls().sorted() print(f&#39;Numer of scenes: {len(scenes)}&#39;) pprint(list(scenes)[:5]) . . Numer of scenes: 59 [Path(&#39;.ipynb_checkpoints&#39;), Path(&#39;L15-0331E-1257N_1327_3160_13&#39;), Path(&#39;L15-0357E-1223N_1429_3296_13&#39;), Path(&#39;L15-0358E-1220N_1433_3310_13&#39;), Path(&#39;L15-0361E-1300N_1446_2989_13&#39;)] . Which folders are in each scene (the last three have been added later during processing) . sample_scene = (path/&#39;L15-0683E-1006N_2732_4164_13&#39;) pprint(list(sample_scene.ls())) . . [Path(&#39;L15-0683E-1006N_2732_4164_13/UDM_masks&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/labels&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/labels_match&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/labels_match_pix&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/binary_mask&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/img_tiles&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/mask_tiles&#39;)] . How many images are in a specific scene: . images_masked = (sample_scene/&#39;images_masked&#39;).ls().sorted() labels = (sample_scene/&#39;labels_match&#39;).ls().sorted() print(f&#39;Numer of images in scene: {len(images_masked)}&#39;) pprint(list(images_masked[:5])) . . Numer of images in scene: 22 [Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_01_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_02_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_03_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_04_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_06_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;)] . There are 58 scenes of 4km x 4km in the dataset, each containing about 24 images over the span of two years. . Let&#39;s pick one example image and its polygons: . image, shapes = images_masked[0], labels[0] . We use the images that have UDM masks where clouds were in the original picture: . show_image(PILImage.create(image), figsize=(12,12)); . Creating binary masks . This is a function to generate binary mask images from geojson vector files. Source . import rasterio from rasterio.plot import reshape_as_image import rasterio.mask from rasterio.features import rasterize import pandas as pd import geopandas as gpd from shapely.geometry import mapping, Point, Polygon from shapely.ops import cascaded_union # SOURCE: https://lpsmlgeo.github.io/2019-09-22-binary_mask/ def generate_mask(raster_path, shape_path, output_path=None, file_name=None): &quot;&quot;&quot;Function that generates a binary mask from a vector file (shp or geojson) raster_path = path to the .tif; shape_path = path to the shapefile or GeoJson. output_path = Path to save the binary mask. file_name = Name of the file. &quot;&quot;&quot; #load raster with rasterio.open(raster_path, &quot;r&quot;) as src: raster_img = src.read() raster_meta = src.meta #load o shapefile ou GeoJson train_df = gpd.read_file(shape_path) #Verify crs if train_df.crs != src.crs: print(&quot; Raster crs : {}, Vector crs : {}. n Convert vector and raster to the same CRS.&quot;.format(src.crs,train_df.crs)) #Function that generates the mask def poly_from_utm(polygon, transform): poly_pts = [] poly = cascaded_union(polygon) for i in np.array(poly.exterior.coords): poly_pts.append(~transform * tuple(i)) new_poly = Polygon(poly_pts) return new_poly poly_shp = [] im_size = (src.meta[&#39;height&#39;], src.meta[&#39;width&#39;]) for num, row in train_df.iterrows(): if row[&#39;geometry&#39;].geom_type == &#39;Polygon&#39;: poly = poly_from_utm(row[&#39;geometry&#39;], src.meta[&#39;transform&#39;]) poly_shp.append(poly) else: for p in row[&#39;geometry&#39;]: poly = poly_from_utm(p, src.meta[&#39;transform&#39;]) poly_shp.append(poly) #set_trace() if len(poly_shp) &gt; 0: mask = rasterize(shapes=poly_shp, out_shape=im_size) else: mask = np.zeros(im_size) # Save or show mask mask = mask.astype(&quot;uint8&quot;) bin_mask_meta = src.meta.copy() bin_mask_meta.update({&#39;count&#39;: 1}) if (output_path != None and file_name != None): os.chdir(output_path) with rasterio.open(file_name, &#39;w&#39;, **bin_mask_meta) as dst: dst.write(mask * 255, 1) else: return mask . . Show a mask: . mask = generate_mask(image, shapes) plt.figure(figsize=(12,12)) plt.tight_layout() plt.xticks([]) plt.yticks([]) plt.imshow(mask,cmap=&#39;cividis&#39;); . . . Note: We can see that there - correctly - are no buildings in the mask where the UDM mask is. . Now we create and save a mask file for every image in the &#39;images_masked&#39; folder of every scene. . path.ls() . (#59) [Path(&#39;L15-0331E-1257N_1327_3160_13&#39;),Path(&#39;L15-0357E-1223N_1429_3296_13&#39;),Path(&#39;L15-0358E-1220N_1433_3310_13&#39;),Path(&#39;L15-0361E-1300N_1446_2989_13&#39;),Path(&#39;L15-0368E-1245N_1474_3210_13&#39;),Path(&#39;L15-0387E-1276N_1549_3087_13&#39;),Path(&#39;L15-0434E-1218N_1736_3318_13&#39;),Path(&#39;L15-0457E-1135N_1831_3648_13&#39;),Path(&#39;L15-0487E-1246N_1950_3207_13&#39;),Path(&#39;L15-0506E-1204N_2027_3374_13&#39;)...] . def save_masks(): for scene in tqdm(path.ls().sorted()): for img in (scene/&#39;images_masked&#39;).ls(): shapes = scene/&#39;labels_match&#39;/(img.name[:-4]+&#39;_Buildings.geojson&#39;) if not os.path.exists(scene/&#39;binary_mask&#39;/img.name): if not os.path.exists(scene/&#39;binary_mask&#39;): os.makedirs(scene/&#39;binary_mask&#39;) generate_mask(img, shapes, scene/&#39;binary_mask&#39;, img.name) . . save_masks() . As mask creation failed on one image for no obvious reason. I simply deleted it from the training set. . Creating subset of dataset . Let&#39;s look at how the images in a scene change over time: . . We can see that the ~24 images of every scene are quite similar. The vegetation changes with the seasons and some scenes show building activity, but overall the similarities are greater than the differences. . Therefore I decided to ignore most images. I originally planned to keep every fifth image of every scene, so for example January, June, November, April, and September. This way we could make use of the variability of the different seasons. But it turned out that just selecting one image per scene yielded similar results with a fraction of the training time. . def get_masked_images(path:Path, n=1)-&gt;list: &quot;Returns the first `n` pictures from every scene&quot; files = [] for folder in path.ls(): files.extend(get_image_files(path=folder, folders=&#39;images_masked&#39;)[:n]) return files . . path.ls() . (#59) [Path(&#39;L15-0331E-1257N_1327_3160_13&#39;),Path(&#39;L15-0357E-1223N_1429_3296_13&#39;),Path(&#39;L15-0358E-1220N_1433_3310_13&#39;),Path(&#39;L15-0361E-1300N_1446_2989_13&#39;),Path(&#39;L15-0368E-1245N_1474_3210_13&#39;),Path(&#39;L15-0387E-1276N_1549_3087_13&#39;),Path(&#39;L15-0434E-1218N_1736_3318_13&#39;),Path(&#39;L15-0457E-1135N_1831_3648_13&#39;),Path(&#39;L15-0487E-1246N_1950_3207_13&#39;),Path(&#39;L15-0506E-1204N_2027_3374_13&#39;)...] . !ls . L15-0331E-1257N_1327_3160_13 L15-1200E-0847N_4802_4803_13 L15-0357E-1223N_1429_3296_13 L15-1203E-1203N_4815_3378_13 L15-0358E-1220N_1433_3310_13 L15-1204E-1202N_4816_3380_13 L15-0361E-1300N_1446_2989_13 L15-1204E-1204N_4819_3372_13 L15-0368E-1245N_1474_3210_13 L15-1209E-1113N_4838_3737_13 L15-0387E-1276N_1549_3087_13 L15-1210E-1025N_4840_4088_13 L15-0434E-1218N_1736_3318_13 L15-1276E-1107N_5105_3761_13 L15-0457E-1135N_1831_3648_13 L15-1289E-1169N_5156_3514_13 L15-0487E-1246N_1950_3207_13 L15-1296E-1198N_5184_3399_13 L15-0506E-1204N_2027_3374_13 L15-1298E-1322N_5193_2903_13 L15-0544E-1228N_2176_3279_13 L15-1335E-1166N_5342_3524_13 L15-0566E-1185N_2265_3451_13 L15-1389E-1284N_5557_3054_13 L15-0571E-1075N_2287_3888_13 L15-1438E-1134N_5753_3655_13 L15-0577E-1243N_2309_3217_13 L15-1439E-1134N_5759_3655_13 L15-0586E-1127N_2345_3680_13 L15-1479E-1101N_5916_3785_13 L15-0595E-1278N_2383_3079_13 L15-1481E-1119N_5927_3715_13 L15-0614E-0946N_2459_4406_13 L15-1538E-1163N_6154_3539_13 L15-0632E-0892N_2528_4620_13 L15-1615E-1205N_6460_3370_13 L15-0683E-1006N_2732_4164_13 L15-1615E-1206N_6460_3366_13 L15-0760E-0887N_3041_4643_13 L15-1617E-1207N_6468_3360_13 L15-0924E-1108N_3699_3757_13 L15-1669E-1153N_6678_3579_13 L15-0977E-1187N_3911_3441_13 L15-1669E-1160N_6678_3548_13 L15-1014E-1375N_4056_2688_13 L15-1669E-1160N_6679_3549_13 L15-1015E-1062N_4061_3941_13 L15-1672E-1207N_6691_3363_13 L15-1025E-1366N_4102_2726_13 L15-1690E-1211N_6763_3346_13 L15-1049E-1370N_4196_2710_13 L15-1691E-1211N_6764_3347_13 L15-1138E-1216N_4553_3325_13 L15-1703E-1219N_6813_3313_13 L15-1172E-1306N_4688_2967_13 L15-1709E-1112N_6838_3742_13 L15-1185E-0935N_4742_4450_13 L15-1716E-1211N_6864_3345_13 . masked_images = get_masked_images(path, 1) len(masked_images) . 58 . Dataset now consists of 58 correct full images after Datacleaning Step . Cutting images in tiles . Since the images are large (1024x1024), we cut them into 16 smaller tiles (255x255) and save them to disk. Most structures are small in relation to the whole scene, so this should not hurt training too much. Smaller tiles allow for larger batch sizes and/or deeper models to fit in GPU RAM. . Most images have 1024x1024 pixels. Some images however have only 1023 pixels in one dimension, therefore I chose 255 instead of 256 as the tile size. This throws away some pixels in most images, but maintains an equal tile size for all images. . To do: Ideally, we would create overlapping tiles to avoid some buildings being cut in half and never seen in their full shape by the model. . def cut_tiles(tile_size:int): &quot;Cuts the large images and masks into equal tiles and saves them to disk&quot; masked_images = get_masked_images(path, 5) for fn in tqdm(masked_images): scene = fn.parent.parent # Create directories if not os.path.exists(scene/&#39;img_tiles&#39;): os.makedirs(scene/&#39;img_tiles&#39;) if not os.path.exists(scene/&#39;mask_tiles&#39;): os.makedirs(scene/&#39;mask_tiles&#39;) # Create mask for current image img = np.array(PILImage.create(fn)) msk_fn = str(fn).replace(&#39;images_masked&#39;, &#39;binary_mask&#39;) msk = np.array(PILMask.create(msk_fn)) x, y, _ = img.shape # Cut tiles and save them for i in range(x//tile_size): for j in range(y//tile_size): img_tile = img[i*tile_size:(i+1)*tile_size,j*tile_size:(j+1)*tile_size] msk_tile = msk[i*tile_size:(i+1)*tile_size,j*tile_size:(j+1)*tile_size] Image.fromarray(img_tile).save(f&#39;{scene}/img_tiles/{fn.name[:-4]}_{i}_{j}.png&#39;) Image.fromarray(msk_tile).save(f&#39;{scene}/mask_tiles/{fn.name[:-4]}_{i}_{j}.png&#39;) . . . len(masked_images) #These are the images that are after the data cleaning step ready to be cut into tiles. . 58 . #del masked_images[-2] . masked_images . [Path(&#39;L15-0331E-1257N_1327_3160_13/images_masked/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif&#39;), Path(&#39;L15-0357E-1223N_1429_3296_13/images_masked/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif&#39;), Path(&#39;L15-0358E-1220N_1433_3310_13/images_masked/global_monthly_2018_03_mosaic_L15-0358E-1220N_1433_3310_13.tif&#39;), Path(&#39;L15-0361E-1300N_1446_2989_13/images_masked/global_monthly_2018_01_mosaic_L15-0361E-1300N_1446_2989_13.tif&#39;), Path(&#39;L15-0368E-1245N_1474_3210_13/images_masked/global_monthly_2018_01_mosaic_L15-0368E-1245N_1474_3210_13.tif&#39;), Path(&#39;L15-0387E-1276N_1549_3087_13/images_masked/global_monthly_2018_03_mosaic_L15-0387E-1276N_1549_3087_13.tif&#39;), Path(&#39;L15-0434E-1218N_1736_3318_13/images_masked/global_monthly_2018_01_mosaic_L15-0434E-1218N_1736_3318_13.tif&#39;), Path(&#39;L15-0457E-1135N_1831_3648_13/images_masked/global_monthly_2018_01_mosaic_L15-0457E-1135N_1831_3648_13.tif&#39;), Path(&#39;L15-0487E-1246N_1950_3207_13/images_masked/global_monthly_2018_02_mosaic_L15-0487E-1246N_1950_3207_13.tif&#39;), Path(&#39;L15-0506E-1204N_2027_3374_13/images_masked/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13.tif&#39;), Path(&#39;L15-0544E-1228N_2176_3279_13/images_masked/global_monthly_2018_03_mosaic_L15-0544E-1228N_2176_3279_13.tif&#39;), Path(&#39;L15-0566E-1185N_2265_3451_13/images_masked/global_monthly_2018_03_mosaic_L15-0566E-1185N_2265_3451_13.tif&#39;), Path(&#39;L15-0571E-1075N_2287_3888_13/images_masked/global_monthly_2018_03_mosaic_L15-0571E-1075N_2287_3888_13.tif&#39;), Path(&#39;L15-0577E-1243N_2309_3217_13/images_masked/global_monthly_2018_04_mosaic_L15-0577E-1243N_2309_3217_13.tif&#39;), Path(&#39;L15-0586E-1127N_2345_3680_13/images_masked/global_monthly_2018_03_mosaic_L15-0586E-1127N_2345_3680_13.tif&#39;), Path(&#39;L15-0595E-1278N_2383_3079_13/images_masked/global_monthly_2018_02_mosaic_L15-0595E-1278N_2383_3079_13.tif&#39;), Path(&#39;L15-0614E-0946N_2459_4406_13/images_masked/global_monthly_2018_05_mosaic_L15-0614E-0946N_2459_4406_13.tif&#39;), Path(&#39;L15-0632E-0892N_2528_4620_13/images_masked/global_monthly_2018_01_mosaic_L15-0632E-0892N_2528_4620_13.tif&#39;), Path(&#39;L15-0683E-1006N_2732_4164_13/images_masked/global_monthly_2018_02_mosaic_L15-0683E-1006N_2732_4164_13.tif&#39;), Path(&#39;L15-0760E-0887N_3041_4643_13/images_masked/global_monthly_2018_02_mosaic_L15-0760E-0887N_3041_4643_13.tif&#39;), Path(&#39;L15-0924E-1108N_3699_3757_13/images_masked/global_monthly_2018_01_mosaic_L15-0924E-1108N_3699_3757_13.tif&#39;), Path(&#39;L15-0977E-1187N_3911_3441_13/images_masked/global_monthly_2018_04_mosaic_L15-0977E-1187N_3911_3441_13.tif&#39;), Path(&#39;L15-1014E-1375N_4056_2688_13/images_masked/global_monthly_2018_03_mosaic_L15-1014E-1375N_4056_2688_13.tif&#39;), Path(&#39;L15-1015E-1062N_4061_3941_13/images_masked/global_monthly_2018_02_mosaic_L15-1015E-1062N_4061_3941_13.tif&#39;), Path(&#39;L15-1025E-1366N_4102_2726_13/images_masked/global_monthly_2018_01_mosaic_L15-1025E-1366N_4102_2726_13.tif&#39;), Path(&#39;L15-1049E-1370N_4196_2710_13/images_masked/global_monthly_2018_01_mosaic_L15-1049E-1370N_4196_2710_13.tif&#39;), Path(&#39;L15-1138E-1216N_4553_3325_13/images_masked/global_monthly_2018_02_mosaic_L15-1138E-1216N_4553_3325_13.tif&#39;), Path(&#39;L15-1172E-1306N_4688_2967_13/images_masked/global_monthly_2018_01_mosaic_L15-1172E-1306N_4688_2967_13.tif&#39;), Path(&#39;L15-1185E-0935N_4742_4450_13/images_masked/global_monthly_2018_04_mosaic_L15-1185E-0935N_4742_4450_13.tif&#39;), Path(&#39;L15-1200E-0847N_4802_4803_13/images_masked/global_monthly_2018_02_mosaic_L15-1200E-0847N_4802_4803_13.tif&#39;), Path(&#39;L15-1203E-1203N_4815_3378_13/images_masked/global_monthly_2018_02_mosaic_L15-1203E-1203N_4815_3378_13.tif&#39;), Path(&#39;L15-1204E-1202N_4816_3380_13/images_masked/global_monthly_2017_08_mosaic_L15-1204E-1202N_4816_3380_13.tif&#39;), Path(&#39;L15-1204E-1204N_4819_3372_13/images_masked/global_monthly_2018_01_mosaic_L15-1204E-1204N_4819_3372_13.tif&#39;), Path(&#39;L15-1209E-1113N_4838_3737_13/images_masked/global_monthly_2018_01_mosaic_L15-1209E-1113N_4838_3737_13.tif&#39;), Path(&#39;L15-1210E-1025N_4840_4088_13/images_masked/global_monthly_2018_01_mosaic_L15-1210E-1025N_4840_4088_13.tif&#39;), Path(&#39;L15-1276E-1107N_5105_3761_13/images_masked/global_monthly_2018_01_mosaic_L15-1276E-1107N_5105_3761_13.tif&#39;), Path(&#39;L15-1289E-1169N_5156_3514_13/images_masked/global_monthly_2018_03_mosaic_L15-1289E-1169N_5156_3514_13.tif&#39;), Path(&#39;L15-1296E-1198N_5184_3399_13/images_masked/global_monthly_2018_02_mosaic_L15-1296E-1198N_5184_3399_13.tif&#39;), Path(&#39;L15-1298E-1322N_5193_2903_13/images_masked/global_monthly_2018_01_mosaic_L15-1298E-1322N_5193_2903_13.tif&#39;), Path(&#39;L15-1335E-1166N_5342_3524_13/images_masked/global_monthly_2018_01_mosaic_L15-1335E-1166N_5342_3524_13.tif&#39;), Path(&#39;L15-1389E-1284N_5557_3054_13/images_masked/global_monthly_2018_02_mosaic_L15-1389E-1284N_5557_3054_13.tif&#39;), Path(&#39;L15-1438E-1134N_5753_3655_13/images_masked/global_monthly_2018_03_mosaic_L15-1438E-1134N_5753_3655_13.tif&#39;), Path(&#39;L15-1439E-1134N_5759_3655_13/images_masked/global_monthly_2018_02_mosaic_L15-1439E-1134N_5759_3655_13.tif&#39;), Path(&#39;L15-1479E-1101N_5916_3785_13/images_masked/global_monthly_2018_03_mosaic_L15-1479E-1101N_5916_3785_13.tif&#39;), Path(&#39;L15-1481E-1119N_5927_3715_13/images_masked/global_monthly_2018_01_mosaic_L15-1481E-1119N_5927_3715_13.tif&#39;), Path(&#39;L15-1538E-1163N_6154_3539_13/images_masked/global_monthly_2018_01_mosaic_L15-1538E-1163N_6154_3539_13.tif&#39;), Path(&#39;L15-1615E-1205N_6460_3370_13/images_masked/global_monthly_2017_07_mosaic_L15-1615E-1205N_6460_3370_13.tif&#39;), Path(&#39;L15-1615E-1206N_6460_3366_13/images_masked/global_monthly_2017_10_mosaic_L15-1615E-1206N_6460_3366_13.tif&#39;), Path(&#39;L15-1617E-1207N_6468_3360_13/images_masked/global_monthly_2018_02_mosaic_L15-1617E-1207N_6468_3360_13.tif&#39;), Path(&#39;L15-1669E-1153N_6678_3579_13/images_masked/global_monthly_2018_01_mosaic_L15-1669E-1153N_6678_3579_13.tif&#39;), Path(&#39;L15-1669E-1160N_6678_3548_13/images_masked/global_monthly_2017_10_mosaic_L15-1669E-1160N_6678_3548_13.tif&#39;), Path(&#39;L15-1669E-1160N_6679_3549_13/images_masked/global_monthly_2017_10_mosaic_L15-1669E-1160N_6679_3549_13.tif&#39;), Path(&#39;L15-1672E-1207N_6691_3363_13/images_masked/global_monthly_2018_02_mosaic_L15-1672E-1207N_6691_3363_13.tif&#39;), Path(&#39;L15-1690E-1211N_6763_3346_13/images_masked/global_monthly_2017_08_mosaic_L15-1690E-1211N_6763_3346_13.tif&#39;), Path(&#39;L15-1691E-1211N_6764_3347_13/images_masked/global_monthly_2018_02_mosaic_L15-1691E-1211N_6764_3347_13.tif&#39;), Path(&#39;L15-1703E-1219N_6813_3313_13/images_masked/global_monthly_2018_01_mosaic_L15-1703E-1219N_6813_3313_13.tif&#39;), Path(&#39;L15-1709E-1112N_6838_3742_13/images_masked/global_monthly_2018_01_mosaic_L15-1709E-1112N_6838_3742_13.tif&#39;), Path(&#39;L15-1716E-1211N_6864_3345_13/images_masked/global_monthly_2018_02_mosaic_L15-1716E-1211N_6864_3345_13.tif&#39;)] . path.ls() . (#59) [Path(&#39;L15-0331E-1257N_1327_3160_13&#39;),Path(&#39;L15-0357E-1223N_1429_3296_13&#39;),Path(&#39;L15-0358E-1220N_1433_3310_13&#39;),Path(&#39;L15-0361E-1300N_1446_2989_13&#39;),Path(&#39;L15-0368E-1245N_1474_3210_13&#39;),Path(&#39;L15-0387E-1276N_1549_3087_13&#39;),Path(&#39;L15-0434E-1218N_1736_3318_13&#39;),Path(&#39;L15-0457E-1135N_1831_3648_13&#39;),Path(&#39;L15-0487E-1246N_1950_3207_13&#39;),Path(&#39;L15-0506E-1204N_2027_3374_13&#39;)...] . path.ls() . (#59) [Path(&#39;L15-0331E-1257N_1327_3160_13&#39;),Path(&#39;L15-0357E-1223N_1429_3296_13&#39;),Path(&#39;L15-0358E-1220N_1433_3310_13&#39;),Path(&#39;L15-0361E-1300N_1446_2989_13&#39;),Path(&#39;L15-0368E-1245N_1474_3210_13&#39;),Path(&#39;L15-0387E-1276N_1549_3087_13&#39;),Path(&#39;L15-0434E-1218N_1736_3318_13&#39;),Path(&#39;L15-0457E-1135N_1831_3648_13&#39;),Path(&#39;L15-0487E-1246N_1950_3207_13&#39;),Path(&#39;L15-0506E-1204N_2027_3374_13&#39;)...] . Data Loading Functions . Little helper functions . def get_image_tiles(path:Path, n_tiles=TILES_PER_SCENE) -&gt; L: &quot;Returns a list of the first `n` image tile filenames in `path`&quot; files = L() for folder in path.ls(): files.extend(get_image_files(path=folder, folders=&#39;img_tiles&#39;)[:n_tiles]) return files . def get_y_fn(fn:Path) -&gt; str: &quot;Returns filename of the associated mask tile for a given image tile&quot; return str(fn).replace(&#39;img_tiles&#39;, &#39;mask_tiles&#39;) . def get_y(fn:Path) -&gt; PILMask: &quot;Returns a PILMask object of 0s and 1s for a given tile&quot; fn = get_y_fn(fn) msk = np.array(PILMask.create(fn)) msk[msk==255] = 1 return PILMask.create(msk) . Visualizing Data . Let&#39;s look at some raw image tiles and their masks. . def show_tiles(n): all_tiles = get_image_tiles(path) subset = random.sample(all_tiles, n) fig, ax = plt.subplots(n//2, 4, figsize=(14,14)) for i in range(n): y = i//2 x = 2*i%4 PILImage.create(subset[i]).show(ctx=ax[y, x]) get_y(subset[i]).show(ctx=ax[y, x+1], cmap=&#39;cividis&#39;) fig.tight_layout() plt.show() . . show_tiles(8) . Challenges of the dataset . As we can see in the visualizations, the dataset provides some challenges: . The buildings are often extremely small, just a few pixels, and very close to each other | On the other hand, there are large structures that cover a much greater area than small buildings | Some buildings are hard to recognize, even for the human eye | The density of buildings varies greatly. There are tiles with no buildings at all, other tiles show urban scenes with hundreds of buildings | The images are very diverse, with great differences in topography, vegetation and urbanization | Some tiles are covered partially or completely with UDM masks | . Distribution of building density . To explore how imbalanced the data is exactly, we&#39;ll analyze the percentages of building pixels in each tile. We create a simple dataloader to easily load and analyze the masks. . tiles = DataBlock( blocks = (ImageBlock(),MaskBlock(codes=CODES)), get_items = get_image_tiles, get_y = get_y ) dls = tiles.dataloaders(path, bs=BATCH_SIZE) dls.vocab = CODES . . targs = torch.zeros((0,255,255)) for _, masks in dls[0]: targs = torch.cat((targs, masks.cpu()), dim=0) targs.shape . torch.Size([732, 255, 255]) . We have 732 image tiles in total. . Calculating the percentage of building pixels vs background pixels: . total_pixels = targs.shape[1]**2 percentages = torch.count_nonzero(targs, dim=(1,2))/total_pixels plt.hist(percentages, bins=20) plt.ylabel(&#39;Number of tiles&#39;) plt.xlabel(&#39;Ratio of pixels that are of class `building`&#39;) plt.gca().spines[&#39;top&#39;].set_color(&#39;none&#39;) plt.gca().spines[&#39;right&#39;].set_color(&#39;none&#39;) plt.show() . . We can see that many tiles contain no or very few buildings. . torch.count_nonzero((percentages==0.).float()).item() . 64 . 64 images do not contain a single pixel of the building class, that&#39;s almost 10% of the images. These can be areas of empty land, water, or tiles that were covered in clouds. . What is the tile with the largest percentage of buildings? . targs[percentages.argsort(descending=True)[0]].show(); . What is the overall ratio building/background? . print(percentages.mean().item(), percentages.median().item()) . 0.06513693183660507 0.036139946430921555 . On average, 6.5% of a tile&#39;s pixels are of the building class. The median is only 3.6%. This means this is a rather imbalanced dataset. . Validation Strategy . To allow the evaluation of the performance of our model, we set aside 15% of the dataset as validation set. . We must be thoughtful about how we create this validation set. Using random images would be too easy, as we have several images per scene that differ only slightly. Our validation set would not be thoroughly separated from the training set. . Instead, I chose to randomly select 9 scenes that are used as validation data. The model will never see any images from these scenes during training. . VALID_SCENES = [&#39;L15-0571E-1075N_2287_3888_13&#39;, &#39;L15-1615E-1205N_6460_3370_13&#39;, &#39;L15-1210E-1025N_4840_4088_13&#39;, &#39;L15-1185E-0935N_4742_4450_13&#39;, &#39;L15-1481E-1119N_5927_3715_13&#39;, &#39;L15-0632E-0892N_2528_4620_13&#39;, &#39;L15-1438E-1134N_5753_3655_13&#39;, &#39;L15-0924E-1108N_3699_3757_13&#39;, &#39;L15-0457E-1135N_1831_3648_13&#39;] . def valid_split(item): scene = item.parent.parent.name return scene in VALID_SCENES . Undersampling . To help mitigating the imbalanced classes, we remove all tiles that contain no buildings at all from the training set. This reduces the amount of samples by ~10%, thereby accelerating the training while helping the model perform better. . def has_buildings(fn:Path) -&gt; bool: &quot;&quot;&quot;Returns whether the mask of a given image tile contains at least one pixel of a building&quot;&quot;&quot; fn = get_y_fn(fn) msk = tensor(PILMask.create(fn)) count = torch.count_nonzero(msk) return count&gt;0. . . def get_undersampled_tiles(path:Path) -&gt; L: &quot;&quot;&quot;Returns a list of image tile filenames in `path`. For tiles in the training set, empty tiles are ignored. All tiles in the validation set are included.&quot;&quot;&quot; files = get_image_tiles(path) train_idxs, valid_idxs = FuncSplitter(valid_split)(files) train_files = L(filter(has_buildings, files[train_idxs])) valid_files = files[valid_idxs] return train_files + valid_files . . Creating Dataloaders . The following transformations seem reasonable for satellite images. We flip the tiles vertically and horizontally, rotate them, change brightness, contrast and saturation by a small amount. We normalize them according to ImageNet stats, so that we can use a pretrained model later. . tfms = [Dihedral(0.5), # Horizontal and vertical flip Rotate(max_deg=180, p=0.9), # Rotation in any direction possible Brightness(0.2, p=0.75), Contrast(0.2), Saturation(0.2), Normalize.from_stats(*imagenet_stats)] . To create the datasets, we use the convenient DataBlock API of fastai. We only load 16 tiles per scene, so only 1 image per region. . tiles = DataBlock( blocks = (ImageBlock(),MaskBlock(codes=CODES)), # Independent variable is Image, dependent variable is Mask get_items = get_undersampled_tiles, # Collect undersampled tiles get_y = get_y, # Get dependent variable: mask splitter = FuncSplitter(valid_split), # Split into training and validation set batch_tfms = tfms # Transforms on GPU: augmentation, normalization ) . dls = tiles.dataloaders(path, bs=BATCH_SIZE) dls.vocab = CODES . len(dls.train_ds), len(dls.valid_ds) . (715, 144) . We have 715 correct tiles in the training set and 144 tiles in the validation set. . Making sure the batches look okay: . inputs, targets = dls.one_batch() . inputs.shape, targets.shape . (torch.Size([12, 3, 255, 255]), torch.Size([12, 255, 255])) . These dimensions are as expected: . 12 images per batch | 3 channels for the input images | no color channels for the target mask | image size: 255x255. | . Check that the mask looks as expected, 0s and 1s: . targets[0].unique() . TensorMask([0, 1], device=&#39;cuda:0&#39;) . Defining the Model . The task at hand is an image segmentation problem. In the original competition, it is required to assign individual labels to each building to keep track of it over time (instance segmentation). Here, I chose to do semantic segmentation instead, so just classifying for every pixel if it belongs to a building or not. . The fastai library allows the remarkably simple creation of a U-Net, a standard architecture for image segmentation problems. The module DynamicUNet - provided with an encoder architecture - automatically constructs a decoder and cross connections. This makes it easy to build a U-Net out of different (and pretrained) architectures. I chose this approach to have more time to experiment instead of writing code from scratch. I considered following aspects: . Encoder: I picked a xResNet34 model that has been pretrained on ImageNet. A 34-layer encoder seems like a good compromise between accuracy and memory/compute requirements. | Loss function: The choice of the loss function is important for segmentation problems. I&#39;ll use a weighted pixel-wise cross-entropy loss. The weights are important for the imbalanced dataset. | Optimizer: I use the default optimizer, Adam. | Metrics: As the classes are very imbalanced, a simple accuracy metric would not be helpful. In a picture with 3% buildings, the model could predict &quot;no building&quot; on every pixel and still get 97% accuracy. | Instead, I focus on the Dice metric, it is often used for segmentation tasks. It is equivalent to the F1 score and measures the ratio of $ frac{2TP}{2TP + FP + FN}$ | Additionally, I added foreground_acc of fastai, it measures the percentage of foreground pixels correctly classified, the Recall. Foreground in this case is the building class. | . | . weights = Tensor(CLASS_WEIGHTS).cuda() loss_func = CrossEntropyLossFlat(axis=1, weight=weights) . With some experimentation, the class weights 0.25 for the background and 0.75 for the building class seem to work fine. . learn = unet_learner(dls, # DataLoaders ARCHITECTURE, # xResNet34 loss_func = loss_func, # Weighted cross entropy loss opt_func = Adam, # Adam optimizer metrics = [Dice(), foreground_acc], # Custom metrics self_attention = False, cbs = [SaveModelCallback( monitor=&#39;dice&#39;, comp=np.greater, fname=&#39;best-model-34&#39; )] ) . Summary of the model: . learn.summary() . DynamicUnet (Input shape: 12) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 12 x 32 x 128 x 128 Conv2d 864 False BatchNorm2d 64 True ReLU Conv2d 9216 False BatchNorm2d 64 True ReLU ____________________________________________________________________________ 12 x 64 x 128 x 128 Conv2d 18432 False BatchNorm2d 128 True ReLU MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 8192 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 32768 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 131072 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Sequential ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Sequential ReLU BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 1024 x 8 x 8 Conv2d 4719616 True ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 4719104 True ReLU ____________________________________________________________________________ 12 x 1024 x 8 x 8 Conv2d 525312 True ReLU PixelShuffle BatchNorm2d 512 True Conv2d 2359808 True ReLU Conv2d 2359808 True ReLU ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 525312 True ReLU PixelShuffle BatchNorm2d 256 True Conv2d 1327488 True ReLU Conv2d 1327488 True ReLU ReLU ____________________________________________________________________________ 12 x 768 x 32 x 32 Conv2d 295680 True ReLU PixelShuffle BatchNorm2d 128 True Conv2d 590080 True ReLU Conv2d 590080 True ReLU ReLU ____________________________________________________________________________ 12 x 512 x 64 x 64 Conv2d 131584 True ReLU PixelShuffle BatchNorm2d 128 True ____________________________________________________________________________ 12 x 96 x 128 x 128 Conv2d 165984 True ReLU Conv2d 83040 True ReLU ReLU ____________________________________________________________________________ 12 x 384 x 128 x 12 Conv2d 37248 True ReLU PixelShuffle ResizeToOrig MergeLayer Conv2d 88308 True ReLU Conv2d 88308 True Sequential ReLU ____________________________________________________________________________ 12 x 2 x 255 x 255 Conv2d 200 True ToTensorBase ____________________________________________________________________________ Total params: 41,240,400 Total trainable params: 19,953,648 Total non-trainable params: 21,286,752 Optimizer used: &lt;function Adam at 0x7fea8c0c5200&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - SaveModelCallback . . This is the full UNet model: . learn.model . DynamicUnet( (layers): ModuleList( (0): Sequential( (0): ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (5): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (6): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (4): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (5): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (7): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) ) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Sequential( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) ) (4): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (5): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (6): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (7): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (8): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (9): ResizeToOrig() (10): MergeLayer() (11): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(99, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(99, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (12): ConvLayer( (0): Conv2d(99, 2, kernel_size=(1, 1), stride=(1, 1)) ) (13): ToTensorBase(tensor_cls=&lt;class &#39;fastai.torch_core.TensorBase&#39;&gt;) ) ) . . Training . We can use fastai&#39;s learning rate finder to pick a reasonable learning rate: . learn.lr_find(suggestions=False)) . SuggestedLRs(valley=tensor(6.3096e-05)) . Somewhere around 1e-4 seems reasonable, where the loss decreases steadily. . lr_max = LR_MAX # 3e-4 . We unfreeze the model to train the encoder and decoder simultaneously. The encoder should be trained at a lower learning rate, since we don&#39;t want to change the pretrained features too much. This is achieved by setting the learning rate to slice(lr_max/10, lr_max) . We use the fit_one_cycle method, where the learning rate starts low for a warm up period, reaches a maximum of lr_max and then anneals to 0 at the end of training. . !nvidia-smi . Tue Jul 6 07:59:28 2021 +--+ | NVIDIA-SMI 465.27 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 60C P0 29W / 70W | 8166MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . ls . L15-0331E-1257N_1327_3160_13/ L15-1203E-1203N_4815_3378_13/ L15-0357E-1223N_1429_3296_13/ L15-1204E-1202N_4816_3380_13/ L15-0358E-1220N_1433_3310_13/ L15-1204E-1204N_4819_3372_13/ L15-0361E-1300N_1446_2989_13/ L15-1209E-1113N_4838_3737_13/ L15-0368E-1245N_1474_3210_13/ L15-1210E-1025N_4840_4088_13/ L15-0387E-1276N_1549_3087_13/ L15-1276E-1107N_5105_3761_13/ L15-0434E-1218N_1736_3318_13/ L15-1289E-1169N_5156_3514_13/ L15-0457E-1135N_1831_3648_13/ L15-1296E-1198N_5184_3399_13/ L15-0487E-1246N_1950_3207_13/ L15-1298E-1322N_5193_2903_13/ L15-0506E-1204N_2027_3374_13/ L15-1335E-1166N_5342_3524_13/ L15-0544E-1228N_2176_3279_13/ L15-1389E-1284N_5557_3054_13/ L15-0566E-1185N_2265_3451_13/ L15-1438E-1134N_5753_3655_13/ L15-0571E-1075N_2287_3888_13/ L15-1439E-1134N_5759_3655_13/ L15-0577E-1243N_2309_3217_13/ L15-1479E-1101N_5916_3785_13/ L15-0586E-1127N_2345_3680_13/ L15-1481E-1119N_5927_3715_13/ L15-0595E-1278N_2383_3079_13/ L15-1538E-1163N_6154_3539_13/ L15-0614E-0946N_2459_4406_13/ L15-1615E-1205N_6460_3370_13/ L15-0632E-0892N_2528_4620_13/ L15-1615E-1206N_6460_3366_13/ L15-0683E-1006N_2732_4164_13/ L15-1617E-1207N_6468_3360_13/ L15-0760E-0887N_3041_4643_13/ L15-1669E-1153N_6678_3579_13/ L15-0924E-1108N_3699_3757_13/ L15-1669E-1160N_6678_3548_13/ L15-0977E-1187N_3911_3441_13/ L15-1669E-1160N_6679_3549_13/ L15-1014E-1375N_4056_2688_13/ L15-1672E-1207N_6691_3363_13/ L15-1015E-1062N_4061_3941_13/ L15-1690E-1211N_6763_3346_13/ L15-1025E-1366N_4102_2726_13/ L15-1691E-1211N_6764_3347_13/ L15-1049E-1370N_4196_2710_13/ L15-1703E-1219N_6813_3313_13/ L15-1138E-1216N_4553_3325_13/ L15-1709E-1112N_6838_3742_13/ L15-1172E-1306N_4688_2967_13/ L15-1716E-1211N_6864_3345_13/ L15-1185E-0935N_4742_4450_13/ models/ L15-1200E-0847N_4802_4803_13/ . !ls . gdrive sample_data . --Experiment Run: 1 . #collapse-output learn.unfreeze() learn.fit_one_cycle( EPOCHS, lr_max=slice(lr_max/ENCODER_FACTOR, lr_max), cbs=[WandbCallback()] ) . epoch train_loss valid_loss dice foreground_acc time . 0 | 0.388443 | 0.413384 | 0.289366 | 0.174573 | 00:50 | . 1 | 0.349046 | 0.359303 | 0.461197 | 0.445781 | 00:44 | . 2 | 0.332909 | 0.349615 | 0.490519 | 0.481620 | 00:45 | . 3 | 0.328296 | 0.404246 | 0.307558 | 0.176636 | 00:44 | . 4 | 0.311649 | 0.331803 | 0.518899 | 0.552372 | 00:44 | . 5 | 0.315864 | 0.333278 | 0.496052 | 0.437288 | 00:45 | . 6 | 0.305936 | 0.369581 | 0.500008 | 0.671848 | 00:45 | . 7 | 0.306035 | 0.335628 | 0.522586 | 0.520908 | 00:45 | . 8 | 0.295921 | 0.330779 | 0.530826 | 0.513260 | 00:46 | . 9 | 0.290359 | 0.311421 | 0.545738 | 0.579524 | 00:46 | . 10 | 0.294897 | 0.325334 | 0.530101 | 0.488870 | 00:46 | . 11 | 0.286177 | 0.327654 | 0.539763 | 0.652674 | 00:46 | . 12 | 0.278001 | 0.354929 | 0.485753 | 0.677047 | 00:46 | . 13 | 0.275921 | 0.346134 | 0.534323 | 0.538300 | 00:46 | . 14 | 0.271893 | 0.336453 | 0.529238 | 0.586955 | 00:46 | . 15 | 0.270011 | 0.351094 | 0.522521 | 0.472264 | 00:46 | . 16 | 0.266521 | 0.455999 | 0.513976 | 0.443002 | 00:46 | . 17 | 0.260103 | 0.323886 | 0.543388 | 0.594548 | 00:46 | . 18 | 0.259548 | 0.317344 | 0.557026 | 0.604788 | 00:46 | . 19 | 0.252726 | 0.342917 | 0.537662 | 0.612756 | 00:46 | . 20 | 0.248773 | 0.313652 | 0.553115 | 0.602638 | 00:46 | . 21 | 0.244121 | 0.335954 | 0.549108 | 0.563031 | 00:46 | . 22 | 0.246857 | 0.315527 | 0.554965 | 0.656071 | 00:46 | . 23 | 0.246583 | 0.309595 | 0.560354 | 0.621056 | 00:46 | . 24 | 0.246868 | 0.322774 | 0.548355 | 0.565873 | 00:46 | . 25 | 0.243858 | 0.314193 | 0.549589 | 0.677579 | 00:46 | . 26 | 0.238657 | 0.318479 | 0.553847 | 0.552282 | 00:46 | . 27 | 0.236678 | 0.319910 | 0.553423 | 0.588908 | 00:46 | . 28 | 0.234124 | 0.306141 | 0.552694 | 0.669329 | 00:46 | . 29 | 0.230853 | 0.328426 | 0.550115 | 0.606573 | 00:46 | . 30 | 0.232298 | 0.314922 | 0.556050 | 0.686611 | 00:46 | . 31 | 0.230854 | 0.324863 | 0.551161 | 0.623305 | 00:46 | . 32 | 0.231132 | 0.321707 | 0.548863 | 0.629805 | 00:46 | . 33 | 0.231481 | 0.318916 | 0.552225 | 0.637253 | 00:46 | . 34 | 0.225996 | 0.320821 | 0.552282 | 0.626153 | 00:46 | . 35 | 0.220080 | 0.326714 | 0.551576 | 0.615147 | 00:46 | . 36 | 0.225850 | 0.327396 | 0.551351 | 0.619062 | 00:47 | . 37 | 0.227767 | 0.326052 | 0.552426 | 0.623551 | 00:46 | . 38 | 0.221003 | 0.317112 | 0.554034 | 0.647620 | 00:46 | . 39 | 0.220426 | 0.318014 | 0.554319 | 0.637630 | 00:46 | . Better model found at epoch 0 with dice value: 0.289366090884329. Better model found at epoch 1 with dice value: 0.4611968357497782. Better model found at epoch 2 with dice value: 0.4905194190974164. Better model found at epoch 4 with dice value: 0.518899214365881. Better model found at epoch 7 with dice value: 0.5225864940854509. Better model found at epoch 8 with dice value: 0.5308255345561974. Better model found at epoch 9 with dice value: 0.5457375700715181. Better model found at epoch 18 with dice value: 0.557025656659146. Better model found at epoch 23 with dice value: 0.5603543324449821. . . learn.recorder.plot_loss() . learn.save(&#39;best-model-34&#39;) . Path(&#39;models/best-model-34.pth&#39;) . The best model at epoch 23 has a Dice score of 0.56. The theoretic maximum - a perfect segmentation - is 1.0. . learn.recorder.plot_loss() . run.finish() . Waiting for W&amp;B process to finish, PID 6232Program ended successfully. Find user logs for this run at: /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train/wandb/run-20210706_090514-3cwyiul0/logs/debug.log Find internal logs for this run at: /content/gdrive/Shareddrives/Undrive/s7/SN7_buildings_train/train/wandb/run-20210706_090514-3cwyiul0/logs/debug-internal.log Synced 4 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s) Synced laced-glade-14: https://wandb.ai/priyank7n/s7/runs/3cwyiul0 Visualizing the Results . . Note: The following results actually come from the model of a different training run, also with a Dice score of 0.57 Let&#39;s get all predictions on the validation set and have a look at them. . probs,targets,preds,losses = learn.get_preds(dl=dls.valid, with_loss=True, with_decoded=True, act=None) . Sort descending by loss: . loss_sorted = torch.argsort(losses, descending=True) n = len(loss_sorted) . Helper function to show predictions: . def show_single_pred(index:int): fig, ax = plt.subplots(1, 4, figsize=(20,5)) dls.valid_ds[index][0].show(ctx=ax[0]); ax[0].set_title(&quot;Input&quot;) show_at(dls.valid_ds, index, cmap=&#39;Blues&#39;, ctx=ax[1]); ax[1].set_title(&quot;Target&quot;) preds[index].show(cmap=&#39;Blues&#39;, ctx=ax[2]); ax[2].set_title(&quot;Prediction Mask&quot;) probs[index][1].show(cmap=&#39;viridis&#39;, ctx=ax[3]); ax[3].set_title(&quot;Building class probability&quot;) . . Plot the samples with the highest losses . for idx in loss_sorted[:3]: print(f&#39;Tile #{idx}, loss: {losses[idx]}&#39;) show_single_pred(idx) . . Tile #110, loss: 0.7653330564498901 Tile #101, loss: 0.541749119758606 Tile #111, loss: 0.4768364429473877 . All images with the highest losses show dense urban areas. What&#39;s noticable is the trouble that the model has with large buildings, which are often completely overlooked. It&#39;s also clear that very small buildings are merged into &quot;blobs&quot;. I suspect tracking individual buildings could be difficult here. . Plot samples with medium losses . for idx in loss_sorted[n//2-1:n//2+2]: print(f&#39;Tile #{idx}, loss: {losses[idx]}&#39;) show_single_pred(idx) . . Tile #1, loss: 0.06586822122335434 Tile #7, loss: 0.06451009213924408 Tile #55, loss: 0.0639784187078476 . The model tends to merge small buildings into larger blobs, and there are several false positives. But there are also some quite good predictions, picking up buildings that are hard even for the human eye to pick up. . Plot some examples with low losses . for idx in loss_sorted[-21:-18]: print(f&#39;Tile #{idx}, loss: {losses[idx]}&#39;) show_single_pred(idx) . . Tile #51, loss: 0.0051777600310742855 Tile #92, loss: 0.005046222358942032 Tile #9, loss: 0.004601902794092894 . The model shows mixed performance in images with few buildings in them. Overall, the accuracy looks better here than in dense areas. But the model tends to produce false positives; and some tiles show weird artifacts in the corners. It seems as if the model interprets the corners itself as buildings, especially on tiles covered with water. . Show complete scenes . Predict all tiles of a scene: . path.ls() . (#60) [Path(&#39;L15-0760E-0887N_3041_4643_13&#39;),Path(&#39;L15-0683E-1006N_2732_4164_13&#39;),Path(&#39;L15-0632E-0892N_2528_4620_13&#39;),Path(&#39;L15-0614E-0946N_2459_4406_13&#39;),Path(&#39;L15-0595E-1278N_2383_3079_13&#39;),Path(&#39;L15-0586E-1127N_2345_3680_13&#39;),Path(&#39;L15-0977E-1187N_3911_3441_13&#39;),Path(&#39;L15-0924E-1108N_3699_3757_13&#39;),Path(&#39;L15-1014E-1375N_4056_2688_13&#39;),Path(&#39;L15-1025E-1366N_4102_2726_13&#39;)...] . def save_predictions(scene, path=path) -&gt; None: &quot;Predicts all 16 tiles of one scene and saves them to disk&quot; output_folder = path/scene/&#39;predicted_tiles&#39; if not os.path.exists(output_folder): os.makedirs(output_folder) tiles = get_image_files(path/scene/&#39;img_tiles&#39;).sorted() for i in range(16): tile_preds = learn.predict(tiles[i]) to_image(tile_preds[2][1].repeat(3,1,1)).save(output_folder/f&#39;{i:02d}.png&#39;) . . VALID_SCENES . [&#39;L15-0571E-1075N_2287_3888_13&#39;, &#39;L15-1615E-1205N_6460_3370_13&#39;, &#39;L15-1210E-1025N_4840_4088_13&#39;, &#39;L15-1185E-0935N_4742_4450_13&#39;, &#39;L15-1481E-1119N_5927_3715_13&#39;, &#39;L15-0632E-0892N_2528_4620_13&#39;, &#39;L15-1438E-1134N_5753_3655_13&#39;, &#39;L15-0924E-1108N_3699_3757_13&#39;, &#39;L15-0457E-1135N_1831_3648_13&#39;] . scene = VALID_SCENES[0] # &#39;L15-0571E-1075N_2287_3888_13&#39; . scene = VALID_SCENES[0:] # &#39; . scene . &#39;L15-1210E-1025N_4840_4088_13&#39; . save_predictions(scene) . Helper function to show several tiles as a large image: . def unblockshaped(arr, h, w): &quot;&quot;&quot; Return an array of shape (h, w) where h * w = arr.size If arr is of shape (n, nrows, ncols), n sublocks of shape (nrows, ncols), then the returned array preserves the &quot;physical&quot; layout of the sublocks. Source: https://stackoverflow.com/a/16873755 &quot;&quot;&quot; try: # with color channel n, nrows, ncols, c = arr.shape return (arr.reshape(h//nrows, -1, nrows, ncols, c) .swapaxes(1,2) .reshape(h, w, c)) except ValueError: # without color channel n, nrows, ncols = arr.shape return (arr.reshape(h//nrows, -1, nrows, ncols) .swapaxes(1,2) .reshape(h, w)) . . Load saved predictions: . def get_saved_preds(scene, path=path): &quot;Load saved prediction mask tiles for a scene and return image + assembled mask&quot; image_file = (path/scene/&#39;images_masked&#39;).ls()[0] image = load_image(image_file) mask_tiles = get_image_files(path/scene/&#39;predicted_tiles&#39;).sorted() mask_arrs = np.array(list(maps(partial(load_image, mode=&quot;L&quot;), np.asarray, mask_tiles))) mask_array = unblockshaped(np.array(mask_arrs), 1020, 1020) return (image, mask_array) . . Show image + stitched predictions: . def show_complete_preds(image, mask_array, scene): &quot;Source: https://github.com/CosmiQ/CosmiQ_SN7_Baseline/blob/master/notebooks/sn7_baseline.ipynb&quot; figsize = (25, 16) fig, (ax0, ax1) = plt.subplots(1, 2, figsize=figsize) _ = ax0.imshow(image) ax0.set_xticks([]) ax0.set_yticks([]) ax0.set_title(&#39;Image&#39;) _ = ax1.imshow(mask_array, cmap=&#39;viridis&#39;) ax1.set_xticks([]) ax1.set_yticks([]) ax1.set_title(&#39;Prediction Mask&#39;) plt.suptitle(scene) plt.tight_layout() plt.savefig(os.path.join(path, scene + &#39;_im0+mask0+dice575.png&#39;)) plt.show() . . show_complete_preds(*get_saved_preds(scene), scene) . VALID_SCENES . [&#39;L15-0571E-1075N_2287_3888_13&#39;, &#39;L15-1615E-1205N_6460_3370_13&#39;, &#39;L15-1210E-1025N_4840_4088_13&#39;, &#39;L15-1185E-0935N_4742_4450_13&#39;, &#39;L15-1481E-1119N_5927_3715_13&#39;, &#39;L15-0632E-0892N_2528_4620_13&#39;, &#39;L15-1438E-1134N_5753_3655_13&#39;, &#39;L15-0924E-1108N_3699_3757_13&#39;, &#39;L15-0457E-1135N_1831_3648_13&#39;] . scene = VALID_SCENES[0:] . from time import sleep for scene in VALID_SCENES: save_predictions(scene) show_complete_preds(*get_saved_preds(scene), scene) time.sleep(1) . . #Initiating Run2 . --Experiment Run: 2 . del learn . learn = unet_learner(dls, # DataLoaders ARCHITECTURE, # xResNet34 loss_func = loss_func, # Weighted cross entropy loss opt_func = Adam, # Adam optimizer metrics = [Dice(), foreground_acc], # Custom metrics self_attention = False, cbs = [SaveModelCallback( monitor=&#39;dice&#39;, comp=np.greater, fname=&#39;best-model&#39; )] ) . learn.summary() . DynamicUnet (Input shape: 12) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 12 x 32 x 128 x 128 Conv2d 864 False BatchNorm2d 64 True ReLU Conv2d 9216 False BatchNorm2d 64 True ReLU ____________________________________________________________________________ 12 x 64 x 128 x 128 Conv2d 18432 False BatchNorm2d 128 True ReLU MaxPool2d Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True Sequential ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 73728 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 8192 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 294912 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 32768 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True Sequential ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 1179648 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 131072 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Sequential ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True Sequential ReLU BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 1024 x 8 x 8 Conv2d 4719616 True ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 4719104 True ReLU ____________________________________________________________________________ 12 x 1024 x 8 x 8 Conv2d 525312 True ReLU PixelShuffle BatchNorm2d 512 True Conv2d 2359808 True ReLU Conv2d 2359808 True ReLU ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 525312 True ReLU PixelShuffle BatchNorm2d 256 True Conv2d 1327488 True ReLU Conv2d 1327488 True ReLU ReLU ____________________________________________________________________________ 12 x 768 x 32 x 32 Conv2d 295680 True ReLU PixelShuffle BatchNorm2d 128 True Conv2d 590080 True ReLU Conv2d 590080 True ReLU ReLU ____________________________________________________________________________ 12 x 512 x 64 x 64 Conv2d 131584 True ReLU PixelShuffle BatchNorm2d 128 True ____________________________________________________________________________ 12 x 96 x 128 x 128 Conv2d 165984 True ReLU Conv2d 83040 True ReLU ReLU ____________________________________________________________________________ 12 x 384 x 128 x 12 Conv2d 37248 True ReLU PixelShuffle ResizeToOrig MergeLayer Conv2d 88308 True ReLU Conv2d 88308 True Sequential ReLU ____________________________________________________________________________ 12 x 2 x 255 x 255 Conv2d 200 True ____________________________________________________________________________ Total params: 41,240,400 Total trainable params: 19,953,648 Total non-trainable params: 21,286,752 Optimizer used: &lt;function Adam at 0x7eff70070cb0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - SaveModelCallback . . learn.model . DynamicUnet( (layers): ModuleList( (0): Sequential( (0): ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (5): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (6): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (4): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (5): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (7): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) ) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Sequential( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) ) (4): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (5): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (6): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (7): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (8): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (9): ResizeToOrig() (10): MergeLayer() (11): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(99, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(99, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (12): ConvLayer( (0): Conv2d(99, 2, kernel_size=(1, 1), stride=(1, 1)) ) ) ) . . . learn.lr_find(suggestions=False) . # Run2 learn.unfreeze() learn.fit_one_cycle( EPOCHS, lr_max=slice(lr_max/ENCODER_FACTOR, lr_max), cbs=[WandbCallback()] ) . epoch train_loss valid_loss dice foreground_acc time . 0 | 1.937740 | 0.952615 | 0.182434 | 0.229873 | 00:43 | . 1 | 0.905160 | 0.588179 | 0.333524 | 0.360573 | 00:46 | . 2 | 0.587758 | 0.485159 | 0.409871 | 0.416817 | 00:47 | . 3 | 0.461647 | 0.448966 | 0.419921 | 0.572241 | 00:47 | . 4 | 0.431045 | 0.412626 | 0.434957 | 0.437551 | 00:47 | . 5 | 0.384136 | 0.399362 | 0.441417 | 0.484893 | 00:48 | . 6 | 0.372647 | 0.469187 | 0.417687 | 0.342526 | 00:49 | . 7 | 0.358055 | 0.401312 | 0.433877 | 0.474332 | 00:48 | . 8 | 0.335158 | 0.426308 | 0.409999 | 0.420248 | 00:49 | . 9 | 0.340261 | 0.388585 | 0.453323 | 0.680999 | 00:49 | . 10 | 0.350476 | 0.424114 | 0.427702 | 0.423918 | 00:49 | . 11 | 0.340361 | 0.830213 | 0.363972 | 0.274424 | 00:49 | . 12 | 0.347802 | 0.396467 | 0.447701 | 0.424461 | 00:49 | . 13 | 0.347854 | 0.361305 | 0.471549 | 0.464989 | 00:49 | . 14 | 0.322265 | 0.496307 | 0.421688 | 0.320692 | 00:49 | . 15 | 0.312543 | 0.419182 | 0.407756 | 0.352709 | 00:49 | . 16 | 0.304165 | 0.388873 | 0.451288 | 0.371343 | 00:48 | . 17 | 0.299325 | 0.477858 | 0.439302 | 0.351467 | 00:48 | . 18 | 0.308990 | 0.409736 | 0.413224 | 0.461884 | 00:48 | . 19 | 0.288411 | 0.498319 | 0.370609 | 0.280490 | 00:48 | . 20 | 0.281468 | 0.360414 | 0.485021 | 0.485740 | 00:48 | . 21 | 0.274416 | 0.418477 | 0.475279 | 0.414546 | 00:48 | . 22 | 0.278710 | 0.325033 | 0.520833 | 0.521762 | 00:48 | . 23 | 0.270224 | 0.359991 | 0.506736 | 0.490314 | 00:49 | . 24 | 0.278280 | 0.342068 | 0.522744 | 0.638762 | 00:48 | . 25 | 0.268276 | 0.384951 | 0.448934 | 0.658303 | 00:48 | . 26 | 0.266887 | 0.318082 | 0.537392 | 0.628973 | 00:48 | . 27 | 0.262979 | 0.364846 | 0.507638 | 0.516916 | 00:48 | . 28 | 0.257349 | 0.333423 | 0.531541 | 0.588342 | 00:48 | . 29 | 0.254129 | 0.337612 | 0.534185 | 0.635413 | 00:47 | . 30 | 0.256843 | 0.371552 | 0.523938 | 0.519276 | 00:47 | . 31 | 0.252976 | 0.376065 | 0.502235 | 0.588909 | 00:48 | . 32 | 0.250810 | 0.393963 | 0.521790 | 0.486784 | 00:48 | . 33 | 0.246234 | 0.375275 | 0.519142 | 0.489137 | 00:48 | . 34 | 0.245064 | 0.331335 | 0.528236 | 0.686900 | 00:48 | . 35 | 0.246199 | 0.377710 | 0.537571 | 0.545870 | 00:47 | . 36 | 0.244448 | 0.368422 | 0.542517 | 0.553300 | 00:49 | . 37 | 0.241390 | 0.349010 | 0.519374 | 0.491989 | 00:48 | . 38 | 0.243630 | 0.359600 | 0.509863 | 0.521250 | 00:48 | . 39 | 0.241627 | 0.336892 | 0.539313 | 0.583476 | 00:48 | . 40 | 0.238271 | 0.367133 | 0.536665 | 0.515662 | 00:47 | . 41 | 0.232399 | 0.347052 | 0.545673 | 0.578082 | 00:48 | . 42 | 0.232242 | 0.319940 | 0.536428 | 0.673036 | 00:48 | . 43 | 0.235520 | 0.320792 | 0.553474 | 0.609017 | 00:47 | . 44 | 0.226998 | 0.326368 | 0.549632 | 0.638580 | 00:48 | . 45 | 0.229752 | 0.361821 | 0.542705 | 0.521525 | 00:48 | . 46 | 0.231764 | 0.356062 | 0.544539 | 0.571413 | 00:48 | . 47 | 0.226325 | 0.372531 | 0.541578 | 0.529744 | 00:48 | . 48 | 0.226063 | 0.374176 | 0.542866 | 0.539490 | 00:48 | . 49 | 0.223156 | 0.416989 | 0.514208 | 0.441924 | 00:48 | . 50 | 0.225011 | 0.389048 | 0.541989 | 0.558846 | 00:48 | . 51 | 0.221490 | 0.363521 | 0.539199 | 0.517616 | 00:48 | . 52 | 0.221223 | 0.344703 | 0.554917 | 0.570877 | 00:48 | . 53 | 0.219953 | 0.349854 | 0.550852 | 0.559267 | 00:47 | . 54 | 0.220182 | 0.371416 | 0.547601 | 0.532858 | 00:48 | . 55 | 0.218004 | 0.387494 | 0.507081 | 0.445336 | 00:47 | . 56 | 0.216417 | 0.355569 | 0.554721 | 0.555101 | 00:48 | . 57 | 0.214520 | 0.388316 | 0.538123 | 0.508856 | 00:48 | . 58 | 0.219770 | 0.350133 | 0.553124 | 0.555288 | 00:48 | . 59 | 0.213634 | 0.380796 | 0.545702 | 0.530716 | 00:48 | . 60 | 0.214770 | 0.403674 | 0.544516 | 0.526405 | 00:48 | . 61 | 0.212837 | 0.382615 | 0.547741 | 0.530949 | 00:49 | . 62 | 0.210907 | 0.385081 | 0.551419 | 0.559432 | 00:50 | . 63 | 0.211821 | 0.383210 | 0.538854 | 0.495931 | 00:49 | . 64 | 0.211744 | 0.366420 | 0.551287 | 0.553639 | 00:49 | . 65 | 0.208235 | 0.393033 | 0.544909 | 0.509530 | 00:49 | . 66 | 0.209825 | 0.404355 | 0.536005 | 0.474010 | 00:49 | . 67 | 0.210346 | 0.362994 | 0.552082 | 0.538402 | 00:49 | . 68 | 0.208779 | 0.372804 | 0.547688 | 0.514751 | 00:48 | . 69 | 0.209830 | 0.351810 | 0.555377 | 0.548285 | 00:48 | . 70 | 0.207887 | 0.386565 | 0.541370 | 0.496355 | 00:49 | . 71 | 0.206442 | 0.377267 | 0.548791 | 0.524507 | 00:48 | . 72 | 0.207366 | 0.374546 | 0.548827 | 0.523453 | 00:48 | . 73 | 0.205153 | 0.365717 | 0.549390 | 0.525388 | 00:49 | . 74 | 0.207726 | 0.367150 | 0.552252 | 0.542283 | 00:48 | . 75 | 0.208129 | 0.372454 | 0.549932 | 0.532464 | 00:48 | . 76 | 0.208461 | 0.360281 | 0.553864 | 0.548102 | 00:48 | . 77 | 0.207639 | 0.372237 | 0.550863 | 0.531641 | 00:48 | . 78 | 0.208036 | 0.364154 | 0.553543 | 0.545880 | 00:48 | . 79 | 0.206991 | 0.361695 | 0.552300 | 0.540092 | 00:47 | . Better model found at epoch 0 with dice value: 0.1824341691331724. Better model found at epoch 1 with dice value: 0.333523651466895. Better model found at epoch 2 with dice value: 0.4098708280811938. Better model found at epoch 3 with dice value: 0.4199211105853764. Better model found at epoch 4 with dice value: 0.4349567064541659. Better model found at epoch 5 with dice value: 0.4414173943941379. Better model found at epoch 9 with dice value: 0.4533229756356818. Better model found at epoch 13 with dice value: 0.4715493178986515. Better model found at epoch 20 with dice value: 0.48502134642674577. Better model found at epoch 22 with dice value: 0.5208334194588561. Better model found at epoch 24 with dice value: 0.5227439605738604. Better model found at epoch 26 with dice value: 0.5373924110580985. Better model found at epoch 35 with dice value: 0.5375711386569465. Better model found at epoch 36 with dice value: 0.5425170370282605. Better model found at epoch 41 with dice value: 0.5456728971466592. Better model found at epoch 43 with dice value: 0.5534739872514315. Better model found at epoch 52 with dice value: 0.5549168145010914. Better model found at epoch 69 with dice value: 0.5553773335762703. . . learn.recorder.plot_loss() . learn.save(&#39;xres34-best-long&#39;) . run.finish() . --Experiment Run: 3 . learn.save(&#39;xres34-best-long&#39;) . del learn . BATCH_SIZE = 3 # 3 for xresnet50, 12 for xresnet34 with Tesla P100 (16GB) TILES_PER_SCENE = 16 ARCHITECTURE = xresnet50 EPOCHS = 40 CLASS_WEIGHTS = [0.25,0.75] LR_MAX = 3e-4 ENCODER_FACTOR = 10 CODES = [&#39;Land&#39;,&#39;Building&#39;] . # Weights and Biases config config_dictionary = dict( bs=BATCH_SIZE, tiles_per_scene=TILES_PER_SCENE, architecture = str(ARCHITECTURE), epochs = EPOCHS, class_weights = CLASS_WEIGHTS, lr_max = LR_MAX, encoder_factor = ENCODER_FACTOR ) . learn = unet_learner(dls, # DataLoaders ARCHITECTURE, # xResNet50 loss_func = loss_func, # Weighted cross entropy loss opt_func = Adam, # Adam optimizer metrics = [Dice(), foreground_acc], # Custom metrics self_attention = False, cbs = [SaveModelCallback( monitor=&#39;dice&#39;, comp=np.greater, fname=&#39;best-model-0&#39; )] ) . learn.model . DynamicUnet( (layers): ModuleList( (0): Sequential( (0): ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (5): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (6): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (4): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (5): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (7): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) ) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Sequential( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(4096, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) ) (4): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (5): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (6): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(1536, 3072, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (7): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(576, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (8): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (9): ResizeToOrig() (10): MergeLayer() (11): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(291, 291, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(291, 291, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (12): ConvLayer( (0): Conv2d(291, 2, kernel_size=(1, 1), stride=(1, 1)) ) (13): ToTensorBase(tensor_cls=&lt;class &#39;fastai.torch_core.TensorBase&#39;&gt;) ) ) . . learn.unfreeze() learn.fit_one_cycle( EPOCHS, lr_max=slice(lr_max/ENCODER_FACTOR, lr_max), cbs=[WandbCallback()] ) . learn = unet_learner(dls, # DataLoaders ARCHITECTURE, # xResNet50 loss_func = loss_func, # Weighted cross entropy loss opt_func = Adam, # Adam optimizer metrics = [Dice(), foreground_acc], # Custom metrics self_attention = False, cbs = [SaveModelCallback( monitor=&#39;dice&#39;, comp=np.greater, fname=&#39;best-model&#39; )] ) . Downloading: &#34;https://s3.amazonaws.com/fast-ai-modelzoo/xrn50_940.pth&#34; to /root/.cache/torch/hub/checkpoints/xrn50_940.pth . . learn.model . DynamicUnet( (layers): ModuleList( (0): Sequential( (0): ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (5): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (6): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (3): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (4): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (5): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) (7): Sequential( (0): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential( (0): AvgPool2d(kernel_size=2, stride=2, padding=0) (1): ConvLayer( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (act): ReLU(inplace=True) ) (1): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (2): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ConvLayer( (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) ) ) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Sequential( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(4096, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) ) (4): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (5): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (6): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(1536, 3072, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (7): UnetBlock( (shuf): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv1): ConvLayer( (0): Conv2d(576, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (conv2): ConvLayer( (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (relu): ReLU() ) (8): PixelShuffle_ICNR( (0): ConvLayer( (0): Conv2d(288, 1152, kernel_size=(1, 1), stride=(1, 1)) (1): ReLU() ) (1): PixelShuffle(upscale_factor=2) ) (9): ResizeToOrig() (10): MergeLayer() (11): ResBlock( (convpath): Sequential( (0): ConvLayer( (0): Conv2d(291, 291, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() ) (1): ConvLayer( (0): Conv2d(291, 291, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ) ) (idpath): Sequential() (act): ReLU(inplace=True) ) (12): ConvLayer( (0): Conv2d(291, 2, kernel_size=(1, 1), stride=(1, 1)) ) ) ) . . learn.summary() . DynamicUnet (Input shape: 12) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 12 x 32 x 128 x 128 Conv2d 864 False BatchNorm2d 64 True ReLU Conv2d 9216 False BatchNorm2d 64 True ReLU ____________________________________________________________________________ 12 x 64 x 128 x 128 Conv2d 18432 False BatchNorm2d 128 True ReLU MaxPool2d Conv2d 4096 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 12 x 256 x 64 x 64 Conv2d 16384 False BatchNorm2d 512 True Conv2d 16384 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 64 x 64 x 64 Conv2d 16384 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 12 x 256 x 64 x 64 Conv2d 16384 False BatchNorm2d 512 True Sequential ReLU ____________________________________________________________________________ 12 x 64 x 64 x 64 Conv2d 16384 False BatchNorm2d 128 True ReLU Conv2d 36864 False BatchNorm2d 128 True ReLU ____________________________________________________________________________ 12 x 256 x 64 x 64 Conv2d 16384 False BatchNorm2d 512 True Sequential ReLU ____________________________________________________________________________ 12 x 128 x 64 x 64 Conv2d 32768 False BatchNorm2d 256 True ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 147456 False BatchNorm2d 256 True ReLU ____________________________________________________________________________ 12 x 512 x 32 x 32 Conv2d 65536 False BatchNorm2d 1024 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 512 x 32 x 32 Conv2d 131072 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 65536 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU ____________________________________________________________________________ 12 x 512 x 32 x 32 Conv2d 65536 False BatchNorm2d 1024 True Sequential ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 65536 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU ____________________________________________________________________________ 12 x 512 x 32 x 32 Conv2d 65536 False BatchNorm2d 1024 True Sequential ReLU ____________________________________________________________________________ 12 x 128 x 32 x 32 Conv2d 65536 False BatchNorm2d 256 True ReLU Conv2d 147456 False BatchNorm2d 256 True ReLU ____________________________________________________________________________ 12 x 512 x 32 x 32 Conv2d 65536 False BatchNorm2d 1024 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 32 x 32 Conv2d 131072 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 524288 False BatchNorm2d 2048 True ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 262144 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 262144 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 262144 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 262144 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True Sequential ReLU ____________________________________________________________________________ 12 x 256 x 16 x 16 Conv2d 262144 False BatchNorm2d 512 True ReLU Conv2d 589824 False BatchNorm2d 512 True ReLU ____________________________________________________________________________ 12 x 1024 x 16 x 16 Conv2d 262144 False BatchNorm2d 2048 True Sequential ReLU ____________________________________________________________________________ 12 x 512 x 16 x 16 Conv2d 524288 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 2359296 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 2048 x 8 x 8 Conv2d 1048576 False BatchNorm2d 4096 True ____________________________________________________________________________ [] AvgPool2d ____________________________________________________________________________ 12 x 2048 x 8 x 8 Conv2d 2097152 False BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 1048576 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 2048 x 8 x 8 Conv2d 1048576 False BatchNorm2d 4096 True Sequential ReLU ____________________________________________________________________________ 12 x 512 x 8 x 8 Conv2d 1048576 False BatchNorm2d 1024 True ReLU Conv2d 2359296 False BatchNorm2d 1024 True ReLU ____________________________________________________________________________ 12 x 2048 x 8 x 8 Conv2d 1048576 False BatchNorm2d 4096 True Sequential ReLU BatchNorm2d 4096 True ReLU ____________________________________________________________________________ 12 x 4096 x 8 x 8 Conv2d 75501568 True ReLU ____________________________________________________________________________ 12 x 2048 x 8 x 8 Conv2d 75499520 True ReLU ____________________________________________________________________________ 12 x 4096 x 8 x 8 Conv2d 8392704 True ReLU PixelShuffle BatchNorm2d 2048 True Conv2d 37750784 True ReLU Conv2d 37750784 True ReLU ReLU ____________________________________________________________________________ 12 x 4096 x 16 x 16 Conv2d 8392704 True ReLU PixelShuffle BatchNorm2d 1024 True Conv2d 21235200 True ReLU Conv2d 21235200 True ReLU ReLU ____________________________________________________________________________ 12 x 3072 x 32 x 32 Conv2d 4721664 True ReLU PixelShuffle BatchNorm2d 512 True Conv2d 9438208 True ReLU Conv2d 9438208 True ReLU ReLU ____________________________________________________________________________ 12 x 2048 x 64 x 64 Conv2d 2099200 True ReLU PixelShuffle BatchNorm2d 128 True ____________________________________________________________________________ 12 x 288 x 128 x 12 Conv2d 1493280 True ReLU Conv2d 746784 True ReLU ReLU ____________________________________________________________________________ 12 x 1152 x 128 x 1 Conv2d 332928 True ReLU PixelShuffle ResizeToOrig MergeLayer Conv2d 762420 True ReLU Conv2d 762420 True Sequential ReLU ____________________________________________________________________________ 12 x 2 x 255 x 255 Conv2d 584 True ____________________________________________________________________________ Total params: 339,089,232 Total trainable params: 315,615,216 Total non-trainable params: 23,474,016 Optimizer used: &lt;function Adam at 0x7ff189be5680&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback - SaveModelCallback . . learn.lr_find(suggestions=False) . !nvidia-smi . Tue Jun 8 16:12:57 2021 +--+ | NVIDIA-SMI 465.27 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 | | N/A 72C P0 32W / 70W | 13552MiB / 15109MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . import torch, gc gc.collect() torch.cuda.empty_cache() . learn.unfreeze() learn.fit_one_cycle( EPOCHS, lr_max=slice(lr_max/ENCODER_FACTOR, lr_max), cbs=[WandbCallback()] ) . learn.save(&#39;xres43-best-55&#39;) . Path(&#39;models/xres43-best-55.pth&#39;) . run.finish() . Waiting for W&amp;B process to finish, PID 15662Program ended successfully. Find user logs for this run at: /content/gdrive/Shareddrives/dataset/s7/SN7_buildings_train/train/L15-1716E-1211N_6864_3345_13/wandb/run-20210607_160231-1bj2r9kt/logs/debug.log Find internal logs for this run at: /content/gdrive/Shareddrives/dataset/s7/SN7_buildings_train/train/L15-1716E-1211N_6864_3345_13/wandb/run-20210607_160231-1bj2r9kt/logs/debug-internal.log Synced 4 W&amp;B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s) Synced clean-plasma-3: https://wandb.ai/priyank7n/s7/runs/1bj2r9kt Discussion . I&#39;m not sure how to rate the results because I don&#39;t have any comparison. A Dice score of 0.57 doesn&#39;t sound great. But considering how difficult the dataset is and that I didn&#39;t customize the architecture at all, I&#39;m quite pleased with the result. There is a lot to improve however! The original SpaceNet7 challenge required recognizing individual buildings and tracking them trough time, that&#39;s something I&#39;d like to work on in the future. . What worked? . Using a pretrained encoder. | Ignoring most images of the dataset. I tried using 5 instead of 1 images per scene, which increased training time by 5 but did not improve the results significantly. | Standard data augmentations. Without them, the model started to overfit sooner. | Undersampling. While it did not have a large effect, it sped up training a little bit and it helped the accuracy. | Weighted cross-entropy loss. Without the weights, the model had a strong bias towards the dominating background class and failed to recognize many buildings. | . What didn&#39;t? . I hoped to get faster training with the Mish activation function, but training was unstable. | Dice loss instead of cross-entropy loss was unstable as well. | I tried adding self-attention to the U-Net, I hoped it would help classifying larger structures. I did not notice a significant difference. | A deeper xResNet50 encoder that I tried increased training time 6-fold, but did not improve results. | . Other ideas to improve the results . Better data processing: using overlapping tiles, scaling up the image tiles | Dynamic thresholding for turning the predicted probabilities into a binary mask. | Implement recent advancements in segmentation models, ie UNet with ASPP or Eff-UNet | More compute: Deeper models. Use cross-validation with several folds to utilize all 60 scenes. Ensemble different models. | . Thank you for reading this far! The challenge was fun and I learned a lot. There is also a lot of room for improvement and work to do :) .",
            "url": "https://priyank7n.me/2021/05/06/SatFoot.html",
            "relUrl": "/2021/05/06/SatFoot.html",
            "date": "  May 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Reproducing Reformer: Our Amazing Submission & Team Experience",
            "content": "Where we all met? here . The Challenge . Way back in October 2020 the Papers With Code ML Reproducibility Challenge 2020 was launched and shared in the fast.ai forums. A few of us jumped at the chance to test our ML knowledge and push our skills. Fast forward 110 days since that initial post and we delivered our Reformer Reproducibility submission via OpenReview!! . Our Whole Project is Documented here : Project . The Wandb reports we made : reports . Here are a few reflections on our experience: what we enjoyed, tools we used and what we would have done differently: . TLDR; . Working as a team pushes your motivation, your skills and your throughput | nbdev for development, Weights &amp; Biases for tracking and Discord for communication | We could have better used task/project management tools more, maybe we needed a different tool | Next time well start experiments sooner and maybe pick a more practical paper | It was a massive learning experience and a lot of fun | . . Why participate . Implementing code from scratch is much more enjoyable and meaningful when there is a direct application, e.g. working towards this reproducibility challenge. Spending weeks and months focussed on a single paper forces you to understand the paper down to the last full stop. It also gives you a great appreciation of how difficult writing a good paper is, you see almost every word and sentence is chosen carefully to communicate a particular concept, problem or model setting. . N heads are better than one a.k.a. Multihead Attention . Our team was distributed across 6 countries and everyone had a somewhat different background, set of skills and personality. This mix was definitely beneficial for getting things done much more smoothly. Having 2 x N eyes researching implementation information or reviewing code really improved coverage and sped up the entire process. It also makes debugging much faster! . . Writing code that the entire team will use also meant writing cleaner code with more tests so that it was as clear as possible for your teammates. And finally, during a long project like this its easy to get distracted or lazy, however seeing everyone else delivering great work quickly pulls you back into line! . . Good tools Are key for us : A good tool improves the way you work. A great tool improves the way you think. . Read more: https://www.wisesayings.com/tool-quotes/#ixzz6mZj38LCP . nbdev . The nbdev literate programming environment from fast.ai was super convenient to minimise the projects development friction. Writing tests as we developed meant that we caught multiple bugs early and auto-generation of docs lends itself immensely to the reproducibility of your code. Most of us will be using this again for our next projects. . Weights &amp; Biases . Weights &amp; Biases generously gave us a team account which enabled us all to log our experiments to a single project. Being directly able to link your runs and results to the final report was really nice. Also it&#39;s pretty exciting monitoring 10+ experiments live! . Discord . A Discord server worked really well for all our chat and voice communication. Frequent calls to catchup and agree on next steps were super useful. Todo lists and core pieces of code often ended up as pinned messages for quick reference and linking Github activity to a channel was useful for keeping an eye on new commits to the repo. . Overleaf . When it came to writing the final report in latex, Overleaf was a wonderful tool for collaborative editing. . ReviewNB . The ReviewNB app on GitHub was very useful for visualizing diffs in notebooks. . . Learn from the best . The Reformer architecture had several complex parts, and having Phil Wang&#39;s and HuggingFace&#39;s Github code was very helpful to understand design decisions and fix issues. . Things we can improve for the next time . Start experiments early . We started our experiments quite late in the project; as we aimed to reimplement Reformer in Pytorch (with reference to existing implementations) about ~90% of our time was spent on ensuring our implementation was faithful to the paper and that it was working correctly. In retrospect starting experiments earlier would have allowed more in depth exploration of what we observed while testing. Full scale experiments have a way of inducing problems you didnt foresee during the implementation phase... . Task distribution and coordination . When working in a distributed and decentralized team, efficient task allocation and tracking is important. Early in the project todo lists lived in peoples heads, or were quickly buried under 50 chat messages. This was suboptimal for a number of reasons, including that it made involving new people in the project more challenging as they could not easily identify where they could best contribute. . We made a switch to Trello to better track open tasks. It worked reasonably well however its effectiveness was probably proportional to how much time a couple of team members had to review the kanban board, advocate for its use and focus the teams attention there. The extra friction associated with needing to use another tool unconnected to Github or Discord was probably the reason for why we didnt use it as much as we could have. Integrating Trello into our workflow or giving Github Projects a trial could have been useful. . More feedback . We had originally intended to get feedback from the fastai community during the project. In the end we were too late in sharing our material, so there wasnt time for much feedback. Early feedback would have been very useful and the project might have benefited from some periodic summary of accomplishments and current problems. We could have solicited additional feedback from the authors too. . Distributed training . This was our first exposure to distributed training and unfortunately we had a lot of issues with it. We were also unable to log the results from distributed runs properly to Weights &amp; Biases. This slowed down our experiment iteration speed and is why we could not train our models for as long as we would have preferred. . . Choice of paper to reproduce . It would have been useful to calculate a rough estimate of the compute budget the papers experiments required before jumping into it. In the latter stages of the project we realised that we would be unable to fully replicate some of the papers experiments, but instead had to run scaled down versions. In addition, where your interest sits between theoretical and practical papers should be considered when selecting a paper for the challenge. . More tools . We could have tried even more handy tools such as knockknock to alert us when models are finished training and Github Projects for task management. . Some final thoughts . We came out of this project even more motivated compared to how we entered; a great indication that it was both enjoyable and useful for us! Our advice would be to not hesitate to join events like this one and challenge yourself, and try and find one or more other folks in the forums or Discord to work with. After successfully delivering our submission to the challenge we are all eager to work together again on our next project, stay tuned for more! . . Thanks for Reading This Far &#128591; . As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter &amp; Linkedin: twitter Linkedin .",
            "url": "https://priyank7n.me/nlp/reformer/transformers/language-modelling/2021/02/19/reformer-reproducibility-challenge.html",
            "relUrl": "/nlp/reformer/transformers/language-modelling/2021/02/19/reformer-reproducibility-challenge.html",
            "date": "  Feb 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "COVID-19 Infection Detection Using Deep Learning",
            "content": "Setup . Dataset is acquired from kaggle.link SARS-CoV-2 CT scan dataset is a public dataset, containing 1252 CT scans (computed tomography scan) from SARS-CoV-2 infected patients (COVID-19) and 1230 CT scans for SARS-CoV-2 non-infected patients. The dataset has been collected from real patients in Sao Paulo, Brazil. The dataset is available in kaggle. . import opendatasets as od dataset_url = &#39;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&#39; od.download(dataset_url) . . 2%| | 5.00M/230M [00:00&lt;00:06, 34.4MB/s] . Downloading sarscov2-ctscan-dataset.zip to ./sarscov2-ctscan-dataset . 100%|| 230M/230M [00:04&lt;00:00, 52.3MB/s] . . path=Path(&#39;sarscov2-ctscan-dataset&#39;) . The Dataset contains two folders namely COVID &amp; non-COVID having CT Scan Images of patients: . path.ls() . . (#2) [Path(&#39;sarscov2-ctscan-dataset/COVID&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID&#39;)] . Preprocessing . Exploring Dataset Structure and displaying sample CT Scan directories: . path.ls() . . (#2) [Path(&#39;sarscov2-ctscan-dataset/COVID&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID&#39;)] . There are 1252 CT scan images from SARS-CoV-2 infected patients. . (path/&#39;COVID&#39;).ls() . . (#1252) [Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (10).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (100).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1000).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1001).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1002).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1003).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1004).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1005).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1006).png&#39;)...] . There are 1230 CT scan images from SARS-CoV-2 non-infected patients. . (path/&#39;non-COVID&#39;).ls() . . (#1229) [Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (10).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (100).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1000).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1001).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1002).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1003).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1004).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1005).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1006).png&#39;)...] . Visualizing the Images: Lets look at some raw images in the dataset: . import PIL #looking into the images downloaded img1 = PIL.Image.open((path/&#39;COVID&#39;).ls()[0]) img1 . . Creating a Datablock . DataBlock API :We divide the dataset as train and valid set and use the random_state argument in order to replicate the result.The valid_pct argument represents the proportion of the dataset to include in the valid (in our case 20%). Presizing is done and Transformations are applied to images keeping 75% of the images and then normalized according to the imagenet stats for applying Transfer Learning later. . Creating Dataloaders . def get_dls(bs,size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(valid_pct=0.2, seed=42), item_tfms=Resize(460), #presizing is done batch_tfms=[*aug_transforms(size=size,min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path,bs=bs) . To Create dataloaders we use DataBlock API of fast ai: We use images of size 224*224 and a single batch containing 224 images. . dls=get_dls(224,224) . Visualizing the Dataloaders . The images in the dataloaders look like: . dls.show_batch(nrows=3, figsize=(7,6)) . . A batch of images in a grid look like: . @patch @delegates(to=draw_label, but=[&quot;font_color&quot;, &quot;location&quot;, &quot;draw_rect&quot;, &quot;fsize_div_factor&quot;, &quot;font_path&quot;, &quot;font_size&quot;]) def show_batch_grid(self:TfmdDL, b=None, n=20, ncol=4, show=True, unique=False, unique_each=True, font_path=None, font_size=20, **kwargs): &quot;&quot;&quot;Show a batch of images Key Params: * n: No. of images to display * n_col: No. of columns in the grid * unique: Display the same image with different augmentations * unique_each: If True, displays a different img on each call * font_path: Path to the `.ttf` font file. Required to display labels * font_size: Size of the font &quot;&quot;&quot; if font_path is not None: self.set_font_path(font_path) if not hasattr(self, &#39;font_path&#39;): self.font_path = font_path if unique: old_get_idxs = self.get_idxs if unique_each: i = np.random.choice(self.n) self.get_idxs = partial(itertools.repeat, i) else: self.get_idxs = lambda: Inf.zeros if b is None: b = self.one_batch() if not show: return self._pre_show_batch(b, max_n=n) _,__, b = self._pre_show_batch(b, max_n=n) if unique: self.get_idxs = old_get_idxs return make_img_grid([draw_label(i, font_path=self.font_path, font_size=font_size) for i in b], ncol=ncol, img_size=None) . . Transfer Learning . The Resnet50 model . What and why did we used Transfer Learning . Transfer learning is meaning use a pre-trained model to build our classifier. A pre-trained model is a model that has been previously trained on a dataset. The model comprehends the updated weights and bias. Using a pre-trained model you are saving time and computational resources. Another avantage is that pre-trained models often perform better that architecture designed from scratch. | To better understand this point, suppose we want to build a classifier able to sort different sailboat types. A model pre-trained on ships would have already capture in its first layers some boat features, learning faster and with better accuracy among the different sailboat types. | . | The Resnet50 architecture: . Resnet50 generally is considered a good choice as first architecture to test, it shows good performance without an excessive size allowing to use a higher batch size and thus less computation time. For this reason, before to test more complex architectures Resnet50 is a good compromise. | Residual net have been ideated to solve the problem of the vanishing gradient. Highly intricate networks with a large number of hidden layer are working effectively in solving complicated tasks. Their structures allow them to catch pattern in complicated data. When we train the network the early layer tend to be trained slower (the gradient are smaller during backpropagation). The initial layers are important because they learn the basic feature of an object (edge, corner and so on). Failing to proper train these layers lead to a decrease in the overall accuracy of the model. | Residual neural network have been ideated to solve this issue. The Resnet model presents the possibility to skip the training of some layer during the initial training. The skipped layer is reusing the learned weights from the previous layer. Original research article | . | Test the Resnet34 architecture with our dataset: . Now we are going to test how the FastaAI implementation of this architechture works with the COVID dataset. | Create the convolutional neural network First we will create the convolutional neural network based on this architechture, to do this we can use the following code block which uses FastAI ( cnn_learner previously create_cnn) function. We pass the loaded data, specify the model, pass error_rate &amp; accuracy as a list for the metrics parameter specifying we want to see both error_rate and accuracy, and finally specify a weight decay of 1e-1 (1.0). | learn.lr_find() &amp; learn.recorder.plot() function to run LR Finder. LR Finder help to find the best learning rate to use with our network. For more information the original paper. As shown from the output of above. . | learn.recorder.plot() function plot the loss over learning rate. Run the following code block to view the graph. The best learning rate should be chosen as the learning rate value where the curve is the steepest. You may try different learning rate values in order to pick up the best. . | [learn.fit_one_cycle() &amp; learn.recorder.plot_losses()] The learn.fit_one_cycle() function can be used to fit the model. Fit one cycle reach a comparable accuracy faster than th fit function in training of complex models. Fit one cycle instead of maintain fix the learning rate during all the iterations is linearly increasing the learning rate and then it is decreasing again (this process is what is called one cycle). Moreover, this learning rate variation is helping in preventing overfitting. We use 5 for the parameter cyc_len to specify the number of cycles to run (on cycle can be considered equivalent to an epoch), and max_lr to specify the maximum learning rate to use which we set as 0.001. Fit one cycle varies the learning rate from 10 fold less the maximum learning rate selected. For more information about fit one cycle: article. . | . | Testing with Deeper Architectures . learn = cnn_learner(dls, resnet101, loss_func=CrossEntropyLossFlat(), metrics=[error_rate,accuracy], wd=1e-1).to_fp16() . learn.cbs . (#4) [TrainEvalCallback,Recorder,ProgressCallback,MixedPrecision] . We apply a very powerful Data Augmentation technique that is Mixup and train the model. . learn.fit_one_cycle(80, 3e-3, cbs=MixUp(0.5)) . . epoch train_loss valid_loss error_rate accuracy time . 0 | 1.098607 | 0.923316 | 0.405242 | 0.594758 | 00:23 | . 1 | 0.959609 | 0.496383 | 0.227823 | 0.772177 | 00:23 | . 2 | 0.907947 | 0.482087 | 0.223790 | 0.776210 | 00:23 | . 3 | 0.874337 | 0.519458 | 0.252016 | 0.747984 | 00:22 | . 4 | 0.838613 | 0.474054 | 0.221774 | 0.778226 | 00:23 | . 5 | 0.800547 | 0.362412 | 0.159274 | 0.840726 | 00:23 | . 6 | 0.766916 | 0.340699 | 0.137097 | 0.862903 | 00:23 | . 7 | 0.732544 | 0.190343 | 0.060484 | 0.939516 | 00:23 | . 8 | 0.705716 | 0.234552 | 0.092742 | 0.907258 | 00:22 | . 9 | 0.672490 | 0.225245 | 0.092742 | 0.907258 | 00:23 | . 10 | 0.646345 | 0.196367 | 0.082661 | 0.917339 | 00:23 | . 11 | 0.614197 | 0.207071 | 0.082661 | 0.917339 | 00:22 | . 12 | 0.580931 | 0.145530 | 0.058468 | 0.941532 | 00:22 | . 13 | 0.551455 | 0.160765 | 0.062500 | 0.937500 | 00:22 | . 14 | 0.530735 | 0.156187 | 0.058468 | 0.941532 | 00:22 | . 15 | 0.508143 | 0.133556 | 0.040323 | 0.959677 | 00:23 | . 16 | 0.486114 | 0.130424 | 0.048387 | 0.951613 | 00:23 | . 17 | 0.468766 | 0.112146 | 0.036290 | 0.963710 | 00:23 | . 18 | 0.451705 | 0.106726 | 0.038306 | 0.961694 | 00:23 | . 19 | 0.435460 | 0.140806 | 0.046371 | 0.953629 | 00:23 | . 20 | 0.421944 | 0.129412 | 0.042339 | 0.957661 | 00:23 | . 21 | 0.409734 | 0.107145 | 0.034274 | 0.965726 | 00:23 | . 22 | 0.399341 | 0.125089 | 0.042339 | 0.957661 | 00:23 | . 23 | 0.389513 | 0.099228 | 0.036290 | 0.963710 | 00:23 | . 24 | 0.380188 | 0.082548 | 0.018145 | 0.981855 | 00:23 | . 25 | 0.370885 | 0.071890 | 0.016129 | 0.983871 | 00:23 | . 26 | 0.363348 | 0.126151 | 0.044355 | 0.955645 | 00:23 | . 27 | 0.356708 | 0.085095 | 0.022177 | 0.977823 | 00:23 | . 28 | 0.351457 | 0.082022 | 0.030242 | 0.969758 | 00:23 | . 29 | 0.346920 | 0.082360 | 0.022177 | 0.977823 | 00:23 | . 30 | 0.343117 | 0.086793 | 0.026210 | 0.973790 | 00:23 | . 31 | 0.338743 | 0.084433 | 0.028226 | 0.971774 | 00:23 | . 32 | 0.332180 | 0.050694 | 0.012097 | 0.987903 | 00:23 | . 33 | 0.329243 | 0.075656 | 0.022177 | 0.977823 | 00:23 | . 34 | 0.325888 | 0.074826 | 0.018145 | 0.981855 | 00:22 | . 35 | 0.321171 | 0.051103 | 0.016129 | 0.983871 | 00:22 | . 36 | 0.317992 | 0.068456 | 0.014113 | 0.985887 | 00:23 | . 37 | 0.317117 | 0.095658 | 0.038306 | 0.961694 | 00:23 | . 38 | 0.314691 | 0.075247 | 0.026210 | 0.973790 | 00:23 | . 39 | 0.312669 | 0.059977 | 0.014113 | 0.985887 | 00:23 | . 40 | 0.311207 | 0.062207 | 0.016129 | 0.983871 | 00:23 | . 41 | 0.307136 | 0.079891 | 0.032258 | 0.967742 | 00:23 | . 42 | 0.303215 | 0.060350 | 0.014113 | 0.985887 | 00:23 | . 43 | 0.301979 | 0.061862 | 0.014113 | 0.985887 | 00:23 | . 44 | 0.302073 | 0.056083 | 0.012097 | 0.987903 | 00:23 | . 45 | 0.298332 | 0.054264 | 0.014113 | 0.985887 | 00:23 | . 46 | 0.297571 | 0.050670 | 0.006048 | 0.993952 | 00:23 | . 47 | 0.295835 | 0.053044 | 0.014113 | 0.985887 | 00:23 | . 48 | 0.295003 | 0.053177 | 0.006048 | 0.993952 | 00:23 | . 49 | 0.295658 | 0.070317 | 0.012097 | 0.987903 | 00:23 | . 50 | 0.293548 | 0.051080 | 0.008064 | 0.991935 | 00:23 | . 51 | 0.293651 | 0.061804 | 0.016129 | 0.983871 | 00:23 | . 52 | 0.291592 | 0.044012 | 0.010081 | 0.989919 | 00:23 | . 53 | 0.289374 | 0.047382 | 0.006048 | 0.993952 | 00:23 | . 54 | 0.288036 | 0.050668 | 0.006048 | 0.993952 | 00:23 | . 55 | 0.286396 | 0.057323 | 0.016129 | 0.983871 | 00:23 | . 56 | 0.285277 | 0.049304 | 0.012097 | 0.987903 | 00:23 | . 57 | 0.283157 | 0.047652 | 0.010081 | 0.989919 | 00:23 | . 58 | 0.282250 | 0.046741 | 0.008064 | 0.991935 | 00:23 | . 59 | 0.282024 | 0.043001 | 0.006048 | 0.993952 | 00:23 | . 60 | 0.281422 | 0.043425 | 0.004032 | 0.995968 | 00:22 | . 61 | 0.279792 | 0.048245 | 0.004032 | 0.995968 | 00:23 | . 62 | 0.278259 | 0.050301 | 0.008064 | 0.991935 | 00:23 | . 63 | 0.276450 | 0.042498 | 0.006048 | 0.993952 | 00:23 | . 64 | 0.275557 | 0.043382 | 0.008064 | 0.991935 | 00:23 | . 65 | 0.274992 | 0.046327 | 0.008064 | 0.991935 | 00:23 | . 66 | 0.274949 | 0.051264 | 0.012097 | 0.987903 | 00:23 | . 67 | 0.276006 | 0.050355 | 0.010081 | 0.989919 | 00:23 | . 68 | 0.277512 | 0.047513 | 0.008064 | 0.991935 | 00:22 | . 69 | 0.275122 | 0.044733 | 0.006048 | 0.993952 | 00:22 | . 70 | 0.275745 | 0.042205 | 0.006048 | 0.993952 | 00:23 | . 71 | 0.274163 | 0.041508 | 0.006048 | 0.993952 | 00:23 | . 72 | 0.273943 | 0.042359 | 0.006048 | 0.993952 | 00:23 | . 73 | 0.273899 | 0.042546 | 0.006048 | 0.993952 | 00:23 | . 74 | 0.271843 | 0.044013 | 0.006048 | 0.993952 | 00:23 | . 75 | 0.271735 | 0.043344 | 0.006048 | 0.993952 | 00:22 | . 76 | 0.271392 | 0.045417 | 0.008064 | 0.991935 | 00:23 | . 77 | 0.270836 | 0.044158 | 0.006048 | 0.993952 | 00:23 | . 78 | 0.272595 | 0.043604 | 0.008064 | 0.991935 | 00:23 | . 79 | 0.272234 | 0.044281 | 0.008064 | 0.991935 | 00:23 | . TTA(Test Time Augmentation) . preds,targs = learn.tta() # TTA applied for validation dataset accuracy(preds, targs).item() . . . 0.9959677457809448 . We get a TTA of 99.59% on the validation set. . ClassificationInterpretationEx . We examine the model predictions in more depth: . import fastai def _get_truths(vocab, label_idx, is_multilabel): if is_multilabel: return &#39;;&#39;.join([vocab[i] for i in torch.where(label_idx==1)][0]) else: return vocab[label_idx] class ClassificationInterpretationEx(ClassificationInterpretation): &quot;&quot;&quot; Extend fastai2&#39;s `ClassificationInterpretation` to analyse model predictions in more depth See: * self.preds_df * self.plot_label_confidence() * self.plot_confusion_matrix() * self.plot_accuracy() * self.get_fnames() * self.plot_top_losses_grid() * self.print_classification_report() &quot;&quot;&quot; def __init__(self, dl, inputs, preds, targs, decoded, losses): super().__init__(dl, inputs, preds, targs, decoded, losses) self.vocab = self.dl.vocab if is_listy(self.vocab): self.vocab = self.vocab[-1] if self.targs.__class__ == fastai.torch_core.TensorMultiCategory: self.is_multilabel = True else: self.is_multilabel = False self.compute_label_confidence() self.determine_classifier_type() def determine_classifier_type(self): if self.targs[0].__class__==fastai.torch_core.TensorCategory: self.is_multilabel = False if self.targs[0].__class__==fastai.torch_core.TensorMultiCategory: self.is_multilabel = True self.thresh = self.dl.loss_func.thresh def compute_label_confidence(self, df_colname:Optional[str]=&quot;fnames&quot;): &quot;&quot;&quot; Collate prediction confidence, filenames, and ground truth labels in DataFrames, and store them as class attributes `self.preds_df` and `self.preds_df_each` If the `DataLoaders` is constructed from a `pd.DataFrame`, use `df_colname` to specify the column name with the filepaths &quot;&quot;&quot; if not isinstance(self.dl.items, pd.DataFrame): self._preds_collated = [ #(item, self.dl.vocab[label_idx], *preds.numpy()*100) (item, _get_truths(self.dl.vocab, label_idx, self.is_multilabel), *preds.numpy()*100) for item,label_idx,preds in zip(self.dl.items, self.targs, self.preds) ] ## need to extract fname from DataFrame elif isinstance(self.dl.items, pd.DataFrame): self._preds_collated = [ #(item[df_colname], self.dl.vocab[label_idx], *preds.numpy()*100) (item[df_colname], _get_truths(self.dl.vocab, label_idx, self.is_multilabel), *preds.numpy()*100) for (_,item),label_idx,preds in zip(self.dl.items.iterrows(), self.targs, self.preds) ] self.preds_df = pd.DataFrame(self._preds_collated, columns = [&#39;fname&#39;,&#39;truth&#39;, *self.dl.vocab]) self.preds_df.insert(2, column=&#39;loss&#39;, value=self.losses.numpy()) if self.is_multilabel: return # preds_df_each doesnt make sense for multi-label self._preds_df_each = {l:self.preds_df.copy()[self.preds_df.truth == l].reset_index(drop=True) for l in self.dl.vocab} self.preds_df_each = defaultdict(dict) sort_desc = lambda x,col: x.sort_values(col, ascending=False).reset_index(drop=True) for label,df in self._preds_df_each.items(): filt = df[label] == df[self.dl.vocab].max(axis=1) self.preds_df_each[label][&#39;accurate&#39;] = df.copy()[filt] self.preds_df_each[label][&#39;inaccurate&#39;] = df.copy()[~filt] self.preds_df_each[label][&#39;accurate&#39;] = sort_desc(self.preds_df_each[label][&#39;accurate&#39;], label) self.preds_df_each[label][&#39;inaccurate&#39;] = sort_desc(self.preds_df_each[label][&#39;inaccurate&#39;], label) assert len(self.preds_df_each[label][&#39;accurate&#39;]) + len(self.preds_df_each[label][&#39;inaccurate&#39;]) == len(df) def get_fnames(self, label:str, mode:(&#39;accurate&#39;,&#39;inaccurate&#39;), conf_level:Union[int,float,tuple]) -&gt; np.ndarray: &quot;&quot;&quot; Utility function to grab filenames of a particular label `label` that were classified as per `mode` (accurate|inaccurate). These filenames are filtered by `conf_level` which can be above or below a certain threshold (above if `mode` == &#39;accurate&#39; else below), or in confidence ranges &quot;&quot;&quot; assert label in self.dl.vocab if not hasattr(self, &#39;preds_df_each&#39;): self.compute_label_confidence() df = self.preds_df_each[label][mode].copy() if mode == &#39;accurate&#39;: if isinstance(conf_level, tuple): filt = df[label].between(*conf_level) if isinstance(conf_level, (int,float)): filt = df[label] &gt; conf_level if mode == &#39;inaccurate&#39;: if isinstance(conf_level, tuple): filt = df[label].between(*conf_level) if isinstance(conf_level, (int,float)): filt = df[label] &lt; conf_level return df[filt].fname.values . . fname truth loss COVID non-COVID . 0 sarscov2-ctscan-dataset/non-COVID/Non-Covid (386).png | non-COVID | 0.047037 | 4.594821 | 95.405182 | . 1 sarscov2-ctscan-dataset/COVID/Covid (581).png | COVID | 0.018380 | 98.178818 | 1.821182 | . ClassificationInterpretationEx.get_fnames . Returns accuratly classified files with accuracy above 85%: . interp.get_fnames(&#39;accurate&#39;, 99.95) . . Returns inaccurately classified files with accuracy between 84.1-85.2%: . interp.get_fnames(&#39;img1&#39;, &#39;accurate&#39;, (84.1, 85.2)) . . Confusion Matrix . Checking the Confusion Matrix: . Plot Accuracy . plotting curves of training process: . functions to plot the accuracy of the labels: . @patch def plot_accuracy(self:ClassificationInterpretationEx, width=0.9, figsize=(6,6), return_fig=False, title=&#39;Accuracy Per Label&#39;, ylabel=&#39;Accuracy (%)&#39;, style=&#39;ggplot&#39;, color=&#39;#2a467e&#39;, vertical_labels=True): &#39;Plot a bar plot showing accuracy per label&#39; if not hasattr(self, &#39;preds_df_each&#39;): raise NotImplementedError plt.style.use(style) if not hasattr(self, &#39;preds_df_each&#39;): self.compute_label_confidence() self.accuracy_dict = defaultdict() for label,df in self.preds_df_each.items(): total = len(df[&#39;accurate&#39;]) + len(df[&#39;inaccurate&#39;]) self.accuracy_dict[label] = 100 * len(df[&#39;accurate&#39;]) / total fig,ax = plt.subplots(figsize=figsize) x = self.accuracy_dict.keys() y = [v for k,v in self.accuracy_dict.items()] rects = ax.bar(x,y,width,color=color) for rect in rects: ht = rect.get_height() ax.annotate(s = f&quot;{ht:.02f}&quot;, xy = (rect.get_x() + rect.get_width()/2, ht), xytext = (0,3), # offset vertically by 3 points textcoords = &#39;offset points&#39;, ha = &#39;center&#39;, va = &#39;bottom&#39; ) ax.set_ybound(lower=0, upper=100) ax.set_yticks(np.arange(0,110,10)) ax.set_ylabel(ylabel) ax.set_xticklabels(x, rotation=&#39;vertical&#39; if vertical_labels else &#39;horizontal&#39;) plt.suptitle(title) plt.tight_layout() if return_fig: return fig . . &lt;ipython-input-67-2c6afab5379f&gt;:25: MatplotlibDeprecationWarning: The &#39;s&#39; parameter of annotate() has been renamed &#39;text&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. ax.annotate(s = f&#34;{ht:.02f}&#34;, &lt;ipython-input-67-2c6afab5379f&gt;:34: UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_xticklabels(x, rotation=&#39;vertical&#39; if vertical_labels else &#39;horizontal&#39;) . Plot Label Confidence . Plotting label confidence as histograms for each label: . @patch def plot_label_confidence(self:ClassificationInterpretationEx, bins:int=5, fig_width:int=12, fig_height_base:int=4, title:str=&#39;Accurate vs. Inaccurate Predictions Confidence (%) Levels Per Label&#39;, return_fig:bool=False, label_bars:bool=True, style=&#39;ggplot&#39;, dpi=150, accurate_color=&#39;#2a467e&#39;, inaccurate_color=&#39;#dc4a46&#39;): &quot;&quot;&quot;Plot label confidence histograms for each label Key Args: * `bins`: No. of bins on each side of the plot * `return_fig`: If True, returns the figure that can be easily saved to disk * `label_bars`: If True, displays the % of samples that fall into each bar * `style`: A matplotlib style. See `plt.style.available` for more * `accurate_color`: Color of the accurate bars * `inaccurate_color`: Color of the inaccurate bars &quot;&quot;&quot; if not hasattr(self, &#39;preds_df_each&#39;): raise NotImplementedError plt.style.use(style) fig, axes = plt.subplots(nrows = len(self.preds_df_each.keys()), ncols=2, dpi=dpi, figsize = (fig_width, fig_height_base * len(self.dl.vocab))) for i, (label, df) in enumerate(self.preds_df_each.items()): height=0 # find max height for mode in [&#39;inaccurate&#39;, &#39;accurate&#39;]: len_bins,_ = np.histogram(df[mode][label], bins=bins) if len_bins.max() &gt; height: height=len_bins.max() for mode,ax in zip([&#39;inaccurate&#39;, &#39;accurate&#39;], axes[i]): range_ = (50,100) if mode == &#39;accurate&#39; else (0,50) color = accurate_color if mode == &#39;accurate&#39; else inaccurate_color num,_,patches = ax.hist(df[mode][label], bins=bins, range=range_, rwidth=.95, color=color) num_samples = len(df[&#39;inaccurate&#39;][label]) + len(df[&#39;accurate&#39;][label]) pct_share = len(df[mode][label]) / num_samples if label_bars: for rect in patches: ht = rect.get_height() ax.annotate(s = f&quot;{round((int(ht) / num_samples) * 100, 1) if ht &gt; 0 else 0}%&quot;, xy = (rect.get_x() + rect.get_width()/2, ht), xytext = (0,3), # offset vertically by 3 points textcoords = &#39;offset points&#39;, ha = &#39;center&#39;, va = &#39;bottom&#39; ) ax.set_ybound(upper=height + height*0.3) ax.set_xlabel(f&#39;{label}: {mode.capitalize()} ({round(pct_share * 100, 2)}%)&#39;) ax.set_ylabel(f&#39;Num. {mode.capitalize()} ({len(df[mode][label])} of {num_samples})&#39;) fig.suptitle(title, y=1.0) plt.subplots_adjust(top = 0.9, bottom=0.01, hspace=0.25, wspace=0.2) plt.tight_layout() if return_fig: return fig . . &lt;ipython-input-69-1241d4a5b1b6&gt;:37: MatplotlibDeprecationWarning: The &#39;s&#39; parameter of annotate() has been renamed &#39;text&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. ax.annotate(s = f&#34;{round((int(ht) / num_samples) * 100, 1) if ht &gt; 0 else 0}%&#34;, . Plot Top Losses grid . plotting the top losses in a grid: . from fastai_amalgam.utils import * @patch def plot_top_losses_grid(self:ClassificationInterpretationEx, k=16, ncol=4, __largest=True, font_path=None, font_size=12, use_dedicated_layout=True) -&gt; PIL.Image.Image: &quot;&quot;&quot;Plot top losses in a grid Uses fastai&#39;a `ClassificationInterpretation.plot_top_losses` to fetch predictions, and makes a grid with the ground truth labels, predictions, prediction confidence and loss ingrained into the image By default, `use_dedicated_layout` is used to plot the loss (bottom), truths (top-left), and predictions (top-right) in dedicated areas of the image. If this is set to `False`, everything is printed at the bottom of the image &quot;&quot;&quot; # all of the pred fetching code is copied over from # fastai&#39;s `ClassificationInterpretation.plot_top_losses` # and only plotting code is added here losses,idx = self.top_losses(k, largest=__largest) if not isinstance(self.inputs, tuple): self.inputs = (self.inputs,) if isinstance(self.inputs[0], Tensor): inps = tuple(o[idx] for o in self.inputs) else: inps = self.dl.create_batch(self.dl.before_batch([tuple(o[i] for o in self.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (self.targs if is_listy(self.targs) else (self.targs,))) x,y,its = self.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (self.decoded if is_listy(self.decoded) else (self.decoded,))) x1,y1,outs = self.dl._pre_show_batch(b_out, max_n=k) #if its is not None: # _plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), self.preds[idx], losses, **kwargs) plot_items = its.itemgot(0), its.itemgot(1), outs.itemgot(slice(len(inps), None)), self.preds[idx], losses def draw_label(x:TensorImage, labels): return PILImage.create(x).draw_labels(labels, font_path=font_path, font_size=font_size, location=&quot;bottom&quot;) # return plot_items results = [] for x, truth, preds, preds_raw, loss in zip(*plot_items): if self.is_multilabel: preds = preds[0] probs_i = np.array([self.dl.vocab.o2i[o] for o in preds]) pred2prob = [f&quot;{pred} ({round(prob.item()*100,2)}%)&quot; for pred,prob in zip(preds,preds_raw[probs_i])] if use_dedicated_layout: # draw loss at the bottom, preds on top-right # and truths on the top img = PILImage.create(x) if isinstance(truth, Category): truth = [truth] truth.insert(0, &quot;TRUTH: &quot;) pred2prob.insert(0, &#39;PREDS: &#39;) loss_text = f&quot;{&#39;LOSS: &#39;.rjust(8)} {round(loss.item(), 4)}&quot; img.draw_labels(truth, location=&quot;top-left&quot;, font_size=font_size, font_path=font_path) img.draw_labels(pred2prob, location=&quot;top-right&quot;, font_size=font_size, font_path=font_path) img.draw_labels(loss_text, location=&quot;bottom&quot;, font_size=font_size, font_path=font_path) results.append(img) else: # draw everything at the bottom out = [] out.append(f&quot;{&#39;TRUTH: &#39;.rjust(8)} {truth}&quot;) bsl = &#39; n&#39; # since f-strings can&#39;t have backslashes out.append(f&quot;{&#39;PRED: &#39;.rjust(8)} {bsl.join(pred2prob)}&quot;) if self.is_multilabel: out.append(&#39; n&#39;) out.append(f&quot;{&#39;LOSS: &#39;.rjust(8)} {round(loss.item(), 4)}&quot;) results.append(draw_label(x, out)) return make_img_grid(results, img_size=None, ncol=ncol) . . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:92: UserWarning: Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked warnings.warn(&#34;Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked&#34;) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:94: UserWarning: `font_size` cannot be used when not using a custom font passed via `font_path` warnings.warn(f&#34;`font_size` cannot be used when not using a custom font passed via `font_path`&#34;) . PlotLowest Losses Grid . plotting the lowest losses in a grid fashion: . @patch @delegates(to=ClassificationInterpretationEx.plot_top_losses_grid, but=[&#39;largest&#39;]) def plot_lowest_losses_grid(self:ClassificationInterpretationEx, **kwargs): &quot;&quot;&quot;Plot the lowest losses. Exact opposite of `ClassificationInterpretationEx.plot_top_losses` &quot;&quot;&quot; return self.plot_top_losses_grid(__largest=False, **kwargs) . . Classification Report . scikit-learn Classification report: . import sklearn.metrics as skm @patch def print_classification_report(self:ClassificationInterpretationEx, as_dict=False): &quot;Get scikit-learn classification report&quot; # `flatten_check` and `skm.classification_report` don&#39;t play # nice together for multi-label # d,t = flatten_check(self.decoded, self.targs) d,t = self.decoded, self.targs if as_dict: return skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=[str(v) for v in self.vocab], output_dict=True) else: return skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=[str(v) for v in self.vocab], output_dict=False) . . precision recall f1-score support COVID 0.99 0.98 0.99 243 non-COVID 0.98 0.99 0.99 253 accuracy 0.99 496 macro avg 0.99 0.99 0.99 496 weighted avg 0.99 0.99 0.99 496 . TTA (Test Time Augmentation) . Getting the TTA Score on the validation set: . . 0.9939516186714172 . Checking the confusion matrix: . Exporting the learner into a pickle file: . learn.export() . . (#1) [Path(&#39;export.pkl&#39;)] . Resnet-50 Test . We train with smaller images of sizes 128*128 rather than orignal size of the image and also smaller batch sizes for faster training. . dls2=get_dls(128,128) . learn2 = cnn_learner(dls2, xresnet50, metrics=[error_rate,accuracy], wd=1e-1).to_fp16() . . Running the l.r Finder: . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . . Minimum/10: 8.32e-03, steepest point: 3.31e-04 . Training the model in first run: . learn2.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.825077 | 2.717013 | 0.504032 | 0.495968 | 00:09 | . 1 | 0.637306 | 0.874084 | 0.328629 | 0.671371 | 00:09 | . 2 | 0.538363 | 0.493991 | 0.203629 | 0.796371 | 00:09 | . 3 | 0.473004 | 0.253523 | 0.122984 | 0.877016 | 00:09 | . 4 | 0.427544 | 0.230788 | 0.098790 | 0.901210 | 00:09 | . plotting the curves of training process: . Unfreezing the model and then running l.r finder again for getting the optimal l.r rate (FineTuneing Approach): . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=2.75422871709452e-06) . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . . Minimum/10: 8.32e-03, steepest point: 3.31e-04 . learn2.dls2 = get_dls(12, 224)# training on orignal size learn2.fit_one_cycle( 12, slice(1e-5, 1e-4)) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.326667 | 0.237666 | 0.106855 | 0.893145 | 00:10 | . 1 | 0.314330 | 0.248609 | 0.110887 | 0.889113 | 00:10 | . 2 | 0.306127 | 0.233944 | 0.090726 | 0.909274 | 00:10 | . 3 | 0.306658 | 0.229167 | 0.094758 | 0.905242 | 00:10 | . 4 | 0.305483 | 0.281535 | 0.125000 | 0.875000 | 00:10 | . 5 | 0.293949 | 0.245766 | 0.102823 | 0.897177 | 00:10 | . 6 | 0.290125 | 0.226233 | 0.102823 | 0.897177 | 00:10 | . 7 | 0.279198 | 0.230645 | 0.110887 | 0.889113 | 00:10 | . 8 | 0.271748 | 0.244468 | 0.110887 | 0.889113 | 00:10 | . 9 | 0.270165 | 0.208932 | 0.086694 | 0.913306 | 00:10 | . 10 | 0.268667 | 0.208460 | 0.084677 | 0.915323 | 00:10 | . 11 | 0.264588 | 0.216885 | 0.096774 | 0.903226 | 00:10 | . Checking the curves again: . Checking the Confusion Matrix: . interp = ClassificationInterpretation.from_learner(learn2)# plot confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=50) . . Plotting top losses . learn2.save(&#39;resnet50run&#39;) . Path(&#39;models/resnet50run.pth&#39;) . learn2=learn2.load(&#39;resnet50run&#39;) . end test . interp = ClassificationInterpretation.from_learner(learn)# plot confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=50) . . interp.plot_top_losses(5, nrows=10)# plot top losses . . learn1=load_learner(&quot;export.pkl&quot;) . GradCam Testing . Steps for plotting GradCAM: . Create your Learner&#39;s test_dl w.r.t. one image and label-Compute activations (forward pass) and gradients (backward pass) | Compute gradcam-map (7x7 in this case) | Take mean of gradients across feature maps: (1280, 7, 7) --&gt; (1280, 1, 1) | Multiply mean activation: (1280,1,1) (1280,7,7) --&gt; (1280,7,7) | Sum (B) across all 1280 channels: (1280,7,7) --&gt; (7,7) | Plot gradcam-map over the image | These steps are shown below one by one and later combined in a Learner.gradcam call | 1. Create Learner&#39;s test_dl w.r.t. one image and label . def create_test_img(learn, f, return_img=True): img = PILImage.create(f) x = first(learn.dls.test_dl([f])) x = x[0] if return_img: return img,x return x . 2. Compute activations (forward pass) and gradients (backward pass) . def get_label_idx(learn:Learner, preds:torch.Tensor, label:Union[str,int,None]) -&gt; Tuple[int,str]: &quot;&quot;&quot;Either: * Get the label idx of a specific `label` * Get the max pred using `learn.loss_func.decode` and `learn.loss_func.activation` * Only works for `softmax` activations as the backward pass requires a scalar index * Throws a `RuntimeError` if the activation is a `sigmoid` activation &quot;&quot;&quot; if label is not None: # if `label` is a string, check that it exists in the vocab # and return the label&#39;s index if isinstance(label,str): if not label in learn.dls.vocab: raise ValueError(f&quot;&#39;{label}&#39; is not part of the Learner&#39;s vocab: {learn.dls.vocab}&quot;) return learn.dls.vocab.o2i[label], label # if `label` is an index, return itself elif isinstance(label,int): return label, learn.dls.vocab[label] else: raise TypeError(f&quot;Expected `str`, `int` or `None`, got {type(label)} instead&quot;) else: # if no `label` is specified, check that `learn.loss_func` has `decodes` # and `activation` implemented, run the predictions through them, # then check that the output length is 1. If not, the activation must be # sigmoid, which is incompatible if not hasattr(learn.loss_func, &#39;activation&#39;) or not hasattr(learn.loss_func, &#39;decodes&#39;): raise NotImplementedError(f&quot;learn.loss_func does not have `.activation` or `.decodes` methods implemented&quot;) decode_pred = compose(learn.loss_func.activation, learn.loss_func.decodes) label_idx = decode_pred(preds) if len(label_idx) &gt; 1: raise RuntimeError(f&quot;Output label idx must be of length==1. If your loss func has a sigmoid activation, please specify `label`&quot;) return label_idx, learn.dls.vocab[label_idx][0] . . def compute_gcam_items(learn: Learner, x: TensorImage, label: Union[str,int,None] = None, target_layer: Union[nn.Module, Callable, None] = None ) -&gt; Tuple[torch.Tensor]: &quot;&quot;&quot;Compute gradient and activations of `target_layer` of `learn.model` for `x` with respect to `label`. If `target_layer` is None, then it is set to `learn.model[:-1]` &quot;&quot;&quot; to_cuda(learn.model, x) target_layer = get_target_layer(learn, target_layer) with HookBwd(target_layer) as hook_g: with Hook(target_layer) as hook: preds = learn.model.eval()(x) activations = hook.stored label_idx, label = get_label_idx(learn,preds,label) #print(preds.shape, label, label_idx) #print(preds) preds[0, label_idx].backward() gradients = hook_g.stored preds = getattr(learn.loss_func, &#39;activation&#39;, noop)(preds) # remove the leading batch_size axis gradients = gradients [0] activations = activations[0] preds = preds.detach().cpu().numpy().flatten() return gradients, activations, preds, label . . shapes of gradients, activations and predictions: . &lt;ipython-input-137-c549b6a525cc&gt;:6: UserWarning: Detected a pooling layer in the model body. Unless this is intentional, ensure that the feature map is not flattened warnings.warn(f&#34;Detected a pooling layer in the model body. Unless this is intentional, ensure that the feature map is not flattened&#34;) . (torch.Size([2048, 7, 7]), torch.Size([2048, 7, 7]), (2,), &#39;COVID&#39;) . 3. Compute gradcam-map . def compute_gcam_map(gradients, activations) -&gt; torch.Tensor: &quot;&quot;&quot;Take the mean of `gradients`, multiply by `activations`, sum it up and return a GradCAM feature map &quot;&quot;&quot; # Mean over the feature maps. If you don&#39;t use `keepdim`, it returns # a value of shape (1280) which isn&#39;t amenable to `*` with the activations gcam_weights = gradients.mean(dim=[1,2], keepdim=True) # (1280,7,7) --&gt; (1280,1,1) gcam_map = (gcam_weights * activations) # (1280,1,1) * (1280,7,7) --&gt; (1280,7,7) gcam_map = gcam_map.sum(0) # (1280,7,7) --&gt; (7,7) return gcam_map . . gcam_map = compute_gcam_map(gradients, activations) gcam_map.shape . torch.Size([7, 7]) . 4. Plot gradcam-map over the image . plotting Grad Cam over image . plot_gcam(learn, img3, x, gcam_map, full_size=True, dpi=300) . . learn.model[1] . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=4096, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . learn.gradcam(item=im, target_layer=learn.model[0]) . . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:92: UserWarning: Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked warnings.warn(&#34;Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked&#34;) . GUI Building . Creating Buttons: . btn_upload = widgets.FileUpload() btn_upload . img= PILImage.create(btn_upload.data[-1]) . img.shape . (350, 408) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(384,404)) out_pl . dls.vocab . [&#39;COVID&#39;, &#39;non-COVID&#39;] . pred,pred_idx,probs = learn.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;,layout=Layout(width=&#39;40%&#39;, height=&#39;80px&#39;), button_style=&#39;success&#39;) btn_run . Click event handler adds functionallity to butttons: . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(320,320)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Adding heatmaps button and functionallity: . HeatMp = widgets.Button(description=&#39;MAGIC&#39;, layout=btn_run.layout, button_style=&#39;danger&#39;) HeatMp . def on_click_map(change): with out_pl: display(img.to_thumb(320,320)) learn.gradcam(img).clear(out_pl) HeatMp.on_click(on_click_map) . Putting all the pieces together in a Vertical Stack for the final GUI: . VBox([widgets.Label(&#39;INPUT YOUR CT SCAN IMAGE FOR DETECTION!&#39;), btn_upload, btn_run, out_pl, lbl_pred,widgets.Label(&#39;Do You Want to See How our Model Decides which is Covid and Which is not?&#39;),widgets.Label(&quot;Click Here To Learn how These predictions are made&quot;), HeatMp]) . If You want to see the GUI that I built for this Project check out my other blog post named: Covify . What Worked? . Using a pretrained model Resnet reduced training time and improved results. | Data Augmentations reduced overfitting. | The Mixup approach worked like a charm and also prevented overfitting. | Presizing approaches worked. | I tried Progressive Resizing approach and it greatly improved results and reduced training time. | . What didn&#39;t? . I tried implementing bottleneck layers design on resnets but training was unstable. | I tried a deeper vanilla Resnet 101 model but did not noticed a a significant difference. | . Other ideas to improve the results? . Trying with Diffrent Architectures like Densenet, Efficient Net etc. | Trying out diffrent metrics and improving on them for better results. | More Compute: Deeper Models. Use cross-validation with several folds and Ensemble models. | . Thank you for reading this far!This was a great challenge and I learned a lot throughout this process.There is also a lot of room for improvement and work to do :) .",
            "url": "https://priyank7n.me/2021/01/23/covify-code.html",
            "relUrl": "/2021/01/23/covify-code.html",
            "date": "  Jan 23, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Covify",
            "content": "About . GUI and its Functionalities are: . We can upload a CT Scan Image, and our model Covify will predict whether the image uploaded is of a Covid Infected person or not. | When we click the Classify button, the model gives a softmax probability of the decision is made and how strongly it tries to put forward its integrity. | When we click on the Magic Button, it shows a heat map of all the activations in the image that influenced the model to make certain decisions. | . Covid Prediction on CT Scan Images . Non-Covid Prediction on CT Scan Images . When we click on CT Scan images, Our Model predicts whether it is covid or not and shows a heat map demonstrating the basis of the model&#39;s predictions . Heatmaps . This is a heat map for the image showcasing which activations or parts of the image led the model to predict certain decisions on whether the person is covid or not. . Applications . Deep learning, a popular research area of artificial intelligence (AI), enables the creation of end-to-end models to achieve promised results using input data without the need for manual feature extraction. . | We have used CT scan images to not sacrifice the quality of diagnosis and improve the speed of data diagnosis. . | To combat COVID, the Current need of the hour is building Medical Diagnosis Support Systems that are Fast, Reliable, Efficient, and Effective. . | Conventional Covid-19 tests that, is PCR (Polymerase chain reaction) test aretime-consuming and also leads to much more False-Negative and False Positive predictions . | We have to send the sample of PCR to the labs, which are sometimes in faraway locations that is far time consuming | Sometimes, When the doctors and Radiologists are not available at that time, we can  generate a preliminary diagnosis | Application of machine learning methods for automatic diagnosis in the medical field have recently gained popularity, i.e., have become far more essential in early detection | Fast and accurate diagnostic methods are heavily needed to combat the disease, so more and more time should be invested in Disease Control | . References . https://www.medrxiv.org/content/10.1101/2020.04.24.20078584v3 https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset .",
            "url": "https://priyank7n.me/2021/01/20/Covify-GUI.html",
            "relUrl": "/2021/01/20/Covify-GUI.html",
            "date": "  Jan 20, 2021"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey Fella My Name is Priyank Negi. I am a first-year graduate student at the Boston University College of Engineering, majoring in Electrical and Computer Engineering with a specialization in Data Analytics. I completed my bachelors degree, focused in Electronics and Communication Engineering, in New Delhi, India. I am learning and exploring new stuff and writing about my interests in topics where AI skills could be leveraged and entrepreneurial opportunities could be created. . Throughout my undergraduate term of 4 years, I was actively involved in the ML and Data Science domain and gained technical experience through internships, global competitions, research work, and academic and community-aided projects. Apart from this, I have experience in leadership and volunteering positions in and outside of school. I find myself well in international and interdisciplinary Teams; I am also a team player in groups but am also very comfortable working independently. . Being an AI enthusiast, I enjoy exploring, researching topics, and leveraging AI skills that create entrepreneurial opportunities. I believe in learning by doing and remaining curious throughout the process instead of the conventional approach of learning everything but doing nothing! Lots of Experimentation and Project-based learning are what I go by! I am also a big believer in the importance of Data Ethics in developing excellent AI systems and look forward to working with diverse people from interdisciplinary fields to create solutions that utilize data and make a societal impact. . Thank you for reading this far!! . My Resume is Listed here, Please click on this hyper link below to get redirected to my Resume: . Click here to get redirected to Priyanks Resume . Also If you want to reach out!, Please reach out to me on LinkedIn! . .",
          "url": "https://priyank7n.me/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page7": {
          "title": "",
          "content": "",
          "url": "https://priyank7n.me/temp",
          "relUrl": "/temp",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ sitemap.xml | absolute_url }} | .",
          "url": "https://priyank7n.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}