{
  
    
        "post0": {
            "title": "Reproducing Reformer: Our Amazing Submission & Team Experience",
            "content": "Where we all met? here‚ù§Ô∏è . The Challenge . Way back in October 2020 the Papers With Code ML Reproducibility Challenge 2020 was launched and shared in the fast.ai forums. A few of us jumped at the chance to test our ML knowledge and push our skills. Fast forward 110 days since that initial post and we delivered our Reformer Reproducibility submission via OpenReview!!ü§© . Our Whole Project is Documented here : Project . The Wandb reports we made : reports . Here are a few reflections on our experience: what we enjoyed, tools we used and what we would have done differently: . TLDR; . Working as a team pushes your motivation, your skills and your throughput | nbdev for development, Weights &amp; Biases for tracking and Discord for communication | We could have better used task/project management tools more, maybe we needed a different tool | Next time we‚Äôll start experiments sooner and maybe pick a more practical paper | It was a massive learning experience and a lot of fun | . . Why participate . Implementing code from scratch is much more enjoyable and meaningful when there is a direct application, e.g. working towards this reproducibility challenge. Spending weeks and months focussed on a single paper forces you to understand the paper down to the last full stop. It also gives you a great appreciation of how difficult writing a good paper is, you see almost every word and sentence is chosen carefully to communicate a particular concept, problem or model setting. . N heads are better than one a.k.a. Multihead Attention . Our team was distributed across 6 countries and everyone had a somewhat different background, set of skills and personality. This mix was definitely beneficial for getting things done much more smoothly. Having 2 x N eyes researching implementation information or reviewing code really improved coverage and sped up the entire process. It also makes debugging much faster! . . Writing code that the entire team will use also meant writing cleaner code with more tests so that it was as clear as possible for your teammates. And finally, during a long project like this it‚Äôs easy to get distracted or lazy, however seeing everyone else delivering great work quickly pulls you back into line! . . Good tools Are key for us : A good tool improves the way you work. A great tool improves the way you think. . Read more: https://www.wisesayings.com/tool-quotes/#ixzz6mZj38LCP . nbdev . The nbdev literate programming environment from fast.ai was super convenient to minimise the project‚Äôs development friction. Writing tests as we developed meant that we caught multiple bugs early and auto-generation of docs lends itself immensely to the reproducibility of your code. Most of us will be using this again for our next projects. . Weights &amp; Biases . Weights &amp; Biases generously gave us a team account which enabled us all to log our experiments to a single project. Being directly able to link your runs and results to the final report was really nice. Also it&#39;s pretty exciting monitoring 10+ experiments live! . Discord . A Discord server worked really well for all our chat and voice communication. Frequent calls to catchup and agree on next steps were super useful. Todo lists and core pieces of code often ended up as pinned messages for quick reference and linking Github activity to a channel was useful for keeping an eye on new commits to the repo. . Overleaf . When it came to writing the final report in latex, Overleaf was a wonderful tool for collaborative editing. . ReviewNB . The ReviewNB app on GitHub was very useful for visualizing diffs in notebooks. . . Learn from the best . The Reformer architecture had several complex parts, and having Phil Wang&#39;s and HuggingFace&#39;s Github code was very helpful to understand design decisions and fix issues. . Things we can improve for the next time . Start experiments early . We started our experiments quite late in the project; as we aimed to reimplement Reformer in Pytorch (with reference to existing implementations) about ~90% of our time was spent on ensuring our implementation was faithful to the paper and that it was working correctly. In retrospect starting experiments earlier would have allowed more in depth exploration of what we observed while testing. Full scale experiments have a way of inducing problems you didn‚Äôt foresee during the implementation phase... . Task distribution and coordination . When working in a distributed and decentralized team, efficient task allocation and tracking is important. Early in the project todo lists lived in people‚Äôs heads, or were quickly buried under 50 chat messages. This was suboptimal for a number of reasons, including that it made involving new people in the project more challenging as they could not easily identify where they could best contribute. . We made a switch to Trello to better track open tasks. It worked reasonably well however its effectiveness was probably proportional to how much time a couple of team members had to review the kanban board, advocate for its use and focus the team‚Äôs attention there. The extra friction associated with needing to use another tool unconnected to Github or Discord was probably the reason for why we didn‚Äôt use it as much as we could have. Integrating Trello into our workflow or giving Github Projects a trial could have been useful. . More feedback . We had originally intended to get feedback from the fastai community during the project. In the end we were too late in sharing our material, so there wasn‚Äôt time for much feedback. Early feedback would have been very useful and the project might have benefited from some periodic summary of accomplishments and current problems. We could have solicited additional feedback from the authors too. . Distributed training . This was our first exposure to distributed training and unfortunately we had a lot of issues with it. We were also unable to log the results from distributed runs properly to Weights &amp; Biases. This slowed down our experiment iteration speed and is why we could not train our models for as long as we would have preferred. . . Choice of paper to reproduce . It would have been useful to calculate a rough estimate of the compute budget the paper‚Äôs experiments required before jumping into it. In the latter stages of the project we realised that we would be unable to fully replicate some of the paper‚Äôs experiments, but instead had to run scaled down versions. In addition, where your interest sits between theoretical and practical papers should be considered when selecting a paper for the challenge. . More tools . We could have tried even more handy tools such as knockknock to alert us when models are finished training and Github Projects for task management. . Some final thoughts . We came out of this project even more motivated compared to how we entered; a great indication that it was both enjoyable and useful for us! Our advice would be to not hesitate to join events like this one and challenge yourself, and try and find one or more other folks in the forums or Discord to work with. After successfully delivering our submission to the challenge we are all eager to work together again on our next project, stay tuned for more! . . Thanks for Reading This Far &#128591; . As always, I would love to hear your feedback, what could have been written better or clearer, you can find me on twitter &amp; Linkedin: twitter Linkedin .",
            "url": "https://priyank7n.github.io/fastblogs/nlp/reformer/transformers/language-modelling/2021/02/19/reformer-reproducibility-challenge.html",
            "relUrl": "/nlp/reformer/transformers/language-modelling/2021/02/19/reformer-reproducibility-challenge.html",
            "date": " ‚Ä¢ Feb 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "COVID-19 Infection Detection Using Deep Learning",
            "content": "Setup . Dataset is acquired from kaggle.link SARS-CoV-2 CT scan dataset is a public dataset, containing 1252 CT scans (computed tomography scan) from SARS-CoV-2 infected patients (COVID-19) and 1230 CT scans for SARS-CoV-2 non-infected patients. The dataset has been collected from real patients in Sao Paulo, Brazil. The dataset is available in kaggle. . import opendatasets as od dataset_url = &#39;https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset&#39; od.download(dataset_url) . . 2%|‚ñè | 5.00M/230M [00:00&lt;00:06, 34.4MB/s] . Downloading sarscov2-ctscan-dataset.zip to ./sarscov2-ctscan-dataset . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 230M/230M [00:04&lt;00:00, 52.3MB/s] . . path=Path(&#39;sarscov2-ctscan-dataset&#39;) . The Dataset contains two folders namely COVID &amp; non-COVID having CT Scan Images of patients: . path.ls() . . (#2) [Path(&#39;sarscov2-ctscan-dataset/COVID&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID&#39;)] . Preprocessing . Exploring Dataset Structure and displaying sample CT Scan directories: . path.ls() . . (#2) [Path(&#39;sarscov2-ctscan-dataset/COVID&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID&#39;)] . There are 1252 CT scan images from SARS-CoV-2 infected patients. . (path/&#39;COVID&#39;).ls() . . (#1252) [Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (10).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (100).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1000).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1001).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1002).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1003).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1004).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1005).png&#39;),Path(&#39;sarscov2-ctscan-dataset/COVID/Covid (1006).png&#39;)...] . There are 1230 CT scan images from SARS-CoV-2 non-infected patients. . (path/&#39;non-COVID&#39;).ls() . . (#1229) [Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (10).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (100).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1000).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1001).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1002).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1003).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1004).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1005).png&#39;),Path(&#39;sarscov2-ctscan-dataset/non-COVID/Non-Covid (1006).png&#39;)...] . Visualizing the Images: Lets look at some raw images in the dataset: . import PIL #looking into the images downloaded img1 = PIL.Image.open((path/&#39;COVID&#39;).ls()[0]) img1 . . Creating a Datablock . DataBlock API :We divide the dataset as train and valid set and use the random_state argument in order to replicate the result.The valid_pct argument represents the proportion of the dataset to include in the valid (in our case 20%). Presizing is done and Transformations are applied to images keeping 75% of the images and then normalized according to the imagenet stats for applying Transfer Learning later. . Creating Dataloaders . def get_dls(bs,size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, splitter=RandomSplitter(valid_pct=0.2, seed=42), item_tfms=Resize(460), #presizing is done batch_tfms=[*aug_transforms(size=size,min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path,bs=bs) . To Create dataloaders we use DataBlock API of fast ai: We use images of size 224*224 and a single batch containing 224 images. . dls=get_dls(224,224) . Visualizing the Dataloaders . The images in the dataloaders look like: . dls.show_batch(nrows=3, figsize=(7,6)) . . A batch of images in a grid look like: . @patch @delegates(to=draw_label, but=[&quot;font_color&quot;, &quot;location&quot;, &quot;draw_rect&quot;, &quot;fsize_div_factor&quot;, &quot;font_path&quot;, &quot;font_size&quot;]) def show_batch_grid(self:TfmdDL, b=None, n=20, ncol=4, show=True, unique=False, unique_each=True, font_path=None, font_size=20, **kwargs): &quot;&quot;&quot;Show a batch of images Key Params: * n: No. of images to display * n_col: No. of columns in the grid * unique: Display the same image with different augmentations * unique_each: If True, displays a different img on each call * font_path: Path to the `.ttf` font file. Required to display labels * font_size: Size of the font &quot;&quot;&quot; if font_path is not None: self.set_font_path(font_path) if not hasattr(self, &#39;font_path&#39;): self.font_path = font_path if unique: old_get_idxs = self.get_idxs if unique_each: i = np.random.choice(self.n) self.get_idxs = partial(itertools.repeat, i) else: self.get_idxs = lambda: Inf.zeros if b is None: b = self.one_batch() if not show: return self._pre_show_batch(b, max_n=n) _,__, b = self._pre_show_batch(b, max_n=n) if unique: self.get_idxs = old_get_idxs return make_img_grid([draw_label(i, font_path=self.font_path, font_size=font_size) for i in b], ncol=ncol, img_size=None) . . Transfer Learning . The Resnet50 model . What and why did we used Transfer Learning . Transfer learning is meaning use a pre-trained model to build our classifier. A pre-trained model is a model that has been previously trained on a dataset. The model comprehends the updated weights and bias. Using a pre-trained model you are saving time and computational resources. Another avantage is that pre-trained models often perform better that architecture designed from scratch. | To better understand this point, suppose we want to build a classifier able to sort different sailboat types. A model pre-trained on ships would have already capture in its first layers some boat features, learning faster and with better accuracy among the different sailboat types. | . | The Resnet50 architecture: . Resnet50 generally is considered a good choice as first architecture to test, it shows good performance without an excessive size allowing to use a higher batch size and thus less computation time. For this reason, before to test more complex architectures Resnet50 is a good compromise. | Residual net have been ideated to solve the problem of the vanishing gradient. Highly intricate networks with a large number of hidden layer are working effectively in solving complicated tasks. Their structures allow them to catch pattern in complicated data. When we train the network the early layer tend to be trained slower (the gradient are smaller during backpropagation). The initial layers are important because they learn the basic feature of an object (edge, corner and so on). Failing to proper train these layers lead to a decrease in the overall accuracy of the model. | Residual neural network have been ideated to solve this issue. The Resnet model presents the possibility to skip the training of some layer during the initial training. The skipped layer is reusing the learned weights from the previous layer. Original research article | . | Test the Resnet34 architecture with our dataset: . Now we are going to test how the FastaAI implementation of this architechture works with the COVID dataset. | Create the convolutional neural network First we will create the convolutional neural network based on this architechture, to do this we can use the following code block which uses FastAI ( cnn_learner previously create_cnn) function. We pass the loaded data, specify the model, pass error_rate &amp; accuracy as a list for the metrics parameter specifying we want to see both error_rate and accuracy, and finally specify a weight decay of 1e-1 (1.0). | learn.lr_find() &amp; learn.recorder.plot() function to run LR Finder. LR Finder help to find the best learning rate to use with our network. For more information the original paper. As shown from the output of above. . | learn.recorder.plot() function plot the loss over learning rate. Run the following code block to view the graph. The best learning rate should be chosen as the learning rate value where the curve is the steepest. You may try different learning rate values in order to pick up the best. . | [learn.fit_one_cycle() &amp; learn.recorder.plot_losses()] The learn.fit_one_cycle() function can be used to fit the model. Fit one cycle reach a comparable accuracy faster than th fit function in training of complex models. Fit one cycle instead of maintain fix the learning rate during all the iterations is linearly increasing the learning rate and then it is decreasing again (this process is what is called one cycle). Moreover, this learning rate variation is helping in preventing overfitting. We use 5 for the parameter cyc_len to specify the number of cycles to run (on cycle can be considered equivalent to an epoch), and max_lr to specify the maximum learning rate to use which we set as 0.001. Fit one cycle varies the learning rate from 10 fold less the maximum learning rate selected. For more information about fit one cycle: article. . | . | Testing with Deeper Architectures . learn = cnn_learner(dls, resnet101, loss_func=CrossEntropyLossFlat(), metrics=[error_rate,accuracy], wd=1e-1).to_fp16() . learn.cbs . (#4) [TrainEvalCallback,Recorder,ProgressCallback,MixedPrecision] . We apply a very powerful Data Augmentation technique that is Mixup and train the model. . learn.fit_one_cycle(80, 3e-3, cbs=MixUp(0.5)) . . epoch train_loss valid_loss error_rate accuracy time . 0 | 1.098607 | 0.923316 | 0.405242 | 0.594758 | 00:23 | . 1 | 0.959609 | 0.496383 | 0.227823 | 0.772177 | 00:23 | . 2 | 0.907947 | 0.482087 | 0.223790 | 0.776210 | 00:23 | . 3 | 0.874337 | 0.519458 | 0.252016 | 0.747984 | 00:22 | . 4 | 0.838613 | 0.474054 | 0.221774 | 0.778226 | 00:23 | . 5 | 0.800547 | 0.362412 | 0.159274 | 0.840726 | 00:23 | . 6 | 0.766916 | 0.340699 | 0.137097 | 0.862903 | 00:23 | . 7 | 0.732544 | 0.190343 | 0.060484 | 0.939516 | 00:23 | . 8 | 0.705716 | 0.234552 | 0.092742 | 0.907258 | 00:22 | . 9 | 0.672490 | 0.225245 | 0.092742 | 0.907258 | 00:23 | . 10 | 0.646345 | 0.196367 | 0.082661 | 0.917339 | 00:23 | . 11 | 0.614197 | 0.207071 | 0.082661 | 0.917339 | 00:22 | . 12 | 0.580931 | 0.145530 | 0.058468 | 0.941532 | 00:22 | . 13 | 0.551455 | 0.160765 | 0.062500 | 0.937500 | 00:22 | . 14 | 0.530735 | 0.156187 | 0.058468 | 0.941532 | 00:22 | . 15 | 0.508143 | 0.133556 | 0.040323 | 0.959677 | 00:23 | . 16 | 0.486114 | 0.130424 | 0.048387 | 0.951613 | 00:23 | . 17 | 0.468766 | 0.112146 | 0.036290 | 0.963710 | 00:23 | . 18 | 0.451705 | 0.106726 | 0.038306 | 0.961694 | 00:23 | . 19 | 0.435460 | 0.140806 | 0.046371 | 0.953629 | 00:23 | . 20 | 0.421944 | 0.129412 | 0.042339 | 0.957661 | 00:23 | . 21 | 0.409734 | 0.107145 | 0.034274 | 0.965726 | 00:23 | . 22 | 0.399341 | 0.125089 | 0.042339 | 0.957661 | 00:23 | . 23 | 0.389513 | 0.099228 | 0.036290 | 0.963710 | 00:23 | . 24 | 0.380188 | 0.082548 | 0.018145 | 0.981855 | 00:23 | . 25 | 0.370885 | 0.071890 | 0.016129 | 0.983871 | 00:23 | . 26 | 0.363348 | 0.126151 | 0.044355 | 0.955645 | 00:23 | . 27 | 0.356708 | 0.085095 | 0.022177 | 0.977823 | 00:23 | . 28 | 0.351457 | 0.082022 | 0.030242 | 0.969758 | 00:23 | . 29 | 0.346920 | 0.082360 | 0.022177 | 0.977823 | 00:23 | . 30 | 0.343117 | 0.086793 | 0.026210 | 0.973790 | 00:23 | . 31 | 0.338743 | 0.084433 | 0.028226 | 0.971774 | 00:23 | . 32 | 0.332180 | 0.050694 | 0.012097 | 0.987903 | 00:23 | . 33 | 0.329243 | 0.075656 | 0.022177 | 0.977823 | 00:23 | . 34 | 0.325888 | 0.074826 | 0.018145 | 0.981855 | 00:22 | . 35 | 0.321171 | 0.051103 | 0.016129 | 0.983871 | 00:22 | . 36 | 0.317992 | 0.068456 | 0.014113 | 0.985887 | 00:23 | . 37 | 0.317117 | 0.095658 | 0.038306 | 0.961694 | 00:23 | . 38 | 0.314691 | 0.075247 | 0.026210 | 0.973790 | 00:23 | . 39 | 0.312669 | 0.059977 | 0.014113 | 0.985887 | 00:23 | . 40 | 0.311207 | 0.062207 | 0.016129 | 0.983871 | 00:23 | . 41 | 0.307136 | 0.079891 | 0.032258 | 0.967742 | 00:23 | . 42 | 0.303215 | 0.060350 | 0.014113 | 0.985887 | 00:23 | . 43 | 0.301979 | 0.061862 | 0.014113 | 0.985887 | 00:23 | . 44 | 0.302073 | 0.056083 | 0.012097 | 0.987903 | 00:23 | . 45 | 0.298332 | 0.054264 | 0.014113 | 0.985887 | 00:23 | . 46 | 0.297571 | 0.050670 | 0.006048 | 0.993952 | 00:23 | . 47 | 0.295835 | 0.053044 | 0.014113 | 0.985887 | 00:23 | . 48 | 0.295003 | 0.053177 | 0.006048 | 0.993952 | 00:23 | . 49 | 0.295658 | 0.070317 | 0.012097 | 0.987903 | 00:23 | . 50 | 0.293548 | 0.051080 | 0.008064 | 0.991935 | 00:23 | . 51 | 0.293651 | 0.061804 | 0.016129 | 0.983871 | 00:23 | . 52 | 0.291592 | 0.044012 | 0.010081 | 0.989919 | 00:23 | . 53 | 0.289374 | 0.047382 | 0.006048 | 0.993952 | 00:23 | . 54 | 0.288036 | 0.050668 | 0.006048 | 0.993952 | 00:23 | . 55 | 0.286396 | 0.057323 | 0.016129 | 0.983871 | 00:23 | . 56 | 0.285277 | 0.049304 | 0.012097 | 0.987903 | 00:23 | . 57 | 0.283157 | 0.047652 | 0.010081 | 0.989919 | 00:23 | . 58 | 0.282250 | 0.046741 | 0.008064 | 0.991935 | 00:23 | . 59 | 0.282024 | 0.043001 | 0.006048 | 0.993952 | 00:23 | . 60 | 0.281422 | 0.043425 | 0.004032 | 0.995968 | 00:22 | . 61 | 0.279792 | 0.048245 | 0.004032 | 0.995968 | 00:23 | . 62 | 0.278259 | 0.050301 | 0.008064 | 0.991935 | 00:23 | . 63 | 0.276450 | 0.042498 | 0.006048 | 0.993952 | 00:23 | . 64 | 0.275557 | 0.043382 | 0.008064 | 0.991935 | 00:23 | . 65 | 0.274992 | 0.046327 | 0.008064 | 0.991935 | 00:23 | . 66 | 0.274949 | 0.051264 | 0.012097 | 0.987903 | 00:23 | . 67 | 0.276006 | 0.050355 | 0.010081 | 0.989919 | 00:23 | . 68 | 0.277512 | 0.047513 | 0.008064 | 0.991935 | 00:22 | . 69 | 0.275122 | 0.044733 | 0.006048 | 0.993952 | 00:22 | . 70 | 0.275745 | 0.042205 | 0.006048 | 0.993952 | 00:23 | . 71 | 0.274163 | 0.041508 | 0.006048 | 0.993952 | 00:23 | . 72 | 0.273943 | 0.042359 | 0.006048 | 0.993952 | 00:23 | . 73 | 0.273899 | 0.042546 | 0.006048 | 0.993952 | 00:23 | . 74 | 0.271843 | 0.044013 | 0.006048 | 0.993952 | 00:23 | . 75 | 0.271735 | 0.043344 | 0.006048 | 0.993952 | 00:22 | . 76 | 0.271392 | 0.045417 | 0.008064 | 0.991935 | 00:23 | . 77 | 0.270836 | 0.044158 | 0.006048 | 0.993952 | 00:23 | . 78 | 0.272595 | 0.043604 | 0.008064 | 0.991935 | 00:23 | . 79 | 0.272234 | 0.044281 | 0.008064 | 0.991935 | 00:23 | . TTA(Test Time Augmentation) . preds,targs = learn.tta() # TTA applied for validation dataset accuracy(preds, targs).item() . . . 0.9959677457809448 . We get a TTA of 99.59% on the validation set. . ClassificationInterpretationEx . We examine the model predictions in more depth: . import fastai def _get_truths(vocab, label_idx, is_multilabel): if is_multilabel: return &#39;;&#39;.join([vocab[i] for i in torch.where(label_idx==1)][0]) else: return vocab[label_idx] class ClassificationInterpretationEx(ClassificationInterpretation): &quot;&quot;&quot; Extend fastai2&#39;s `ClassificationInterpretation` to analyse model predictions in more depth See: * self.preds_df * self.plot_label_confidence() * self.plot_confusion_matrix() * self.plot_accuracy() * self.get_fnames() * self.plot_top_losses_grid() * self.print_classification_report() &quot;&quot;&quot; def __init__(self, dl, inputs, preds, targs, decoded, losses): super().__init__(dl, inputs, preds, targs, decoded, losses) self.vocab = self.dl.vocab if is_listy(self.vocab): self.vocab = self.vocab[-1] if self.targs.__class__ == fastai.torch_core.TensorMultiCategory: self.is_multilabel = True else: self.is_multilabel = False self.compute_label_confidence() self.determine_classifier_type() def determine_classifier_type(self): if self.targs[0].__class__==fastai.torch_core.TensorCategory: self.is_multilabel = False if self.targs[0].__class__==fastai.torch_core.TensorMultiCategory: self.is_multilabel = True self.thresh = self.dl.loss_func.thresh def compute_label_confidence(self, df_colname:Optional[str]=&quot;fnames&quot;): &quot;&quot;&quot; Collate prediction confidence, filenames, and ground truth labels in DataFrames, and store them as class attributes `self.preds_df` and `self.preds_df_each` If the `DataLoaders` is constructed from a `pd.DataFrame`, use `df_colname` to specify the column name with the filepaths &quot;&quot;&quot; if not isinstance(self.dl.items, pd.DataFrame): self._preds_collated = [ #(item, self.dl.vocab[label_idx], *preds.numpy()*100) (item, _get_truths(self.dl.vocab, label_idx, self.is_multilabel), *preds.numpy()*100) for item,label_idx,preds in zip(self.dl.items, self.targs, self.preds) ] ## need to extract fname from DataFrame elif isinstance(self.dl.items, pd.DataFrame): self._preds_collated = [ #(item[df_colname], self.dl.vocab[label_idx], *preds.numpy()*100) (item[df_colname], _get_truths(self.dl.vocab, label_idx, self.is_multilabel), *preds.numpy()*100) for (_,item),label_idx,preds in zip(self.dl.items.iterrows(), self.targs, self.preds) ] self.preds_df = pd.DataFrame(self._preds_collated, columns = [&#39;fname&#39;,&#39;truth&#39;, *self.dl.vocab]) self.preds_df.insert(2, column=&#39;loss&#39;, value=self.losses.numpy()) if self.is_multilabel: return # preds_df_each doesnt make sense for multi-label self._preds_df_each = {l:self.preds_df.copy()[self.preds_df.truth == l].reset_index(drop=True) for l in self.dl.vocab} self.preds_df_each = defaultdict(dict) sort_desc = lambda x,col: x.sort_values(col, ascending=False).reset_index(drop=True) for label,df in self._preds_df_each.items(): filt = df[label] == df[self.dl.vocab].max(axis=1) self.preds_df_each[label][&#39;accurate&#39;] = df.copy()[filt] self.preds_df_each[label][&#39;inaccurate&#39;] = df.copy()[~filt] self.preds_df_each[label][&#39;accurate&#39;] = sort_desc(self.preds_df_each[label][&#39;accurate&#39;], label) self.preds_df_each[label][&#39;inaccurate&#39;] = sort_desc(self.preds_df_each[label][&#39;inaccurate&#39;], label) assert len(self.preds_df_each[label][&#39;accurate&#39;]) + len(self.preds_df_each[label][&#39;inaccurate&#39;]) == len(df) def get_fnames(self, label:str, mode:(&#39;accurate&#39;,&#39;inaccurate&#39;), conf_level:Union[int,float,tuple]) -&gt; np.ndarray: &quot;&quot;&quot; Utility function to grab filenames of a particular label `label` that were classified as per `mode` (accurate|inaccurate). These filenames are filtered by `conf_level` which can be above or below a certain threshold (above if `mode` == &#39;accurate&#39; else below), or in confidence ranges &quot;&quot;&quot; assert label in self.dl.vocab if not hasattr(self, &#39;preds_df_each&#39;): self.compute_label_confidence() df = self.preds_df_each[label][mode].copy() if mode == &#39;accurate&#39;: if isinstance(conf_level, tuple): filt = df[label].between(*conf_level) if isinstance(conf_level, (int,float)): filt = df[label] &gt; conf_level if mode == &#39;inaccurate&#39;: if isinstance(conf_level, tuple): filt = df[label].between(*conf_level) if isinstance(conf_level, (int,float)): filt = df[label] &lt; conf_level return df[filt].fname.values . . fname truth loss COVID non-COVID . 0 sarscov2-ctscan-dataset/non-COVID/Non-Covid (386).png | non-COVID | 0.047037 | 4.594821 | 95.405182 | . 1 sarscov2-ctscan-dataset/COVID/Covid (581).png | COVID | 0.018380 | 98.178818 | 1.821182 | . ClassificationInterpretationEx.get_fnames . Returns accuratly classified files with accuracy above 85%: . interp.get_fnames(&#39;accurate&#39;, 99.95) . . Returns inaccurately classified files with accuracy between 84.1-85.2%: . interp.get_fnames(&#39;img1&#39;, &#39;accurate&#39;, (84.1, 85.2)) . . Confusion Matrix . Checking the Confusion Matrix: . Plot Accuracy . plotting curves of training process: . functions to plot the accuracy of the labels: . @patch def plot_accuracy(self:ClassificationInterpretationEx, width=0.9, figsize=(6,6), return_fig=False, title=&#39;Accuracy Per Label&#39;, ylabel=&#39;Accuracy (%)&#39;, style=&#39;ggplot&#39;, color=&#39;#2a467e&#39;, vertical_labels=True): &#39;Plot a bar plot showing accuracy per label&#39; if not hasattr(self, &#39;preds_df_each&#39;): raise NotImplementedError plt.style.use(style) if not hasattr(self, &#39;preds_df_each&#39;): self.compute_label_confidence() self.accuracy_dict = defaultdict() for label,df in self.preds_df_each.items(): total = len(df[&#39;accurate&#39;]) + len(df[&#39;inaccurate&#39;]) self.accuracy_dict[label] = 100 * len(df[&#39;accurate&#39;]) / total fig,ax = plt.subplots(figsize=figsize) x = self.accuracy_dict.keys() y = [v for k,v in self.accuracy_dict.items()] rects = ax.bar(x,y,width,color=color) for rect in rects: ht = rect.get_height() ax.annotate(s = f&quot;{ht:.02f}&quot;, xy = (rect.get_x() + rect.get_width()/2, ht), xytext = (0,3), # offset vertically by 3 points textcoords = &#39;offset points&#39;, ha = &#39;center&#39;, va = &#39;bottom&#39; ) ax.set_ybound(lower=0, upper=100) ax.set_yticks(np.arange(0,110,10)) ax.set_ylabel(ylabel) ax.set_xticklabels(x, rotation=&#39;vertical&#39; if vertical_labels else &#39;horizontal&#39;) plt.suptitle(title) plt.tight_layout() if return_fig: return fig . . &lt;ipython-input-67-2c6afab5379f&gt;:25: MatplotlibDeprecationWarning: The &#39;s&#39; parameter of annotate() has been renamed &#39;text&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. ax.annotate(s = f&#34;{ht:.02f}&#34;, &lt;ipython-input-67-2c6afab5379f&gt;:34: UserWarning: FixedFormatter should only be used together with FixedLocator ax.set_xticklabels(x, rotation=&#39;vertical&#39; if vertical_labels else &#39;horizontal&#39;) . Plot Label Confidence . Plotting label confidence as histograms for each label: . @patch def plot_label_confidence(self:ClassificationInterpretationEx, bins:int=5, fig_width:int=12, fig_height_base:int=4, title:str=&#39;Accurate vs. Inaccurate Predictions Confidence (%) Levels Per Label&#39;, return_fig:bool=False, label_bars:bool=True, style=&#39;ggplot&#39;, dpi=150, accurate_color=&#39;#2a467e&#39;, inaccurate_color=&#39;#dc4a46&#39;): &quot;&quot;&quot;Plot label confidence histograms for each label Key Args: * `bins`: No. of bins on each side of the plot * `return_fig`: If True, returns the figure that can be easily saved to disk * `label_bars`: If True, displays the % of samples that fall into each bar * `style`: A matplotlib style. See `plt.style.available` for more * `accurate_color`: Color of the accurate bars * `inaccurate_color`: Color of the inaccurate bars &quot;&quot;&quot; if not hasattr(self, &#39;preds_df_each&#39;): raise NotImplementedError plt.style.use(style) fig, axes = plt.subplots(nrows = len(self.preds_df_each.keys()), ncols=2, dpi=dpi, figsize = (fig_width, fig_height_base * len(self.dl.vocab))) for i, (label, df) in enumerate(self.preds_df_each.items()): height=0 # find max height for mode in [&#39;inaccurate&#39;, &#39;accurate&#39;]: len_bins,_ = np.histogram(df[mode][label], bins=bins) if len_bins.max() &gt; height: height=len_bins.max() for mode,ax in zip([&#39;inaccurate&#39;, &#39;accurate&#39;], axes[i]): range_ = (50,100) if mode == &#39;accurate&#39; else (0,50) color = accurate_color if mode == &#39;accurate&#39; else inaccurate_color num,_,patches = ax.hist(df[mode][label], bins=bins, range=range_, rwidth=.95, color=color) num_samples = len(df[&#39;inaccurate&#39;][label]) + len(df[&#39;accurate&#39;][label]) pct_share = len(df[mode][label]) / num_samples if label_bars: for rect in patches: ht = rect.get_height() ax.annotate(s = f&quot;{round((int(ht) / num_samples) * 100, 1) if ht &gt; 0 else 0}%&quot;, xy = (rect.get_x() + rect.get_width()/2, ht), xytext = (0,3), # offset vertically by 3 points textcoords = &#39;offset points&#39;, ha = &#39;center&#39;, va = &#39;bottom&#39; ) ax.set_ybound(upper=height + height*0.3) ax.set_xlabel(f&#39;{label}: {mode.capitalize()} ({round(pct_share * 100, 2)}%)&#39;) ax.set_ylabel(f&#39;Num. {mode.capitalize()} ({len(df[mode][label])} of {num_samples})&#39;) fig.suptitle(title, y=1.0) plt.subplots_adjust(top = 0.9, bottom=0.01, hspace=0.25, wspace=0.2) plt.tight_layout() if return_fig: return fig . . &lt;ipython-input-69-1241d4a5b1b6&gt;:37: MatplotlibDeprecationWarning: The &#39;s&#39; parameter of annotate() has been renamed &#39;text&#39; since Matplotlib 3.3; support for the old name will be dropped two minor releases later. ax.annotate(s = f&#34;{round((int(ht) / num_samples) * 100, 1) if ht &gt; 0 else 0}%&#34;, . Plot Top Losses grid . plotting the top losses in a grid: . from fastai_amalgam.utils import * @patch def plot_top_losses_grid(self:ClassificationInterpretationEx, k=16, ncol=4, __largest=True, font_path=None, font_size=12, use_dedicated_layout=True) -&gt; PIL.Image.Image: &quot;&quot;&quot;Plot top losses in a grid Uses fastai&#39;a `ClassificationInterpretation.plot_top_losses` to fetch predictions, and makes a grid with the ground truth labels, predictions, prediction confidence and loss ingrained into the image By default, `use_dedicated_layout` is used to plot the loss (bottom), truths (top-left), and predictions (top-right) in dedicated areas of the image. If this is set to `False`, everything is printed at the bottom of the image &quot;&quot;&quot; # all of the pred fetching code is copied over from # fastai&#39;s `ClassificationInterpretation.plot_top_losses` # and only plotting code is added here losses,idx = self.top_losses(k, largest=__largest) if not isinstance(self.inputs, tuple): self.inputs = (self.inputs,) if isinstance(self.inputs[0], Tensor): inps = tuple(o[idx] for o in self.inputs) else: inps = self.dl.create_batch(self.dl.before_batch([tuple(o[i] for o in self.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (self.targs if is_listy(self.targs) else (self.targs,))) x,y,its = self.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (self.decoded if is_listy(self.decoded) else (self.decoded,))) x1,y1,outs = self.dl._pre_show_batch(b_out, max_n=k) #if its is not None: # _plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), self.preds[idx], losses, **kwargs) plot_items = its.itemgot(0), its.itemgot(1), outs.itemgot(slice(len(inps), None)), self.preds[idx], losses def draw_label(x:TensorImage, labels): return PILImage.create(x).draw_labels(labels, font_path=font_path, font_size=font_size, location=&quot;bottom&quot;) # return plot_items results = [] for x, truth, preds, preds_raw, loss in zip(*plot_items): if self.is_multilabel: preds = preds[0] probs_i = np.array([self.dl.vocab.o2i[o] for o in preds]) pred2prob = [f&quot;{pred} ({round(prob.item()*100,2)}%)&quot; for pred,prob in zip(preds,preds_raw[probs_i])] if use_dedicated_layout: # draw loss at the bottom, preds on top-right # and truths on the top img = PILImage.create(x) if isinstance(truth, Category): truth = [truth] truth.insert(0, &quot;TRUTH: &quot;) pred2prob.insert(0, &#39;PREDS: &#39;) loss_text = f&quot;{&#39;LOSS: &#39;.rjust(8)} {round(loss.item(), 4)}&quot; img.draw_labels(truth, location=&quot;top-left&quot;, font_size=font_size, font_path=font_path) img.draw_labels(pred2prob, location=&quot;top-right&quot;, font_size=font_size, font_path=font_path) img.draw_labels(loss_text, location=&quot;bottom&quot;, font_size=font_size, font_path=font_path) results.append(img) else: # draw everything at the bottom out = [] out.append(f&quot;{&#39;TRUTH: &#39;.rjust(8)} {truth}&quot;) bsl = &#39; n&#39; # since f-strings can&#39;t have backslashes out.append(f&quot;{&#39;PRED: &#39;.rjust(8)} {bsl.join(pred2prob)}&quot;) if self.is_multilabel: out.append(&#39; n&#39;) out.append(f&quot;{&#39;LOSS: &#39;.rjust(8)} {round(loss.item(), 4)}&quot;) results.append(draw_label(x, out)) return make_img_grid(results, img_size=None, ncol=ncol) . . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:92: UserWarning: Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked warnings.warn(&#34;Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked&#34;) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:94: UserWarning: `font_size` cannot be used when not using a custom font passed via `font_path` warnings.warn(f&#34;`font_size` cannot be used when not using a custom font passed via `font_path`&#34;) . PlotLowest Losses Grid . plotting the lowest losses in a grid fashion: . @patch @delegates(to=ClassificationInterpretationEx.plot_top_losses_grid, but=[&#39;largest&#39;]) def plot_lowest_losses_grid(self:ClassificationInterpretationEx, **kwargs): &quot;&quot;&quot;Plot the lowest losses. Exact opposite of `ClassificationInterpretationEx.plot_top_losses` &quot;&quot;&quot; return self.plot_top_losses_grid(__largest=False, **kwargs) . . Classification Report . scikit-learn Classification report: . import sklearn.metrics as skm @patch def print_classification_report(self:ClassificationInterpretationEx, as_dict=False): &quot;Get scikit-learn classification report&quot; # `flatten_check` and `skm.classification_report` don&#39;t play # nice together for multi-label # d,t = flatten_check(self.decoded, self.targs) d,t = self.decoded, self.targs if as_dict: return skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=[str(v) for v in self.vocab], output_dict=True) else: return skm.classification_report(t, d, labels=list(self.vocab.o2i.values()), target_names=[str(v) for v in self.vocab], output_dict=False) . . precision recall f1-score support COVID 0.99 0.98 0.99 243 non-COVID 0.98 0.99 0.99 253 accuracy 0.99 496 macro avg 0.99 0.99 0.99 496 weighted avg 0.99 0.99 0.99 496 . TTA (Test Time Augmentation) . Getting the TTA Score on the validation set: . . 0.9939516186714172 . Checking the confusion matrix: . Exporting the learner into a pickle file: . learn.export() . . (#1) [Path(&#39;export.pkl&#39;)] . Resnet-50 Test . We train with smaller images of sizes 128*128 rather than orignal size of the image and also smaller batch sizes for faster training. . dls2=get_dls(128,128) . learn2 = cnn_learner(dls2, xresnet50, metrics=[error_rate,accuracy], wd=1e-1).to_fp16() . . Running the l.r Finder: . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . . Minimum/10: 8.32e-03, steepest point: 3.31e-04 . Training the model in first run: . learn2.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.825077 | 2.717013 | 0.504032 | 0.495968 | 00:09 | . 1 | 0.637306 | 0.874084 | 0.328629 | 0.671371 | 00:09 | . 2 | 0.538363 | 0.493991 | 0.203629 | 0.796371 | 00:09 | . 3 | 0.473004 | 0.253523 | 0.122984 | 0.877016 | 00:09 | . 4 | 0.427544 | 0.230788 | 0.098790 | 0.901210 | 00:09 | . plotting the curves of training process: . Unfreezing the model and then running l.r finder again for getting the optimal l.r rate (FineTuneing Approach): . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=2.75422871709452e-06) . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . . Minimum/10: 8.32e-03, steepest point: 3.31e-04 . learn2.dls2 = get_dls(12, 224)# training on orignal size learn2.fit_one_cycle( 12, slice(1e-5, 1e-4)) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.326667 | 0.237666 | 0.106855 | 0.893145 | 00:10 | . 1 | 0.314330 | 0.248609 | 0.110887 | 0.889113 | 00:10 | . 2 | 0.306127 | 0.233944 | 0.090726 | 0.909274 | 00:10 | . 3 | 0.306658 | 0.229167 | 0.094758 | 0.905242 | 00:10 | . 4 | 0.305483 | 0.281535 | 0.125000 | 0.875000 | 00:10 | . 5 | 0.293949 | 0.245766 | 0.102823 | 0.897177 | 00:10 | . 6 | 0.290125 | 0.226233 | 0.102823 | 0.897177 | 00:10 | . 7 | 0.279198 | 0.230645 | 0.110887 | 0.889113 | 00:10 | . 8 | 0.271748 | 0.244468 | 0.110887 | 0.889113 | 00:10 | . 9 | 0.270165 | 0.208932 | 0.086694 | 0.913306 | 00:10 | . 10 | 0.268667 | 0.208460 | 0.084677 | 0.915323 | 00:10 | . 11 | 0.264588 | 0.216885 | 0.096774 | 0.903226 | 00:10 | . Checking the curves again: . Checking the Confusion Matrix: . interp = ClassificationInterpretation.from_learner(learn2)# plot confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=50) . . Plotting top losses . learn2.save(&#39;resnet50run&#39;) . Path(&#39;models/resnet50run.pth&#39;) . learn2=learn2.load(&#39;resnet50run&#39;) . end test . interp = ClassificationInterpretation.from_learner(learn)# plot confusion matrix interp.plot_confusion_matrix(figsize=(12,12), dpi=50) . . interp.plot_top_losses(5, nrows=10)# plot top losses . . learn1=load_learner(&quot;export.pkl&quot;) . GradCam Testing . Steps for plotting GradCAM: . Create your Learner&#39;s test_dl w.r.t. one image and label-Compute activations (forward pass) and gradients (backward pass) | Compute gradcam-map (7x7 in this case) | Take mean of gradients across feature maps: (1280, 7, 7) --&gt; (1280, 1, 1) | Multiply mean activation: (1280,1,1) (1280,7,7) --&gt; (1280,7,7) | Sum (B) across all 1280 channels: (1280,7,7) --&gt; (7,7) | Plot gradcam-map over the image | These steps are shown below one by one and later combined in a Learner.gradcam call | 1. Create Learner&#39;s test_dl w.r.t. one image and label . def create_test_img(learn, f, return_img=True): img = PILImage.create(f) x = first(learn.dls.test_dl([f])) x = x[0] if return_img: return img,x return x . 2. Compute activations (forward pass) and gradients (backward pass) . def get_label_idx(learn:Learner, preds:torch.Tensor, label:Union[str,int,None]) -&gt; Tuple[int,str]: &quot;&quot;&quot;Either: * Get the label idx of a specific `label` * Get the max pred using `learn.loss_func.decode` and `learn.loss_func.activation` * Only works for `softmax` activations as the backward pass requires a scalar index * Throws a `RuntimeError` if the activation is a `sigmoid` activation &quot;&quot;&quot; if label is not None: # if `label` is a string, check that it exists in the vocab # and return the label&#39;s index if isinstance(label,str): if not label in learn.dls.vocab: raise ValueError(f&quot;&#39;{label}&#39; is not part of the Learner&#39;s vocab: {learn.dls.vocab}&quot;) return learn.dls.vocab.o2i[label], label # if `label` is an index, return itself elif isinstance(label,int): return label, learn.dls.vocab[label] else: raise TypeError(f&quot;Expected `str`, `int` or `None`, got {type(label)} instead&quot;) else: # if no `label` is specified, check that `learn.loss_func` has `decodes` # and `activation` implemented, run the predictions through them, # then check that the output length is 1. If not, the activation must be # sigmoid, which is incompatible if not hasattr(learn.loss_func, &#39;activation&#39;) or not hasattr(learn.loss_func, &#39;decodes&#39;): raise NotImplementedError(f&quot;learn.loss_func does not have `.activation` or `.decodes` methods implemented&quot;) decode_pred = compose(learn.loss_func.activation, learn.loss_func.decodes) label_idx = decode_pred(preds) if len(label_idx) &gt; 1: raise RuntimeError(f&quot;Output label idx must be of length==1. If your loss func has a sigmoid activation, please specify `label`&quot;) return label_idx, learn.dls.vocab[label_idx][0] . . def compute_gcam_items(learn: Learner, x: TensorImage, label: Union[str,int,None] = None, target_layer: Union[nn.Module, Callable, None] = None ) -&gt; Tuple[torch.Tensor]: &quot;&quot;&quot;Compute gradient and activations of `target_layer` of `learn.model` for `x` with respect to `label`. If `target_layer` is None, then it is set to `learn.model[:-1]` &quot;&quot;&quot; to_cuda(learn.model, x) target_layer = get_target_layer(learn, target_layer) with HookBwd(target_layer) as hook_g: with Hook(target_layer) as hook: preds = learn.model.eval()(x) activations = hook.stored label_idx, label = get_label_idx(learn,preds,label) #print(preds.shape, label, label_idx) #print(preds) preds[0, label_idx].backward() gradients = hook_g.stored preds = getattr(learn.loss_func, &#39;activation&#39;, noop)(preds) # remove the leading batch_size axis gradients = gradients [0] activations = activations[0] preds = preds.detach().cpu().numpy().flatten() return gradients, activations, preds, label . . shapes of gradients, activations and predictions: . &lt;ipython-input-137-c549b6a525cc&gt;:6: UserWarning: Detected a pooling layer in the model body. Unless this is intentional, ensure that the feature map is not flattened warnings.warn(f&#34;Detected a pooling layer in the model body. Unless this is intentional, ensure that the feature map is not flattened&#34;) . (torch.Size([2048, 7, 7]), torch.Size([2048, 7, 7]), (2,), &#39;COVID&#39;) . 3. Compute gradcam-map . def compute_gcam_map(gradients, activations) -&gt; torch.Tensor: &quot;&quot;&quot;Take the mean of `gradients`, multiply by `activations`, sum it up and return a GradCAM feature map &quot;&quot;&quot; # Mean over the feature maps. If you don&#39;t use `keepdim`, it returns # a value of shape (1280) which isn&#39;t amenable to `*` with the activations gcam_weights = gradients.mean(dim=[1,2], keepdim=True) # (1280,7,7) --&gt; (1280,1,1) gcam_map = (gcam_weights * activations) # (1280,1,1) * (1280,7,7) --&gt; (1280,7,7) gcam_map = gcam_map.sum(0) # (1280,7,7) --&gt; (7,7) return gcam_map . . gcam_map = compute_gcam_map(gradients, activations) gcam_map.shape . torch.Size([7, 7]) . 4. Plot gradcam-map over the image . plotting Grad Cam over image . plot_gcam(learn, img3, x, gcam_map, full_size=True, dpi=300) . . learn.model[1] . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=4096, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . learn.gradcam(item=im, target_layer=learn.model[0]) . . /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai_amalgam/utils.py:92: UserWarning: Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked warnings.warn(&#34;Loaded default PIL ImageFont. It&#39;s highly recommended you use a custom font as the default font&#39;s size cannot be tweaked&#34;) . GUI Building . Creating Buttons: . btn_upload = widgets.FileUpload() btn_upload . img= PILImage.create(btn_upload.data[-1]) . img.shape . (350, 408) . out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(384,404)) out_pl . dls.vocab . [&#39;COVID&#39;, &#39;non-COVID&#39;] . pred,pred_idx,probs = learn.predict(img) . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . btn_run = widgets.Button(description=&#39;Classify&#39;,layout=Layout(width=&#39;40%&#39;, height=&#39;80px&#39;), button_style=&#39;success&#39;) btn_run . Click event handler adds functionallity to butttons: . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(320,320)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . Adding heatmaps button and functionallity: . HeatMp = widgets.Button(description=&#39;MAGIC&#39;, layout=btn_run.layout, button_style=&#39;danger&#39;) HeatMp . def on_click_map(change): with out_pl: display(img.to_thumb(320,320)) learn.gradcam(img).clear(out_pl) HeatMp.on_click(on_click_map) . Putting all the pieces together in a Vertical Stack for the final GUI: . VBox([widgets.Label(&#39;INPUT YOUR CT SCAN IMAGE FOR DETECTION!&#39;), btn_upload, btn_run, out_pl, lbl_pred,widgets.Label(&#39;Do You Want to See How our Model Decides which is Covid and Which is not?&#39;),widgets.Label(&quot;Click Here To Learn how These predictions are made&quot;), HeatMp]) . If You want to see the GUI that I built for this Project check out my other blog post named: Covify . What Worked? . Using a pretrained model Resnet reduced training time and improved results. | Data Augmentations reduced overfitting. | The Mixup approach worked like a charm and also prevented overfitting. | Presizing approaches worked. | I tried Progressive Resizing approach and it greatly improved results and reduced training time. | . What didn&#39;t? . I tried implementing bottleneck layers design on resnets but training was unstable. | I tried a deeper vanilla Resnet 101 model but did not noticed a a significant difference. | . Other ideas to improve the results? . Trying with Diffrent Architectures like Densenet, Efficient Net etc. | Trying out diffrent metrics and improving on them for better results. | More Compute: Deeper Models. Use cross-validation with several folds and Ensemble models. | . Thank you for reading this far!üòäThis was a great challenge and I learned a lot throughout this process.There is also a lot of room for improvement and work to do :) .",
            "url": "https://priyank7n.github.io/fastblogs/2021/01/23/covify-code.html",
            "relUrl": "/2021/01/23/covify-code.html",
            "date": " ‚Ä¢ Jan 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Covify",
            "content": "About . GUI and its Functionalities are: . We can upload a CT Scan Image and our model Covify will predict wether the image uploaded is of a Covid Infected person or not. | When we click the Classify button the model gives a softmax probability of the decision it made and how strongly it tries to put forward its integrity. | When we click on the Magic Button it shows a heat map of all the activations in the image that influenced the model to take certain decisions. | . Covid Prediction on CT Scan Images . when we click on CT Scan images , Our Model predicts wether it is covid or not and also a heat map showcasing its predictions. . Non-Covid Prediction on CT Scan Images . When we click on CT Scan images, Our Model predicts wether it is covid or not and also showcases a heat map. . Heatmaps . This is a heat map for the image showcasing which activations or parts of the image led the model to predict certain decisions on wether the person is covid or not. . Applications . Deep learning, which is a popular research area of artificial intelligence (AI), enables the creation of end-to-end models to achieve promised results using input data, without the need for manual feature extraction. . | We have used CT scan images to not sacrifice on quality but also to improve speed of data diagnosis. . | In order to combat COVID the Current need of the hour is building Medical Diagnosis Support Systems that are Fast, Reliable, Efficient, and Effective. . | Conventional Covid-19 tests that is PCR (Polymerase chain reaction) test are¬†time consuming and also leads to much more False-Negative and False Positive predictions . | We have to send the sample of PCR¬† to the labs which are sometimes in faraway locations that¬† is far time consuming | Sometimes When the doctors and Radiologists are not available at that time we can ¬† generate a preliminary diagnosis¬† | Application of machine learning methods for automatic diagnosis in the medical field have recently gained popularity i.e, have become far more essential in early detection¬† | Fast and accurate diagnostic methods are heavily needed to combat the disease so that more and more time should be invested in Disease Control | . References . https://www.medrxiv.org/content/10.1101/2020.04.24.20078584v3 https://www.kaggle.com/plameneduardo/sarscov2-ctscan-dataset .",
            "url": "https://priyank7n.github.io/fastblogs/2021/01/20/Covify-GUI.html",
            "relUrl": "/2021/01/20/Covify-GUI.html",
            "date": " ‚Ä¢ Jan 20, 2021"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hey Fella My Name is Priyank Negi, I am learning ,explore new stuff and writing on my interests a about topics where AI Skills could be leveraged and Entrepeunership could be created. I believe in Learning by doing and also being curious throughout the process. I think Privacy is essential in this digital age and AI should be Responsibly used. I also care deeply about the essence of Data Ethics in Building great AI Systems followed with responsible use of AI Systems and Data Ethics is Key in building great AI systems. I am always curious and awed by the self driving software of Tesla and Comma AI producing Cars that can tackle almost every situation. . I beleive in learning by doing Stuff instead of the conventional approach of learning everything but actually doing Nothing! I love to experiment with stuff and beleive in Project based learning so i am not a Computer major myself currently pursuing BTech in Electronics &amp; Comm so by following these amazing Courses curated by fast ai team Jeremy howard and syllvian gugger that made DL so easy to actually build stuff on top of our ideas , so I highly recommend checking out these courses as they are excellent to get started and advance in the Feild of AI fast.ai‚Äôs . Thank you and u can reach me out on Twitter or on linkedin . . Thank you .",
          "url": "https://priyank7n.github.io/fastblogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://priyank7n.github.io/fastblogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}