<p><strong>Generative Pre-trained Transformer</strong></p>

<p><strong>(Artificial General Intelligence)</strong></p>

<h1 id="gpt-3">GPT-3</h1>

<p>GPT-3 stands for Generative Pre-trained Transformer 3.</p>

<p>It is a gargantuan artificial Neural Network (NN) around the size of a mouse brain, trained on essentially the whole internet and millions of and GPT-3 is a language model, which means that, using sequence transduction, it can predict the likelihood of an output sequence given an input sequence. This can be used, for instance to predict which word makes the most sense given a text sequence.</p>

<p><img src="/images/name/image1.gif" style="width:6.26789in;height:3.31102in" /></p>

<p>#</p>

<h1 id="facts-about-gpt-3">Facts about GPT-3<img src="/images/name/image2.png" style="width:6.26806in;height:2.02014in" /></h1>

<ul>
  <li>
    <p>Trained on 300 Billion words has been trained using also huge datasets, including the <a href="https://commoncrawl.org/">Common Crawl</a> dataset and the <a href="https://en.wikipedia.org/wiki/Main_Page">English-language Wikipedia</a> (<a href="https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential">spanning some 6 million articles, and making up only 0.6 percent of its training data</a>),</p>
  </li>
  <li>
    <p>GPT-3 uses 175 Billion parameters (Previous model GPT-2 “only” used 1,5 billion parameters.)</p>
  </li>
  <li>
    <p>96 NN Layers (more than human brain has for interactive tasks!)</p>
  </li>
  <li>
    <p>A few $Million$ computing cost for training,but can write a whole book for $1 electricity cost.</p>
  </li>
  <li>
    <p>Top-5 super-computer: 285’000 CPUs + 10’000 GPUs</p>
  </li>
  <li>
    <p>Simpler works better: no encoders and no recurrence</p>
  </li>
  <li>
    <p>GPT-3 is a Generative Pre-Trained Transformer (once only!)</p>
  </li>
  <li>
    <p>Once trained, you prompt it with a (con)text of up to 12288 (sub)words</p>
  </li>
  <li>
    <p>GPT-3 is not fine-tuned (adapted in any way) to different tasks</p>
  </li>
</ul>

<h1 id="gpt-3-training-phase">GPT-3 Training phase</h1>

<p><img src="/images/name/image3.gif" style="width:6.03542in;height:3.39097in" /></p>

<p>The dataset of 300 billion tokens of text is used to generate training examples for the model. For example, these are three training examples generated from the one sentence at the top.</p>

<p><img src="/images/name/image4.gif" style="width:6.26806in;height:3.52153in" /></p>

<p>The model is presented with an example. We only show it the features and ask it to predict the next word.</p>

<p>The model’s prediction will be wrong. We calculate the error in its prediction and update the model so next time it makes a better prediction. Repeat millions of times</p>

<h1 id="gpt-3-working--architecture">GPT-3 Working &amp; Architecture</h1>

<p><img src="/images/name/image5.gif" style="width:4.22222in;height:2.38611in" /></p>

<p>Let’s follow the purple track. How does a system process the word “robotics” and produce “A”?</p>

<p>High-level steps:</p>

<ol>
  <li>
    <p>Convert the word to <a href="https://jalammar.github.io/illustrated-word2vec/">a vector (list of numbers) representing the word</a></p>
  </li>
  <li>
    <p>Compute prediction</p>
  </li>
  <li>
    <p>Convert resulting vector to word</p>
  </li>
</ol>

<p><img src="/images/name/image6.gif" style="width:4.70139in;height:2.66667in" /></p>

<p>The important calculations of the GPT3 occur inside its stack of 96 transformer decoder layers. This is the “depth” in “deep learning”. Each of these layers has its own 1.8B parameter to make its calculations. That is where the “magic” happens.</p>

<p>Let’s follow the purple track. How does a system process the word “robotics” and produce “A”?</p>

<p>High-level steps:</p>

<ol>
  <li>
    <p>Convert the word to <a href="https://jalammar.github.io/illustrated-word2vec/">a vector (list of numbers) representing the word</a></p>
  </li>
  <li>
    <p>Compute prediction</p>
  </li>
  <li>
    <p>Convert resulting vector to word</p>
  </li>
</ol>

<h1 id="gpt-3s-potential-applications">GPT-3’s Potential Applications</h1>

<ul>
  <li>
    <p>Writing short fiction, poetry, press releases, jokes, technical manuals, news articles, …(semi) automated journalism</p>
  </li>
  <li>
    <p>Text translation</p>
  </li>
  <li>
    <p>Text Adventure Game creation/generation</p>
  </li>
  <li>
    <p>Text summarization</p>
  </li>
  <li>
    <p>Question answering</p>
  </li>
  <li>
    <p>Convert plain text to and from legal English</p>
  </li>
  <li>
    <p>Produce functional code Mathematical formulas</p>
  </li>
  <li>
    <p>Write poetry and</p>
  </li>
  <li>
    <p>Play Chess and Go, but not well.</p>
  </li>
  <li>
    <p>Do very simple arithmetic</p>
  </li>
  <li>
    <p>customer support chat bot</p>
  </li>
  <li>
    <p>grammar assistance</p>
  </li>
  <li>
    <p>improving search engine responses auto-generated articles (stocks, finance)</p>
  </li>
</ul>

<h2 id="harmful-applications">Harmful-applications</h2>

<ul>
  <li>
    <p>troll bots derailing online discussions</p>
  </li>
  <li>
    <p>fake news</p>
  </li>
  <li>
    <p>cheat on exams and essay assignments</p>
  </li>
</ul>

<h1 id="gpt-3-limitations">GPT-3 Limitations</h1>

<ul>
  <li>
    <p>Limited common-sense and causal reasoning compared to SOTA and humans <em>(bias towards knowledge rather than intelligence)</em></p>
  </li>
  <li>
    <p>Limited Natural Language &amp; Logical Inference, e.g. comparing sentences A and B (e.g. is word used the same way in A and B , A paraphrases B, A implies B)</p>
  </li>
  <li>
    <p>Unsuitable for bidirectional tasks, such as Cloze</p>
  </li>
  <li>
    <p><em>(a robot obey orders)</em></p>
  </li>
  <li>
    <p>Only good for prediction tasks, not suitable for problems that require sequential decision-making and real truth grounding <em>(acting and planning ahead)</em></p>
  </li>
  <li>
    <p>Performance is unreliable and unpredictable</p>
  </li>
</ul>

<h1 id="future-scope">Future Scope</h1>

<ul>
  <li>
    <p>Larger Models: GPT-3 is still “only” 0.05% of a human brain</p>
  </li>
  <li>
    <p>More data? We nearly reached the limit of available English text</p>
  </li>
  <li>
    <p>Different modalities: most important: vision; speech I/O is solved</p>
  </li>
  <li>
    <p>Other languages: for translation, non-English conversation, culture- specific knowledge</p>
  </li>
  <li>
    <p>More data-efficient training: humans 1000x more efficient</p>
  </li>
  <li>
    <p>Lifelong learning: online, fine-tuning</p>
  </li>
</ul>
